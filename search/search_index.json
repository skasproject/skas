{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SKAS: Simple Kubernetes Authentication System","text":"<p>SKAS is a Kubernetes extension aimed to handle users authentication and authorization.</p> <p>Its main features are:</p> <p></p> <ul> <li>Provide an Kubernetes authentication webhook and a kubectl extension, for seamless Kubernetes CLI integration (Without browser interaction)</li> <li>Allow definition of Users and Groups as Kubernetes custom resources.</li> <li>Provide a DEX connector to support all OIDC aware applications (Argocd, argo workflow, ...)</li> <li>Support one or several LDAP server(s).</li> <li>Provide ability to combine users informations from several sources (LDAP, Local users database, ....).</li> <li>Allow centralized user management in a multi-cluster environment.</li> <li>Allow delegation of the management for a subset of Users and/or Groups.</li> <li>Provide flexible architecture to handle sophisticated user management in a complex environment.</li> <li>Can use a ReadOnly access to the LDAP/AD server(s). User profile can then be enriched with local information.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":""},{"location":"architecture/#initial-deployment","title":"Initial deployment","text":"<p>Here is the different modules involved for a SKAS authentication, right after installation: </p> <p></p> <p>SKAS is deployed as a Kubernetes Pod, this pod hosting three containers:</p> <ul> <li><code>skAuth</code> which is in charge of delivering Kubernetes tokens and validate them.</li> <li><code>skmerge</code>, which is in charge of building a consolidated identity from several identity providers. As in this configuration there is a single one, it act as a simple passthrough</li> <li><code>skCrd</code>, which is an identity provider storing user's information in the Kubernetes storage, in namesapce <code>skas-admin</code>.</li> </ul> <p>Arrow figure out the main communication flow between components. All of them are HTTP exchanges.</p> <p></p> <p>Here is a summary of exchange for an initial interaction</p> <ul> <li>The user issue a <code>kubectl</code> command (such as <code>kubectl get pods</code>). For this, a token is needed. It will be provided by the <code>kubectl-sk</code> client-go credential plugins.</li> <li><code>kubectl-sk</code> prompt the user for login and password, then issue a <code>tokenCreate()</code> request to the <code>skAuth</code> module.</li> <li>The <code>skAuth</code> module issue a <code>getIdentity()</code> request with user credential. This request is forwarded to the <code>skCrd</code>module.</li> <li>The <code>skCrd</code> module retrieve user's information, check password validity and send information upward, to the <code>skMerge</code> module, which forward them to the <code>skAuth</code> module.</li> <li>The <code>skAuth</code> module generate a token and send it back to the <code>kubectl-sk</code> module. Which forward to <code>kubectl</code>.</li> <li><code>kubectl</code> send the original request with the token to the Kubernetes API server</li> <li>The API Server send a <code>tokenReview()</code> request to the <code>skAuth</code> module, which reply with the user's informations (user id and groups).</li> <li>The API Server apply its RBAC rules on user's information to allow or deny requested operation. </li> </ul> <p>There is a more detailed description of this interaction as sequence diagram.</p>"},{"location":"architecture/#ldap-setup","title":"LDAP setup","text":"<p>This schema describe the architecture when an LDAP server has been configured, as described in a previous chapter.</p> <p>The <code>skMerge</code> module is now plugged on two Identity providers: <code>skCrd</code> and <code>skLdap</code>.</p> <p>And <code>skLdap</code> is connected to an external LDAP server</p> <p></p>"},{"location":"architecture/#modules-and-interfaces","title":"Modules and interfaces","text":"<p>Here is a description of the modules composing SKAS and their interfaces:</p>"},{"location":"architecture/#identity-providers-skcrd-skldap-skstatic","title":"Identity providers (skCrd, skLdap, skStatic)","text":"<p>The <code>skStatic</code> provider host its user database in a configMap. It was used for testing in in primary development stages.  It is now deprecated and undocumented.</p> <p>Under default configuration, interfaces provided by this module are associated to an HTTP server bound on <code>localhost</code>,  thus accessible only from another container in the same pod (Typically <code>skMerge</code>).</p>"},{"location":"architecture/#identity","title":"Identity","text":"<p>These module provides an <code>identity</code> interface. The request contains user login and optionally the user password.</p> <p>The response will convey user's information (uid, commonNames, emails, groups) if the user is found and a status:</p> <ul> <li><code>UserNotFound</code>: If the user does not exists in this provider</li> <li><code>PasswordUnchecked</code>: If the password was not provided on the request or if there is no password defined by this provider.</li> <li><code>PassswordChecked</code>: If a password was provided in the request and the provider validate it.</li> <li><code>PasswordFailed</code>: If a password was provided in the request and the provider does not validate it.</li> <li><code>Disabled</code>: The user is found, but its <code>disabled</code> flag is set</li> <li><code>Undefined</code>: The provider is out of order (i.e the ldap server is down)</li> </ul>"},{"location":"architecture/#passwordchange","title":"PasswordChange","text":"<p>The <code>skCrd</code> provider also handle another interface: <code>passwordChange</code>, which allow password modification by providing old and new password (This last in its hashed form)</p>"},{"location":"architecture/#skmerge","title":"skMerge","text":"<p>Under default configuration, interfaces provided by this module are associated to an HTTP server bound on <code>localhost</code>, thus accessible only from another container in the same pod (Typically <code>skAuth</code>)</p>"},{"location":"architecture/#identity_1","title":"Identity","text":"<p>The <code>skMerge</code> module support the same <code>identity</code> interface as a provider. Of course, the returned value is the merge of its underlying providers information.</p> <p>This module also support is some extension of this <code>identity</code> protocol:</p> <ul> <li>The returned result also indicate which provider was the 'authority' (The one who validate the password) for this user.</li> <li>The request can support a <code>detailed</code> flag. If set, the response will be completed with a detailed list of response from each provider. This is aimed to  be displayed by the <code>kubectl-sk user describe</code> CLI command. </li> </ul>"},{"location":"architecture/#passwordchange_1","title":"PasswordChange","text":"<p>The <code>skMerge</code> module also support a <code>passwordChange</code> interface. The request must contains the user's authority which is the target provider to which this message will be forwarded.</p>"},{"location":"architecture/#skauth","title":"skAuth","text":"<p>Under default configuration, all interfaces provided by this module are exposed to outside world through an ingress controller.  They are secured using SSL, from end to end (SSL termination is handled by the module itself, the ingress being configured in SSL passthrough mode).</p> <p>Except <code>login</code> and <code>tokenReview</code>, all other interfaces of this module are intended to be called from the <code>kubectl-sk</code> client executable.</p>"},{"location":"architecture/#tokencreate","title":"TokenCreate","text":"<p>The <code>tokenCreate</code> request contains a user login and password. If the authentication succeed, a token is generated. The response will contains:</p> <ul> <li>The generated token</li> <li>User's information, for <code>whoami</code> subcommand.</li> <li>The clientTTL, for expiration of the token in the client local cache.</li> <li>The authority, the provider who validate the login/password.</li> </ul>"},{"location":"architecture/#tokenrenew","title":"TokenRenew","text":"<p>The <code>tokenRenew</code> interface check if the token is still valid and renew (touch) it.</p>"},{"location":"architecture/#passwordchange_2","title":"PasswordChange","text":"<p>This is a simple passthrough, which forward the request to the underlying <code>skMerge</code> provider.</p>"},{"location":"architecture/#kubeconfig","title":"Kubeconfig","text":"<p>This interface provide a set of informations allowing <code>kubectl-sk</code> to create an entry in the client config file (typically <code>~/.kube/config</code>),  to access the targeted SKAS enabled cluster. This to allow automatic client side configuration.</p>"},{"location":"architecture/#identity_2","title":"Identity","text":"<p>This interface is intended to be called for a <code>kubectl-sk user describe</code> operation. It forward the request to the underlying <code>skMerge</code> protocol. But, as this operation is reserved the the SKAS administrator, the caller must provide authentication information (typically its token) to ensure he has such rights.</p>"},{"location":"architecture/#login","title":"Login","text":"<p>This interface check user credentials. If successful, user's information is provided in the response. It is intended to be use by others applications, such as the DEX connector.</p> <p>This interface is diabled in the default configuration</p>"},{"location":"architecture/#tokenreview","title":"TokenReview","text":"<p>This interface is aimed to support the Webhook Token Authentication. It will be called by the Kubernetes API server to validate a token and retrieve associated user attributes.</p>"},{"location":"architecture/#interfaces-exposition","title":"Interfaces exposition","text":"<p>Each interface can be exposed at three levels:</p> <ul> <li>On localhost, accessible only from other containers in the same pod.</li> <li>Inside Kubernetes, by adding a kubernetes Service.</li> <li>Outside Kubernetes, by adding a kubernetes ingress controller.</li> </ul> <p>For each module, every exposed API can be accessed on two ports:</p> <ul> <li>One associated to a server bound on localhost, for intra-pod communication</li> <li>One associated to a server bound on pod interface, to be exposed as a service or through an ingress. Always using SSL encrypted communication</li> </ul> <p>Depending of the configuration, only one or both server can be activated.</p> <p>For the default configuration, only the server bound on localhost is activated for the <code>skCrd</code>, <code>skLdap</code> and <code>skMerge</code> module. </p> <p>And only the server bound on the pod interface is activated for the <code>skAuth</code> module</p>"},{"location":"architecture/#sequence-diagrams","title":"Sequence diagrams","text":""},{"location":"architecture/#initial-user-connexion","title":"Initial user connexion","text":"<p>Here is the sequence for a successful initial connexion.</p> <pre><code>sequenceDiagram\n  participant User\n  participant kubectl\n  participant kubectl-sk\n  participant skAuth\n  participant Api server\n  autonumber\n  User-&gt;&gt;kubectl: User issue a&lt;br&gt;kubectl command\n  kubectl-&gt;&gt;kubectl-sk: kubectl launch&lt;br&gt;the kubectl-sk&lt;br&gt;credential plugin\n  kubectl-sk-&gt;&gt;kubectl-sk: kubeclt-sk lookup&lt;br&gt;for token in its &lt;br&gt;local cache.&lt;br&gt;NO in this case\n  kubectl-sk-&gt;&gt;User: kubectl-sk prompt&lt;br&gt;for login and password\n  User--&gt;&gt;kubectl-sk: User provides its credential\n  kubectl-sk-&gt;&gt;skAuth: HTTP GET REQ:&lt;br&gt;getToken()\n  skAuth-&gt;&gt;skAuth: skAuth call&lt;br&gt;skMerge which&lt;br&gt;validate user&lt;br&gt;credentials and return&lt;br&gt;user's information&lt;br&gt;A token is generated.\n  skAuth--&gt;&gt;kubectl-sk: Token in&lt;br&gt;HTTP response\n  kubectl-sk--&gt;&gt;kubectl: kubectl-sk return&lt;br&gt;the token to kubectl&lt;br&gt;by printing it on&lt;br&gt;stdout and exit.\n  kubectl-&gt;&gt;Api server: kubectl issue the appropriate API call&lt;br&gt;with the provided bearer token\n  Api server-&gt;&gt;skAuth: The API Server&lt;br&gt;issue an&lt;br&gt;HTTP POST&lt;br&gt;with a&lt;br&gt;TokenReview&lt;br&gt;command\n  skAuth--&gt;&gt;Api server: skAuth validate&lt;br&gt;the token and &lt;br&gt;provide user's&lt;br&gt;name and groups&lt;br&gt;in the response.\n  Api server-&gt;&gt;Api server: API Server validate&lt;br&gt;if user is allowed by&lt;br&gt;RBAC to perform&lt;br&gt;the requested action\n  Api server--&gt;&gt;kubectl: API Server return the action result\n  kubectl--&gt;&gt;User: kubectl display&lt;br&gt;the result and exits</code></pre>"},{"location":"architecture/#token-renewal","title":"Token renewal","text":"<p>Here is the sequence when a valid token is already present in client local cache:</p> <pre><code>sequenceDiagram\n  participant User\n  participant kubectl\n  participant kubectl-sk\n  participant skAuth\n  participant Api server\n  autonumber\n  User-&gt;&gt;kubectl: User issue a&lt;br&gt;kubectl command\n  kubectl-&gt;&gt;kubectl-sk: kubectl launch&lt;br&gt;the kubectl-sk&lt;br&gt;credential plugin\n  kubectl-sk-&gt;&gt;kubectl-sk: kubeclt-sk lookup&lt;br&gt;for token in its &lt;br&gt;local cache.&lt;br&gt;YES in this case\n  kubectl-sk-&gt;&gt;kubectl-sk: Is the token still&lt;br&gt;valid against the&lt;br&gt;clientTokenTTL&lt;br&gt;YES in this case\n  kubectl-sk--&gt;&gt;kubectl: kubectl-sk return&lt;br&gt;the token to kubectl&lt;br&gt;by printing it on&lt;br&gt;stdout and exit.\n  kubectl-&gt;&gt;Api server: kubectl issue the appropriate API call&lt;br&gt;with the provided bearer token\n  Api server-&gt;&gt;skAuth: The API Server&lt;br&gt;issue an&lt;br&gt;HTTP POST&lt;br&gt;with a&lt;br&gt;TokenReview&lt;br&gt;command\n  skAuth--&gt;&gt;Api server: skAuth validate&lt;br&gt;the token and &lt;br&gt;provide user's&lt;br&gt;name and groups&lt;br&gt;in the response.\n  Api server-&gt;&gt;Api server: API Server validate&lt;br&gt;if user is allowed by&lt;br&gt;RBAC to perform&lt;br&gt;the requested action\n  Api server--&gt;&gt;kubectl: API Server return the action result\n  kubectl--&gt;&gt;User: kubectl display&lt;br&gt;the result and exits</code></pre> <p>Here is the sequence when a token is still valid, but the local cache (which is short lived) has expired:</p> <pre><code>sequenceDiagram\n  participant User\n  participant kubectl\n  participant kubectl-sk\n  participant skAuth\n  participant Api server\n  autonumber\n  User-&gt;&gt;kubectl: User issue a&lt;br&gt;kubectl command\n  kubectl-&gt;&gt;kubectl-sk: kubectl launch&lt;br&gt;the kubectl-sk&lt;br&gt;credential plugin\n  kubectl-sk-&gt;&gt;kubectl-sk: kubeclt-sk lookup&lt;br&gt;for token in its &lt;br&gt;local cache.&lt;br&gt;YES in this case\n  kubectl-sk-&gt;&gt;kubectl-sk: Is the token still&lt;br&gt;valid against the&lt;br&gt;clientTokenTTL&lt;br&gt;NO in this case\n  kubectl-sk-&gt;&gt;skAuth: HTTP GET REQ:&lt;br&gt;validateToken()\n  skAuth-&gt;&gt;skAuth: skAuth check&lt;br&gt;if token is still valid.&lt;br&gt;Yes in this case\n  skAuth--&gt;&gt;kubectl-sk: tokenValid response\n  kubectl-sk--&gt;&gt;kubectl: kubectl-sk return&lt;br&gt;the token to kubectl&lt;br&gt;by printing it on&lt;br&gt;stdout and exit.\n  kubectl-&gt;&gt;Api server: kubectl issue the appropriate API call&lt;br&gt;with the provided bearer token\n  Api server-&gt;&gt;skAuth: The API Server&lt;br&gt;issue an&lt;br&gt;HTTP POST&lt;br&gt;with a&lt;br&gt;TokenReview&lt;br&gt;command\n  skAuth--&gt;&gt;Api server: skAuth validate&lt;br&gt;the token and &lt;br&gt;provide user's&lt;br&gt;name and groups&lt;br&gt;in the response.\n  Api server-&gt;&gt;Api server: API Server validate&lt;br&gt;if user is allowed by&lt;br&gt;RBAC to perform&lt;br&gt;the requested action\n  Api server--&gt;&gt;kubectl: API Server return the action result\n  kubectl--&gt;&gt;User: kubectl display&lt;br&gt;the result and exits</code></pre>"},{"location":"architecture/#token-expired","title":"Token expired","text":"<p>And here is the sequence when a token has expired:</p> <pre><code>sequenceDiagram\n  participant User\n  participant kubectl\n  participant kubectl-sk\n  participant skAuth\n  participant Api server\n  autonumber\n  User-&gt;&gt;kubectl: User issue a&lt;br&gt;kubectl command\n  kubectl-&gt;&gt;kubectl-sk: kubectl launch&lt;br&gt;the kubectl-sk&lt;br&gt;credential plugin\n  kubectl-sk-&gt;&gt;kubectl-sk: kubeclt-sk lookup&lt;br&gt;for token in its &lt;br&gt;local cache.&lt;br&gt;YES in this case\n  kubectl-sk-&gt;&gt;kubectl-sk: Is the token still&lt;br&gt;valid against the&lt;br&gt;clientTokenTTL&lt;br&gt;NO in this case\n  kubectl-sk-&gt;&gt;skAuth: HTTP GET REQ:&lt;br&gt;validateToken()\n  skAuth-&gt;&gt;skAuth: skAuth check&lt;br&gt;if token is still valid.&lt;br&gt;NO in this case\n  skAuth--&gt;&gt;kubectl-sk: tokenInvalid&lt;br&gt;response\n  kubectl-sk-&gt;&gt;User: kubectl-sk prompt&lt;br&gt;for login and password</code></pre>"},{"location":"chaining/","title":"Identity Providers chaining","text":""},{"location":"chaining/#overview","title":"Overview","text":"<p>In the previous chapter, a configuration has been setup with two Identity Providers:</p> <pre><code>skMerge:\n  providers:\n    - name: crd\n    - name: ldap\n</code></pre> <p>The <code>crd</code> provider refers to the user database stored in the <code>skas-namespace</code> while the <code>ldap</code> refers to a connected LDAP server.</p> <p>The function of the <code>skMerge</code> module is to make this chain of provider acting as a single one. </p> <p>By default, user information are consolidated the following way: </p> <ul> <li> <p>If a given user exits only in one provider, this one is the authoritative one.</p> </li> <li> <p>If a given user exists in several providers:</p> <ul> <li>The resulting group set is the union of all groups of all providers hosting this user.</li> <li>The resulting email set is the union of all emails of all providers hosting this user.</li> <li>The resulting commonName set is the union of all commonNames of all providers hosting this user.</li> <li>The first provider hosting this user in the chain will be the authoritative one for the password validation.<ul> <li>This imply there can't be two valid passwords for a single user.</li> <li>This also imply providers order is important in the list.</li> <li>There is an exception on this rule: If a user has no password defined (This is a valid case for our    <code>crd</code> provider), then the authoritative one is the next in the list. </li> </ul> </li> <li>The UID will be defined by the authoritative provider.</li> </ul> </li> </ul>"},{"location":"chaining/#cli-user-management","title":"CLI user management","text":"<p>Of course, all <code>kubctl sk user ...</code> operation such as <code>create</code>, <code>patch</code>, <code>bind/unbind</code> can only modify resources in the <code>crd</code> provider. They have no impact on <code>ldap</code> or other external provider.</p> <p>From the SKAS perspective, LDAP is 'Read Only'</p> <p>A specific <code>kubectl sk user describe</code> subcommand will display consolidated information for any user. For example:</p> <pre><code>$ kubectl sk user describe jsmith\nUSER     STATUS              UID      GROUPS             EMAILS                                          COMMON NAMES   AUTH\njsmith   passwordUnchecked   100001   devs,itdep,staff   john.smith@mycompany.com,jsmith@mycompany.com   John SMITH     crd\n</code></pre> <p>Note the last column, which indicate which provider is authoritative for this user.</p> <p>Access to this subcommand is reserved to the members of the <code>skas-admin</code> group.</p> <p>The flag <code>--explain</code> will allow to understand from where user's information are sourced:</p> <pre><code>$ kubectl sk user describe jsmith --explain\nUSER     STATUS              UID      GROUPS             EMAILS                                          COMMON NAMES   AUTH\njsmith   passwordUnchecked   100001   devs,itdep,staff   john.smith@mycompany.com,jsmith@mycompany.com   John SMITH     crd\n\nDetail:\nPROVIDER   STATUS              UID          GROUPS        EMAILS                     COMMON NAMES\ncrd        passwordUnchecked   100001       devs          jsmith@mycompany.com       John SMITH\nldap       passwordUnchecked   1148400004   staff,itdep   john.smith@mycompany.com   John SMITH\n</code></pre> <p>There is also two flags (<code>--password</code> or <code>\u00ecnputPassword</code>) for the administrator to validate a password, if it know it:</p> <pre><code>$ kubectl sk user describe jsmith --explain --inputPassword\nPassword for user 'jsmith':\nUSER     STATUS            UID      GROUPS             EMAILS                                          COMMON NAMES   AUTH\njsmith   passwordChecked   100001   devs,itdep,staff   john.smith@mycompany.com,jsmith@mycompany.com   John SMITH     crd\n\nDetail:\nPROVIDER   STATUS            UID          GROUPS        EMAILS                     COMMON NAMES\ncrd        passwordChecked   100001       devs          jsmith@mycompany.com       John SMITH\nldap       passwordFail      1148400004   staff,itdep   john.smith@mycompany.com   John SMITH\n</code></pre>"},{"location":"chaining/#group-bindings","title":"Group bindings","text":"<p>In the Admin guide, it as been explained how to bind a group to a user from the <code>crd</code> provider. This capability is also possible for any user, whatever his provider is. </p> <p>For example, let say we have a user <code>oriley</code> in the LDAP server (While not defined in our <code>crd</code> provider):</p> <pre><code>$ kubectl sk user describe oriley --explain\nUSER     STATUS              UID          GROUPS        EMAILS                 COMMON NAMES   AUTH\noriley   passwordUnchecked   1148400003   itdep,staff   oriley@mycompany.com   Oliver RILEY   ldap\n\nDetail:\nPROVIDER   STATUS              UID          GROUPS        EMAILS                 COMMON NAMES\ncrd        userNotFound        0\nldap       passwordUnchecked   1148400003   staff,itdep   oriley@mycompany.com   Oliver RILEY\n</code></pre> <p>Let's say we want this user to able to ba an admin for SKAS and also for the Kubernetes cluster. For this, we need to setup two GroupBindings:</p> <pre><code>$ kubectl sk user bind oriley system:masters\nGroupBinding 'oriley.system.masters' created in namespace 'skas-system'.\n$ kubectl sk user bind oriley skas-admin\nGroupBinding 'oriley.skas-admin' created in namespace 'skas-system'.\n\n$ $ kubectl sk user describe oriley --explain\nUSER     STATUS              UID          GROUPS                                  EMAILS                 COMMON NAMES   AUTH\noriley   passwordUnchecked   1148400003   itdep,skas-admin,staff,system:masters   oriley@mycompany.com   Oliver RILEY   ldap\n\nDetail:\nPROVIDER   STATUS              UID          GROUPS                      EMAILS                 COMMON NAMES\ncrd        userNotFound        0            system:masters,skas-admin\nldap       passwordUnchecked   1148400003   staff,itdep                 oriley@mycompany.com   Oliver RILEY\n</code></pre> <p>Of course, this group binding could have been performed on the LDAP server. But this imply to have some Write access on it.  And it could be a best practice to manage cluster authorization at cluster level.  (We will see later a way to centralize authorization in a multi-clusters context).</p>"},{"location":"chaining/#role-binding","title":"Role binding","text":"<p>As it is possible to bind a group to a user defined in whatever provider, it is possible to bind a Kubernetes <code>role</code>  (or <code>clusterRole</code>) to a group defined in the LDAP provider:</p> <pre><code>$ kubectl -n ldemo create rolebinding configurator-itdep --role=configurator --group=itdep\nrolebinding.rbac.authorization.k8s.io/configurator-itdep created\n</code></pre> <p>(See the Admin Guide for the <code>configurator</code> role definition)</p>"},{"location":"chaining/#provider-configuration","title":"Provider configuration.","text":"<p>Up to now, in the configuration, the providers chain has been defined as below:</p> <pre><code>skMerge:\n  providers:\n    - name: crd\n    - name: ldap\n</code></pre> <p>But each provider can support some optional attributes. Here is a snippet with all attributes with their default values:</p> <pre><code>skMerge:\n  providers:\n    - name: crd\n      credentialAuthority: true\n      groupAuthority: true\n      critical: true\n      groupPattern: \"%s\"\n      uidOffset: 0\n    - name: ldap\n      credentialAuthority: true\n      groupAuthority: true\n      critical: true\n      groupPattern: \"%s\"\n      uidOffset: 0\n</code></pre> <ul> <li><code>credentialAuthority</code>: Setting false on this attribute will prevent this provider to authenticate any user.</li> <li><code>groupAuthority</code>: Setting <code>false</code> on this attribute will prevent the groups of this provider to be added to each user.</li> <li><code>critical</code>: Define the behavior of the chain if this provider is out or order. (i.e LDAP server is down). If set, then all authentication will fail in such case.</li> <li><code>groupPattern</code>: Will allow to 'decorate' all groups provided by this provider. See example below.</li> <li><code>uidOffset</code>: This will be added to the UID value if this provider is the authority for this user.</li> </ul> <p>For example:</p> <pre><code>skMerge:\n  providers:\n    - name: crd\n      credentialAuthority: false\n      groupAuthority: true\n      critical: true\n      groupPattern: \"%s\"\n      uidOffset: 0\n    - name: ldap\n      credentialAuthority: true\n      groupAuthority: true\n      critical: true\n      groupPattern: \"dep1_%s\"\n      uidOffset: 0\n</code></pre> <p>The <code>crd</code> provider will not be able to authenticate any user (<code>credentialAuthority</code> is <code>false</code>). This means we have 'lost' our initial <code>admin</code> user.</p> <p>Fortunately, we previously granted <code>oriley</code> with full admin rights. </p> <pre><code>$ kubectl sk login oriley\nPassword:\nlogged successfully..\n\n$ kubectl sk whoami\nUSER     ID           GROUPS\noriley   1148400003   dep1_itdep,dep1_staff,skas-admin,system:masters\n</code></pre> <p>We can check here that this user still belong to the kubernetes admin groups (<code>skas-admin</code>, <code>system:masters</code>) but  the groups of the <code>ldap</code> provider has been renamed with the <code>dep1_</code> prefix.</p> <p>A deeper view on this user:</p> <pre><code>$ kubectl sk user describe oriley --explain\nUSER     STATUS              UID          GROUPS                                            EMAILS                 COMMON NAMES   AUTH\noriley   passwordUnchecked   1148400003   dep1_itdep,dep1_staff,skas-admin,system:masters   oriley@mycompany.com   Oliver RILEY   ldap\n\nDetail:\nPROVIDER   STATUS              UID          GROUPS                      EMAILS                 COMMON NAMES\ncrd        userNotFound        0            system:masters,skas-admin\nldap       passwordUnchecked   1148400003   staff,itdep                 oriley@mycompany.com   Oliver RILEY\n</code></pre> <p>What about the <code>admin</code> user:</p> <pre><code>$ kubectl sk user describe admin --explain\nUSER    STATUS            UID   GROUPS                      EMAILS   COMMON NAMES         AUTH\nadmin   passwordMissing   0     skas-admin,system:masters            SKAS administrator\n\nDetail:\nPROVIDER   STATUS            UID   GROUPS                      EMAILS   COMMON NAMES\ncrd        passwordMissing   0     skas-admin,system:masters            SKAS administrator\nldap       userNotFound      0\n</code></pre> <p>The fact we denied <code>credentialAuthority</code> will translate to <code>passwordMissing</code> (While in fact the password is still physically present in the storage.) </p> <p>Such configuration is aimed to comply with some overall management policies:</p> <ul> <li>A corporate policy that requires all users should be referenced in a central LDAP server. This constraint is fulfilled,  as, although a user can still be created in the <code>crd</code> provider, its corresponding credentials will not be activated.</li> <li>As Kubernetes cluster administrator, we want to have exclusive control of who can manage the cluster. By adding a group decorator (<code>groupPattern: \"dep1_%s\"</code>), we prevent a malicious LDAP administrator to grant access to critical groups  (<code>skas-admin</code>, <code>system:master</code>, ....) to any LDAP users. </li> </ul> <p>Two complementary remarks:</p> <ul> <li>There still may be a interest to create a user in the <code>crd</code> provider. It is to add more information such as email or commonName on a user existing in the <code>ldap</code> provider.</li> <li>Group binding of the user <code>admin</code> should be removed. Otherwise, an LDAP administrator may create a user with such name to have full kubernetes access.</li> </ul>"},{"location":"clusterfederation/","title":"Cluster federation","text":""},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#principle","title":"Principle","text":"<p>As installation was performed using an helm chart, configuration will be performed by providing a 'values' file overriding  the default <code>values.yaml</code> of the helm chart</p> <p>This is what was did in the initial configuration, which such a file:</p> <pre><code>$ cat &gt;./values.init.yaml &lt;&lt;EOF\nclusterIssuer: your-cluster-issuer\nskAuth:\n  exposure:\n    external:\n      ingress:\n        host: skas.ingress.mycluster.internal\n  kubeconfig:\n    context:\n      name: skas@mycluster.internal\n    cluster:\n      apiServerUrl: https://kubernetes.ingress.mycluster.internal\nEOF\n</code></pre> <p>To apply a modified file, the <code>helm upgrade</code> command should be used.</p> <pre><code>$ helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml\n</code></pre>"},{"location":"configuration/#pod-restart","title":"Pod restart","text":"<p>For the new configuration to be taken in account, the <code>skas</code> pod(s) must be restarted. The best and simple way is to perform a 'rollout' on the skas deployment:</p> <pre><code>$ kubectl -n skas-system rollout restart deployment skas\ndeployment.apps/skas restarted\n</code></pre> <p>There is some solution to perform an automatic restart. See reloader</p> <p>SKAS is a very flexible product and, as such, there is a lot of variables in the default <code>values.yaml</code> of the helm chart.  Fortunately, default values are appropriate in most case. </p> <p>We will not describe in this chapter all the variables (You can refer to comments in the file) but will explicit some typical configuration variation. </p>"},{"location":"configuration/#skas-behavior","title":"Skas behavior","text":"<p>Here is a values file which redefine the most common variable related to SKAS behavior:</p> <pre><code>$ cat &gt;./values.behavior.yaml &lt;&lt;\"EOF\"\n# Default value. May be overridden by component\nlog: \n  mode: json # 'json' or 'dev'\n  level: info\n\nskAuth:\n  # Define password requirement\n  passwordStrength:\n    forbidCommon: true    # Test against lists of common password\n    minimumScore: 3       # From 0 (Accept anything) to 4\n\n  tokenConfig:\n    # After this period without token validation, the session expire\n    inactivityTimeout: \"30m\"\n    # After this period, the session expire, in all case.\n    sessionMaxTTL: \"12h\"\n    # This is intended for the client CLI, for token caching\n    clientTokenTTL: \"30s\"\n\nskCrd:\n  initialUser:\n    login: admin\n    # passwordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\n    commonNames: [\"SKAS administrator\"]\n    groups:\n      - skas-admin\nEOF\n</code></pre> <ul> <li>There is a <code>log</code> section, to adjust the level and to set the mode. By default, <code>log.mode</code> is set to <code>json</code>,  aimed to be injected to a log management external system. To have a more 'human' form, <code>log.mode</code> can be set to <code>dev</code>.</li> <li><code>skAuth.passwordStrength</code> will allow to modify the criteria of a valid password. </li> <li><code>skAuth.token.config</code> section will configure the token lifecycle.</li> <li><code>skCrd.initialUser</code> will define the default admin user. Note the <code>passwordHash</code> has been commented out,  otherwise password would be reset on each apply of these values.</li> </ul> <p>The meaning of <code>skAuth</code> and <code>skCrd</code> subsection is described in the Architecture chapter.</p> <p>Then, to apply a modified configuration:</p> <pre><code>$ helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.behavior.yaml\n</code></pre> <p>We still need to add <code>values.init.yaml</code>, otherwise, corresponding default/empty values will be reset.</p> <p>Don't forget to restart the pod(s). See above</p>"},{"location":"configuration/#kubernetes-integration","title":"Kubernetes integration","text":"<p>Here is a values file which redefine the most common variable related to SKAS integration with Kubernetes:</p> <pre><code>$ cat &gt;./values.k8s.yaml &lt;&lt;EOF\n\nreplicaCount: 1\n\n# -- Annotations to be added to the pod\npodAnnotations: {}\n\n# -- Annotations to be added to all other resources\ncommonAnnotations: {}\n\nimage:\n  pullSecrets: []\n  repository: ghcr.io/skasproject/skas\n  # -- Overrides the image tag whose default is the chart appVersion.\n  tag:\n  pullPolicy: IfNotPresent\n\n# Node placement of SKAS pod(s) \nnodeSelector: {}\ntolerations: []\naffinity: {}\n\nEOF\n</code></pre> <ul> <li><code>replicaCount</code> allow to define the number of pod replica for SKAS deployment. Note we are in an active-active configuration, with no need for a leader election mechanism.</li> <li><code>podAnnotations</code> and <code>commonAnnotations</code> will allow to annotate pods and others SKAS resources, if required.</li> <li><code>image</code> subsection will allow to define an alternate image version or location. Useful in an air-gap deployment, where SKAS image is stored in a private repository. </li> <li><code>nodeSelector</code>, <code>toleration</code> and <code>affinity</code> are usual Kubernetes properties related to the node placement of SKAS pod(s)</li> </ul> <p>To apply a modified configuration:</p> <pre><code>$ helm -n skas-system upgrade skas skas/skas \\\n--values ./values.init.yaml --values ./values.behavior.yaml --values ./values.k8s.yaml\n</code></pre> <p>Don't forget to restart the pod(s). See above</p>"},{"location":"delegated/","title":"Delegated users management","text":"<p>As the two configurations are quite similar, there is a lot of redundancy between this chapter and Two LDAP servers configuration chapter</p> <p>Aim of this configuration is the ability to delegate the management of a certain set of users and/or their group bindings.</p> <p>But, we want to restrict the rights of the administrator of the delegated space. Especially , we don't want them to be able to promote themself to global system administrator.</p> <p></p> <p>In this sample configuration, we will setup a separate user database for a department 'dep1'.</p> <p>To achieve this, the solution is to create a specific namespace <code>dep1-userdb</code> which will host <code>skusers</code> and <code>groupBinding</code> SKAS resources</p> <p>And to handle this namespace, we need to instantiate a second Identity Provider, of type <code>skCrd</code>.</p> <p>For the reasons described in Two LDAP servers configuration, we need to instantiate this POD as a separate Helm deployment, although using the same Helm Chart</p> <p>This configuration requires two steps:</p> <ul> <li>Setup a new Helm deployment for <code>skas2</code> pod.</li> <li>Reconfigure the <code>skMerge</code> module of the main SKAS pod to connect to this new IDP.</li> </ul> <p></p> <p>In the following, three variants of this configuration will be described. One with the connection in clear text, and two secured, with network encryption and inter-pod authentication.</p>"},{"location":"delegated/#clear-text-connection","title":"Clear text connection","text":""},{"location":"delegated/#auxiliary-pod-configuration","title":"Auxiliary POD configuration","text":"<p>Here is a sample values file to configure the auxiliary POD:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskLdap:\nenabled: false\nskCrd:\nenabled: true\nnamespace: dep1-userdb\nadminGroups:\n- dep1-admin\ninitialUser:\nlogin: dep1-admin\npasswordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\ncommonNames: [\"DEP1 administrator\"]\ngroups:\n- admin\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7112\nssl: false\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"*\"\nsecret: \"*\"\nprotected: true\n</code></pre> <ul> <li> <p>At the beginning of the file, we disable all other modules than <code>skCrd</code>.</p> </li> <li> <p><code>skCrd.namespace:  dep1-userdb</code>  define the namespace this IDP will use to manage the user's information.</p> </li> <li> <p>Then, we define one adminGroup: <code>dep1-admin</code>. The Helm chart will setup RBAC rules to allow members of this group to  access SKAS users resources in the namespace set above. </p> </li> <li> <p>Then, we create an initial admin user <code>dep1-admin</code>, belonging to the group <code>admin</code>. More on this later.</p> </li> </ul> <p>Then, there is the <code>exposure</code> part, who define how this service will be exposed. (The default configuration is expose  to <code>localhost</code> in clear text)</p> <ul> <li><code>exposure.internal.enabled: false</code> shutdown the HTTP server bound on localhost.</li> <li><code>exposure.external.enabled: true</code> set the HTTP server bound on the POD IP up. This on port 7112 with no SSL.</li> <li>Then the is the configuration of the <code>identity</code> service to expose:<ul> <li><code>clients[]</code> is a mechanism to validate who is able to access this service, by providing an <code>id</code> and a <code>secret</code>   (or password). the values \"*\" will disable this feature.</li> <li><code>protected: true</code> activate an internal mechanism against attacks of type 'Brut force', by introducing delays on   unsuccessful connection attempts, and by limiting the number of simultaneous connections. There is no reason to   disable it, except if you suspect a misbehavior.</li> </ul> </li> </ul> <p>To deploy this configuration:</p> <pre><code>helm -n skas-system install skas2 skas/skas --values ./values.skas2.yaml\n</code></pre> <p>Note the `skas2' release name</p> <p>The Helm chart will deploy the new pod(s), under the name <code>skas2</code>. it will also deploy an associated Kubernetes <code>service</code>.</p>"},{"location":"delegated/#main-pod-reconfiguration","title":"Main pod reconfiguration","text":"<p>Second step is to reconfigure the main POD</p> <p>Here is a sample of appropriate configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: http://skas2-crd.skas-system.svc\n</code></pre> <p>There is two entries aimed to configure a provider on the bottom of the <code>skMerge</code> module:</p> <ul> <li><code>providers</code> is a list of the connected providers, which allow to define their behavior. The order is important here. Note the <code>groupPattern: \"dep1-%s\"</code>   Refers to the IDP chaining: Provider configuration chapter for more information.</li> <li><code>providerInfo</code> is a map providing information on how to reach these providers.For <code>crd</code>, we use the   default <code>localhost</code> port.For <code>crd_dep1</code> we use the service created by the <code>skas2</code> deployment.</li> </ul> <p>The link between these two entries is of course the provider name.</p> <p>Then, the reconfiguration must be applied:</p> <pre><code>$ helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.skas.yaml\n</code></pre> <p>Don't forget to add the <code>values.init.yaml</code>, or to merge it in the <code>values.main.yaml</code> file. Also, if you have others values file, they must be added on each upgrade</p> <p>And don't forget to restart the pod(s). See Configuration: Pod restart</p> <p>If deploying two separate Charts is a constraint for you, you may setup a 'meta chart'. See here</p>"},{"location":"delegated/#test-and-usage","title":"Test and Usage","text":"<p>Then, you can now test your configuration:</p> <p>You can login using the dep1-admin user that has been created by the previous deployment:</p> <pre><code>$ kubectl sk login\nLogin:dep1-admin\nPassword:\nlogged successfully..\n</code></pre> <p>Password is <code>admin</code>. Of course, to change ASAP (<code>kubectl sk password</code>)</p> <p>Now, look at what our account look like:</p> <pre><code>$ kubectl sk whoami\nUSER         ID   GROUPS\ndep1-admin   0    dep1-admin\n</code></pre> <p>Note the group: <code>dep1-admin</code>. The prefix <code>dep1-</code> has been added, as for any group of this provider.  This is the result of  <code>groupPattern: \"dep1-%s\"</code> configuration above.</p>"},{"location":"delegated/#user-management","title":"User management","text":"<p>As <code>dep1-admin</code>, you can manage users in the namespace <code>dep1-userdb</code>:</p> <pre><code>$ kubectl sk -n dep1-userdb user create fred --commonName \"Fred Astair\" --password \"GtaunPMgP5f\"\nUser 'fred' created in namespace 'dep1-userdb'.\n\n$ kubectl sk -n dep1-userdb user bind fred managers\nGroupBinding 'fred.managers' created in namespace 'dep1-userdb'.\n\n$ kubectl -n dep1-userdb get skusers\nNAME         COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\ndep1-admin   [\"DEP1 administrator\"]\nfred         [\"Fred Astair\"]                                   false\n$ kubectl -n dep1-userdb get groupbindings\nNAME               USER         GROUP\ndep1-admin-admin   dep1-admin   admin\nfred.managers      fred         managers\n</code></pre> <p>Then you can test the user 'fred'</p> <pre><code>$ kubectl sk login\nLogin:fred\nPassword:\nlogged successfully..\n\n$ kubectl sk whoami\nUSER   ID   GROUPS\nfred   0    dep1-managers\n</code></pre> <p>_Note the group prefixed by <code>dep1-</code>. This will ensure no user managed by this identity provider can belong to some strategic groups such as <code>skas-admin</code> or <code>system:masters</code>.</p> <p>Now, logged back to <code>dep1-admin</code>, ensure we are limited to our namespace:</p> <pre><code>$ kubectl sk login dep1-admin\nPassword:\nlogged successfully..\n\n$ kubectl -n skas-system get skusers\nError from server (Forbidden): users.userdb.skasproject.io is forbidden: User \"dep1-admin\" cannot list resource \"users\" in API group \"userdb.skasproject.io\" in the namespace \"skas-system\"\n$ kubectl get --all-namespaces skusers\nError from server (Forbidden): users.userdb.skasproject.io is forbidden: User \"dep1-admin\" cannot list resource \"users\" in API group \"userdb.skasproject.io\" at the cluster scope\n</code></pre> <p>The <code>sk user describe</code> subcommand is also unauthorized, as it is, by definition, a cross provider feature.</p> <pre><code>$ kubectl sk user describe fred\nUnauthorized!\n</code></pre>"},{"location":"delegated/#default-namespace","title":"Default namespace","text":"<p>Providing the namespace to each command can be tedious. It can be set as the default one for both <code>kubectl</code> and <code>kubectl-sk</code> subcommands:</p> <pre><code>kubectl config set-context --current --namespace=dep1-userdb\n</code></pre> <p>Then:</p> <pre><code>$ kubectl get skusers\nNAME         COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\ndep1-admin   [\"DEP1 administrator\"]\nfred         [\"Fred Astair\"]                                   false\n</code></pre> <p>Or it can be set with an environment variable:</p> <pre><code>export SKAS_NAMESPACE=\"dep1-userdb\"\n</code></pre> <p>But this last method will apply only on <code>kubectl-sk</code> subcommands</p>"},{"location":"delegated/#user-describe","title":"User describe","text":"<p>As stated above, a <code>dev1-admin</code> user is not allowed use the <code>kubectl sk user describe</code> subcommand.</p> <p>This control is performed by the <code>skAuth</code> module, with a list of allowed groups.  Here is a modified version of the values file which allow <code>dep1-admin</code> members to perform a user describe subcommand. </p> values.skas.yaml <pre><code>skAuth:\n# Members of these group will be allowed to perform 'kubectl_sk user describe'\n# Also, they will be granted by RBAC to access token resources\nadminGroups:\n- skas-admin\n- dep1-admin\nskMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: http://skas2-crd.skas-system.svc\n</code></pre> <p>As stated in the comment, these users will also be able to view and delete session tokens.</p> <p>So, be conscious than setting a group as an admin for the <code>skAuth</code> module will allow some operation out of the strict  perimeter defined be the namespace <code>dep1-userdb</code>. Anyway, members of such groups will still be prevented from listing or  editing users and groupBinding out of their initial namespace. </p>"},{"location":"delegated/#the-skas-admin-user","title":"The SKAS admin user","text":"<p>And what about the initial SKAS global <code>admin</code> user. Is it  able to manager also the <code>dep1-userdb</code> database ?</p> <p>It's depend if you have bind this user to the <code>system:master</code> group. If so, it will have full cluster access:</p> <pre><code>$ kubectl get -n dep1-userdb skusers\nNAME         COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\ndep1-admin   [\"DEP1 administrator\"]\nfred         [\"Fred Astair\"]                                   false\n</code></pre> <p>You can remove this binding (Then logout/login):</p> <pre><code>$ kubectl sk whoami\nUSER    ID   GROUPS\nadmin   0    skas-admin,system:masters\n\n$ kubectl sk user unbind admin system:masters\nGroupBinding 'admin.system.masters' in namespace 'skas-system' has been deleted.\n\n$ kubectl sk logout\nBye!\n\n$ kubectl sk login admin\nPassword:\nlogged successfully..\n\n$ kubectl sk whoami\nUSER    ID   GROUPS\nadmin   0    skas-admin\n</code></pre> <p>Now, access should be denied </p> <pre><code>$ kubectl get -n dep1-userdb skusers\nError from server (Forbidden): users.userdb.skasproject.io is forbidden: User \"admin\" cannot list resource \"users\" in API group \"userdb.skasproject.io\" in the namespace \"dep1-userdb\"\n</code></pre> <p>But, as SKAS admin, you can promote yourself as a member of the <code>dep1-admin</code> group.</p> <pre><code>$ kubectl sk user bind admin dep1-admin\nGroupBinding 'admin.dep1-admin' created in namespace 'skas-system'.\n\n$ kubectl sk logout\nBye!\n\n$ kubectl sk login\nLogin:admin\nPassword:\nlogged successfully..\n\n$ kubectl sk whoami\nUSER    ID   GROUPS\nadmin   0    dep1-admin,skas-admin\n\n$ kubectl get -n dep1-userdb skusers\nNAME         COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\ndep1-admin   [\"DEP1 administrator\"]\nfred         [\"Fred Astair\"]                                   false\n</code></pre>"},{"location":"delegated/#securing-connection","title":"Securing connection","text":"<p>It should ne noted than unencrypted passwords will transit through the link between the two pods. So, setting up encryption is a must have.</p>"},{"location":"delegated/#auxiliary-pod-configuration_1","title":"Auxiliary POD configuration","text":"<p>Here is the modified version for the <code>skas2</code> pod configuration:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskLdap:\nenabled: false\nclusterIssuer: cluster-issuer1\nskCrd:\nenabled: true\nnamespace: dep1-userdb\nadminGroups:\n- dep1-admin\ninitialUser:\nlogin: dep1-admin\npasswordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\ncommonNames: [\"DEP1 administrator\"]\ngroups:\n- admin\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7112\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"skMerge\"\nsecret: \"aSharedSecret\"\nprotected: true\n</code></pre> <p>The differences are the following:</p> <ul> <li>There is a <code>clusterIssuer</code> definition to be able to generate a certificate. (It is assumed here than <code>cert-manager</code> is deployed in the cluster)</li> <li><code>exposure.external.ssl</code> is set to <code>true</code>. This will also leads the generation of the server certificate.</li> <li>The <code>service.identity.clients</code> authentication is also activated. The <code>id</code> and <code>secret</code> values will have to be provided by the <code>skMerge</code> client.</li> </ul> <p>To deploy this configuration:</p> <pre><code>helm -n skas-system install skas2 skas/skas --values ./values.skas2.yaml\n</code></pre> <p>Note the `skas2' release name</p> <p>The Helm chart will deploy the new pod(s), under the name <code>skas2</code>. it will also deploy an associated Kubernetes <code>service</code> and the <code>cert-manager.io/v1/Certificate</code> request.</p>"},{"location":"delegated/#main-pod-reconfiguration_1","title":"Main pod reconfiguration","text":"<p>Here is the modified version for the main SKAS POD configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: https://skas2-crd.skas-system.svc\nrootCaPath: /tmp/cert/skas2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: aSharedSecret\nextraSecrets:\n- secret: skas2-crd-cert\nvolume: skas2-cert\nmountPath: /tmp/cert/skas2\n</code></pre> <p>The <code>providerInfo.crd_dep1</code> has been modified for SSL and authenticated connection:</p> <ul> <li><code>url</code> begins with <code>https</code>.</li> <li><code>clientAuth</code> provides information to authenticated against the <code>skas2</code> pod.</li> <li><code>insecureSkipVerify</code> is set to false, as we want to check certificate validity.</li> <li><code>rootCaPath</code> is set to access the <code>ca.crt</code>, the CA validating the <code>skas2</code> server certificate.</li> </ul> <p>As stated above, during the deployment of the <code>skas2</code> auxiliary POD, a server certificate has been generated to allow SSL enabled services. This certificate is stored in a secret (of type <code>kubernetes.io/tls</code>) named <code>skas2-crd-cert</code>. Alongside the private/public key pair, it also contains the root Certificate authority under the name<code>ca.crt</code>.</p> <p>The <code>skMerge.extraSecrets</code> subsection instruct the POD to mount this secret to the defined location. The property <code>skMerge.providerInfo.crd_dep1.rootCaPath</code> can now refer to the mounted value.</p> <p>Then, the reconfiguration must be applied:</p> <pre><code>$ helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.skas.yaml\n</code></pre> <p>Don't forget to add the <code>values.init.yaml</code>, or to merge it in the <code>values.ldap.yaml</code> file. Also, if you have others values file, they must be added on each upgrade</p> <p>And don't forget to restart the pod(s). See Configuration: Pod restart</p> <p>You can now test again your configuration, as described above</p>"},{"location":"delegated/#use-a-kubernetes-secrets","title":"Use a Kubernetes secrets","text":"<p>There is still a security issue, as the shared secret (<code>aSharedSecret</code>) is in clear text in both values file. As such it may ends up in some version control system.</p> <p>The good practice will be to store the secret value in a kubernetes <code>secret</code> resource, such as:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: skas2-client-secret\nnamespace: skas-system\ndata:\nclientSecret: Sk1rbkNyYW5WV1YwR0E5\ntype: Opaque\n</code></pre> <p>Where <code>data.clientSecret</code> is the secret encoded in base 64.</p> <p>There is several solution to generate such secret value. One can use Helm with some random generator function. Or use a Secret generator</p>"},{"location":"delegated/#auxiliary-pod-configuration_2","title":"Auxiliary POD configuration","text":"<p>To use this secret, here is the new modified version for the <code>skas2</code> POD configuration:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskLdap:\nenabled: false\nclusterIssuer: cluster-issuer1\nskCrd:\nenabled: true\nnamespace: dep1-userdb\nadminGroups:\n- dep1-admin\ninitialUser:\nlogin: dep1-admin\npasswordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\ncommonNames: [\"DEP1 administrator\"]\ngroups:\n- admin\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7112\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"skMerge\"\nsecret: ${SKAS2_CLIENT_SECRET}\nprotected: true\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\n</code></pre> <p>The modifications are the following:</p> <ul> <li>The <code>skLdap.extraEnv</code> subsection inject the secret value as an environment variable in the container.</li> <li>the <code>exposure.external.services.identity.clients[0].secret</code> fetch its value through this environment variable.</li> </ul> <p>Most of the values provided by the helm chart ends up inside a configMap, which is then loaded by the SKAS executable. The environment variable interpolation occurs during this load.</p>"},{"location":"delegated/#main-pod-reconfiguration_2","title":"Main pod reconfiguration","text":"<p>Here is the modified version, with <code>secret</code> handling, for the main SKAS pod configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: https://skas2-crd.skas-system.svc\nrootCaPath: /tmp/cert/skas2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: ${SKAS2_CLIENT_SECRET}\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\nextraSecrets:\n- secret: skas2-crd-cert\nvolume: skas2-cert\nmountPath: /tmp/cert/skas2\n</code></pre> <p>The modifications are the same as the SKAS2 POD</p>"},{"location":"dex/","title":"DEX integration","text":""},{"location":"dex/#configuration","title":"Configuration","text":""},{"location":"dex/#example-argocd","title":"Example: ArgoCD","text":""},{"location":"installation/","title":"INSTALLATION","text":""},{"location":"installation/#install-skas-helm-chart","title":"Install SKAS helm chart","text":"<p>The simplest and recommended method to install the skas server is to use the provided helm chart.</p> <p>The following is assumed</p> <ul> <li>Certificate manager is deployed in the target cluster and a <code>ClusterIssuer</code> is defined.</li> <li>There is an nginx ingress controller deployed in the target cluster.</li> <li>You have a local client kubernetes configuration with full admin rights on target cluster.</li> <li>Helm is installed locally.</li> </ul> <p>First, add the SKAS helm repo:</p> <pre><code>$ helm repo add skas https://skasproject.github.io/skas-charts\n</code></pre> <p>Then, create a dedicated namespace:</p> <pre><code>$ kubectl create namespace skas-system\n</code></pre> <p>Then, you can deploy the helm chart:</p> <pre><code>$ helm -n skas-system install skas skas/skas \\\n--set clusterIssuer=your-cluster-issuer \\\n--set skAuth.exposure.external.ingress.host=skas.ingress.mycluster.internal \\\n--set skAuth.kubeconfig.context.name=skas@mycluster.internal \\\n--set skAuth.kubeconfig.cluster.apiServerUrl=https://kubernetes.ingress.mycluster.internal\n</code></pre> <p>With the following values, adjusted to your context:</p> <ul> <li><code>clusterIssuer</code>: The Certificate Manager <code>ClusterIssuer</code> used to generate the certificate for all ingress access.</li> <li><code>skAuth.exposure.external.ingress.host</code>: The ingress hostname used to access the SKAS service from outside of the cluster.    You will also have to define this name in your DNS.</li> </ul> <p>The two following values will be used on generation of user's k8s config files:</p> <ul> <li><code>skAuth.kubeconfig.context.name</code>: The context name which will be used to identify this cluster in the local config file. Can be any name</li> <li><code>skAuth.kubeconfig.cluster.apiServerUrl</code>: The API server URL, from outside of the cluster. If you don't know it, you can find the information on an existing config file, with the yaml path <code>cluters[X].cluster.server</code>.</li> </ul> <p>As an alternate approach, you can create a local values yaml file:</p> <pre><code>$ cat &gt;./values.init.yaml &lt;&lt;EOF\nclusterIssuer: your-cluster-issuer\nskAuth:\n  exposure:\n    external:\n      ingress:\n        host: skas.ingress.mycluster.internal\n  kubeconfig:\n    context:\n      name: skas@mycluster.internal\n    cluster:\n      apiServerUrl: https://kubernetes.ingress.mycluster.internal\nEOF\n</code></pre> <p>And issue the helm command as:</p> <pre><code>$ helm -n skas-system install skas skas/skas --values ./values.init.yaml\n</code></pre> <p>Then, if the installation is successful, you should be able to see the 'skas' server pod:</p> <pre><code>$ kubectl -n skas-system get pods\nNAME                    READY   STATUS    RESTARTS   AGE\nskas-746c54dc75-v8v2f   3/3     Running   0          25s\n</code></pre>"},{"location":"installation/#use-another-ingress-controller-instead-of-nginx","title":"Use another ingress controller instead of nginx","text":"<p>If using another ingress controller, launch the helm chart with <code>--set ingressClass=xxxx</code>. As the value will not be 'nginx', no ingress resource will be  created by the helm chart. It is up to you to setup your own ingress.  (Here is the nginx definition, as a starting point.)</p> <p>Please note that the ingress is configured with <code>ssl-passthroughs</code>. The underlying service will handle SSL.</p>"},{"location":"installation/#no-certificate-manager","title":"No Certificate Manager","text":"<p>If you do not use Certificate Manager, launch the helm chart without <code>ClusterIssuer</code> definition.  Then, the secret hosting the certificate for the services will be missing and will need to be created it manually. (The <code>skas</code> pod will fail)</p> <ul> <li>Prepare PEM encoded self-signed certificate and key files.The certificate must be valid for the following hostnames:<ul> <li><code>skas-auth</code></li> <li><code>skas-auth.skas-system.svc</code></li> <li><code>localhost</code></li> <li><code>skas.ingress.mycluster.internal</code> (To be adjusted to your the effective host name provided above)</li> </ul> </li> <li>Base64-encode the CA cert (in its PEM format) and its key.</li> <li>Create Secret in <code>skas-system</code> namespace as follows:</li> </ul> <pre><code>$ kubectl -n skas-system create secret tls skas-auth-cert --cert=&lt;CERTIFICATE FILE&gt; --key=&lt;KEY FILE&gt;\n</code></pre> <p>Then, the <code>skas</code> pod should start successfully.</p>"},{"location":"installation/#api-server-configuration","title":"API Server configuration.","text":"<p>The Authentication Webhook of the API server should be configured to reach our authentication module.</p>"},{"location":"installation/#manual-configuration","title":"Manual configuration","text":"<p>Depending of your installation, the directory mentioned below may differs (For information, the clusters used for test and documentation are built with kubespray)</p> <p>Also, this procedure assume the API Server is managed by the Kubelet, as a static Pod. If your API Server is managed by another system (i.e. systemd), you should adapt accordingly.</p> <p>The following operations must be performed on all nodes hosting an instance of the Kubernetes API server. Typically, all nodes of the control plane.</p> <p>Also, these operations require <code>root</code> access on these nodes.</p> <p>First, create a folder dedicated to <code>skas</code>:</p> <pre><code>[root]$ mkdir -p /etc/kubernetes/skas\n</code></pre> <p>Then, create the Authentication webhook configuration file in this folder (You can cut/paste the following):</p> <pre><code>[root]$ cat &gt;/etc/kubernetes/skas/hookconfig.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Config\n# clusters refers to the remote service.\nclusters:\n  - name: sk-auth\n    cluster:\n      certificate-authority: /etc/kubernetes/skas/skas_auth_ca.crt        # CA for verifying the remote service.\n      server: https://sk-auth.skas-system.svc:7014/v1/tokenReview # URL of remote service to query. Must use 'https'.\n# users refers to the API server's webhook configuration.\nusers:\n  - name: skasapisrv\n# kubeconfig files require a context. Provide one for the API server.\ncurrent-context: authwebhook\ncontexts:\n- context:\n    cluster: sk-auth\n    user: skasapisrv\n  name: authwebhook\nEOF\n</code></pre> <p>As you can see in this file, there is a reference to the certificate authority of the authentication webhook service. So, you must fetch it and copy to this location:</p> <p>NB: It is assumed <code>kubectl</code> command was installed in this node, with an administrator configuration.</p> <pre><code>[root]$ kubectl -n skas-system get secret skas-auth-cert -o=jsonpath='{.data.ca\\.crt}' | base64 -d &gt;/etc/kubernetes/skas/skas_auth_ca.crt  </code></pre> <pre><code>[root]$ ls -l /etc/kubernetes/skas\ntotal 8\n-rw-r--r--. 1 root root  620 May 11 12:36 hookconfig.yaml\n-rw-r--r--. 1 root root 1220 May 11 12:58 skas_auth_ca.crt\n</code></pre> <p>Now, you must edit the API server manifest file (<code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>) to load the <code>hookconfig.yaml</code> file:</p> <pre><code>[root]$ vi /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> <p>First step is to add two flags to the kube-apiserver command line:</p> <ul> <li><code>--authentication-token-webhook-cache-ttl</code>: How long to cache authentication decisions.</li> <li><code>--authentication-token-webhook-config-file</code>: The path to the configuration file we just setup</li> </ul> <p>Here is what it should look like:</p> <pre><code>...\nspec:\ncontainers:\n- command:\n- kube-apiserver\n- --authentication-token-webhook-cache-ttl=30s\n- --authentication-token-webhook-config-file=/etc/kubernetes/skas/hookconfig.yaml\n- --advertise-address=192.168.33.16\n- --allow-privileged=true\n- --anonymous-auth=True\n...\n</code></pre> <p>And the second step will consists to map the node folder <code>/etc/kubernetes/skas</code> inside the API server pod, under the same path. This is required as these files are accessed in the API Server container context.</p> <p>For this, a new <code>volumeMounts</code> entry should be added:</p> <pre><code>    volumeMounts:\n- mountPath: /etc/kubernetes/skas\nname: skas-config\n....\n</code></pre> <p>And a corresponding new <code>volumes</code>  entry:</p> <pre><code>  volumes:\n- hostPath:\npath: /etc/kubernetes/skas\ntype: \"\"\nname: skas-config\n....\n</code></pre> <p>And another configuration parameter must be defined. The <code>dnsPolicy</code> must be set to <code>ClusterFirstWithHostNet</code>. Ensure such key does not already exists and add it:</p> <pre><code>  hostNetwork: true\ndnsPolicy: ClusterFirstWithHostNet </code></pre> <p>This complete the API Server configuration. Saving the edited file will trigger a restart of the API Server.</p> <p>For more information, the kubernetes documentation on this topic is here</p> <p>Remember: Perform this on all nodes hosting an instance of API Server.</p>"},{"location":"installation/#using-an-ansible-role","title":"Using an Ansible role","text":"<p>If ansible is one of your favorite tool, you may automate these tedious tasks by using an ansible role.</p> <p>You will find such a role here</p> <p>As the manual installation, you may need to modify it accordingly to you local context.</p> <p>To use it, we assume you have an ansible configuration with an inventory defining the target cluster. Then:</p> <ul> <li>Download, and untar the role archive provided above in a folder which is part of the rolepath. </li> <li>create a playbook file, such as :</li> </ul> <pre><code>$ cat &gt;./skas.yaml &lt;&lt;EOF\n- hosts: kube_control_plane  # This group must target all the nodes hosting an instance of the kubernetes API server\n  tags: [ \"skas\" ]\n  vars:\n    skas_state: present\n  roles:\n  - skas-apiserver\nEOF\n</code></pre> <ul> <li>Launch the playbook:</li> </ul> <pre><code>$ ansible-playbook ./skas.yaml\n</code></pre> <p>The playbook will perform all the steps described in the manual installation above. This will generate a restart of the API server.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>A small typo or incoherence in configuration may lead to API Server unable to restart. If this is the case, you may have a look in the logs of the Kubelet (Remember, as a static pod, the API Server is managed by the Kubelet) in order to figure out what'is happen.</p> <p>If you made a modification in this the <code>hookconfig.yaml</code> file, or if you update the CA file, will need to restart the API Server to reload the configuration. But, the API Server is a 'static pod', a pod managed by the kubelet. As such, it can't be restarted as a standard pod. The simplest way to trigger its effective reload is to modify the <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> file. And you will need a real modification. Touch may not be enough. A common trick here is to modify slightly <code>the authentication-token-webhook-cache-ttl</code> flag value.</p>"},{"location":"installation/#skas-cli-installation","title":"SKAS CLI installation.","text":"<p>SKAS provide a CLI interface as an extension of kubectl</p> <p>Installation is straightforward:</p> <ul> <li>Download the executable for your OS/Architecture</li> <li>Name it <code>kubectl-sk</code> (To comply to the naming convention of kubectl extension)</li> <li>Make it executable</li> <li>Move it to a folder accessed by your PATH.</li> </ul> <p>For example, on a Mac Intel:</p> <pre><code>$ cd /tmp\n$ curl -L https://github.com/skasproject/skas/releases/download/0.2.1/kubectl-sk_0.2.1_darwin_amd64 -o ./kubectl-sk\n$ chmod 755 kubectl-sk\n$ sudo mv kubectl-sk /usr/local/bin\n</code></pre> <p>You can now check the extension is effective</p> <pre><code>$ kubectl sk\nA kubectl plugin for Kubernetes authentication\n\nUsage:\nkubectl-sk [command]\nAvailable Commands:\ncompletion  Generate the autocompletion script for the specified shell\nhash        Provided password hash, for use in config file\nhelp        Help about any command\ninit        Add a new context in Kubeconfig file for skas access\nlogin       Logout and get a new token\nlogout      Clear local token\npassword    Change current password\nuser        Skas user management\nversion     display skas client version\nwhoami      Display current logged user, if any\n\nFlags:\n-h, --help                help for kubectl-sk\n--kubeconfig string   kubeconfig file path. Override default configuration.\n--logLevel string     Log level (default \"INFO\")\n--logMode string      Log mode: 'dev' or 'json' (default \"dev\")\nUse \"kubectl-sk [command] --help\" for more information about a command.\n</code></pre> <p>There is also a command to list all available plugins:</p> <pre><code>$ kubectl plugin list\n....\n/usr/local/bin/kubectl-sk\n....\n</code></pre> <p>SKAS is now fully installed. You can now move on the User guide. </p>"},{"location":"installation/#skas-removal","title":"SKAS Removal","text":"<p>When performing SKAS removal, the first step is to reconfigure the API server.</p> <p>If you configured it manually, then you must remove the two entries <code>--authentication-token-webhook-cache-ttl</code> and <code>--authentication-token-webhook-config-file</code> from the API server manifest file (/etc/kubernetes/manifests/kube-apiserver.yaml)</p> <p>If you configured it using the ansible role, just modify the playbook by setting <code>skas_state: absent</code>: </p> <pre><code>$ cat &gt;./skas.yaml &lt;&lt;EOF\n- hosts: kube_control_plane  # This group must target all the nodes hosting an instance of the kubernetes API server\n  tags: [ \"skas\" ]\n  vars:\n    skas_state: absent\n  roles:\n  - skas-apiserver\nEOF\n</code></pre> <p>and launch it:</p> <pre><code>$ ansible-playbook ./skas.yaml\n</code></pre> <p>Then, you can uninstall the helm chart</p> <pre><code>$ helm -n skas-system uninstall skas\n</code></pre> <p>And delete the namespace</p> <pre><code>$ kubectl delete namespace skas-system\n</code></pre>"},{"location":"ldap/","title":"LDAP Setup","text":""},{"location":"ldap/#overview","title":"Overview","text":"<p>SKAS support the concept of 'Identity Provider'. An Identity provider (IDP) can be defined as a Database hosting  users and groups and providing user authentication and information. </p> <p>Up to now, we have used a single IDP based on resources stored in Kubernetes, in the namespace <code>skas-system</code>.</p> <p>SKAS provide another Identity Provider backed by an LDAP directory</p> <p>The SKAS LDAP identity provider was strongly inspired from the LDAP connector of DEX project. Great thanks to all its contributors.</p>"},{"location":"ldap/#security-considerations","title":"Security considerations","text":"<p>SKAS  attempts to bind with the backing LDAP server using the admin and end user's plain text password. Though some LDAP implementations allow passing hashed passwords, SKAS doesn't support hashing and instead strongly recommends that all administrators just use TLS. This can often be achieved by using port 636 instead of 389, and by handling certificate authority.</p> <p>SKAS currently allows insecure connections to ensure connectivity with the wide variety of LDAP implementations and to ease initial setup. But such configuration should never be used in a production context, as they are actively leaking passwords.</p>"},{"location":"ldap/#ldap-configuration","title":"LDAP Configuration","text":"<p>To have a better understanding of what this configuration implies, you can refer to the Architecture Overview part.</p>"},{"location":"ldap/#the-helm-values-file","title":"The helm values file","text":"<p>LDAP activation and configuration can be performed by adding specifics values during Helm (re)deployment.</p> <p>Here is a template of such values: </p> <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\n# The host and port of the LDAP server.\n# If port isn't supplied, it will be guessed based on the TLS configuration. 389 or 636.\nhost:\nport:\n# Timeout on connection to ldap server. Default to 10\ntimeoutSec: 10\n# Required if LDAP host does not use TLS.\ninsecureNoSSL: false\n# Don't verify the CA.\ninsecureSkipVerify: false\n# Connect to the insecure port then issue a StartTLS command to negotiate a\n# secure connection. If not supplied secure connections will use the LDAPS protocol.\nstartTLS: false\n# Path to a trusted root certificate file, or Base64 encoded PEM data containing root CAs.\nrootCaPath:\nrootCaData:\n# If server require client authentication with certificate.\n#  Path to a client cert file and a private key file\nclientCert:\nclientKey:\n# BindDN and BindPW for an application service account. The connector uses these\n# credentials to search for users and groups.\nbindDN:\nbindPW:\nuserSearch:\n# BaseDN to start the search from. For example \"cn=users,dc=example,dc=com\"\nbaseDN:\n# Optional filter to apply when searching the directory. For example \"(objectClass=person)\"\nfilter:\n# Attribute to match against the login. This will be translated and combined\n# with the other filter as \"(&lt;loginAttr&gt;=&lt;login&gt;)\".\nloginAttr:\n#  Can either be:\n# * \"sub\" - search the whole sub tree (Default)\n# * \"one\" - only search one level\nscope: \"sub\"\n# The attribute providing the numerical user ID\nnumericalIdAttr:\n# The attribute providing the user's email\nemailAttr:\n# The attribute providing the user's common name\ncnAttr:\ngroupSearch:\n# BaseDN to start the search from. For example \"cn=groups,dc=example,dc=com\"\nbaseDN:\n# Optional filter to apply when searching the directory. For example \"(objectClass=posixGroup)\"\nfilter: (objectClass=posixgroup)\n# Defaults to \"sub\"\nscope: \"sub\"\n# The attribute of the group that represents its name.\nnameAttr: cn\n# The filter for group/user relationship will be: (&lt;linkGroupAttr&gt;=&lt;Value of LinkUserAttr for the user&gt;)\n# If there is several value for LinkUserAttr, we will loop on.\nlinkGroupAttr:\nlinkUserAttr:\n</code></pre> <ul> <li><code>skMerge</code> is the SKAS module aimed to merge information from several Identity Provider. This merge obey to some rules which will be described here.</li> <li><code>skMerge.providers</code> is the ordered list of Identity Providers. </li> <li><code>crd</code> is the name of the provider managing the user database in the <code>skas-system</code> namespace.</li> <li><code>ldap</code> is the name of our LDAP provider, which will be configured under the <code>skLdap</code> subsection.</li> <li><code>skLdap.enabled</code> must be set to <code>true</code> (It is <code>false</code> by default).</li> <li><code>skLdap.ldap.*</code> is the LDAP configuration with all parameters and their description.</li> </ul> <p>To apply this configuration:</p> <pre><code>$ helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.ldap.yaml\n</code></pre> <p>Don't forget to add the <code>values.init.yaml</code>, or to merge it in the <code>values.ldap.yaml</code> file. Also, if you have others values file, they must be added on each upgrade</p> <p>And don't forget to restart the pod(s). See Configuration: Pod restart</p> <p>In this configuration, there is two source of identity: Our original <code>skas-system</code> user database and the newly added <code>ldap</code> server.  How these two sources are merged is the object of the next chapter. </p> <p>Once deployed, you can test your configuration using a <code>kubectl sk user describe &lt;login&gt; --explain</code> command. See the next chapter for more information</p>"},{"location":"ldap/#sample-configurations","title":"Sample configurations","text":"<p>Here is a sample of configuration, aimed to connect to an OpenLDAP server</p> <pre><code>$ cat &gt;./values.ldap.yaml &lt;&lt;\"EOF\"\nskMerge:\n  providers:\n    - name: crd\n    - name: ldap\n\nskLdap:\n  enabled: true\n  # --------------------------------- LDAP configuration\n  ldap:\n    host: ldap.mydomain.internal\n    insecureNoSSL: false\n    rootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\n    bindDN: cn=Manager,dc=mydomain,dc=internal\n    bindPW: admin123\n    groupSearch:\n      baseDN: ou=Groups,dc=mydomain,dc=internal\n      filter: (objectClass=posixgroup)\n      linkGroupAttr: memberUid\n      linkUserAttr: uid\n      nameAttr: cn\n    timeoutSec: 10\n    userSearch:\n      baseDN: ou=Users,dc=mydomain,dc=internal\n      cnAttr: cn\n      emailAttr: mail\n      filter: (objectClass=inetOrgPerson)\n      loginAttr: uid\n      numericalIdAttr: uidNumber\nEOF\n</code></pre> <p>Note that, as the connection is using SSL, there is a need to provide a Certificate Authority.  Such CA is provided here in <code>skLdap.ldap.rootCaData</code>, as a base64 encoded certificate file.</p> <p>And here is a sample of configuration, aimed to connect to an FreeIPA LDAP server</p> <pre><code>$ cat &gt;./values.ldap.yaml &lt;&lt;\"EOF\"\nskMerge:\n  providers:\n    - name: crd\n    - name: ldap\n\nskLdap:\n  enabled: true\n  # --------------------------------- LDAP configuration\n  ldap:\n    host: ipa1.mydomain.internal\n    port: 636\n    rootCaData: \"LS0tLS1CRUdJTiBDRU4zclBySE.........................JRklDQVRFLS0tLS0K\"\n    bindDN: uid=admin,cn=users,cn=accounts,dc=mydomain,dc=internal\n    bindPW: ipaadmin\n    userSearch:\n      baseDN: cn=users,cn=accounts,dc=mydomain,dc=internal\n      emailAttr: mail\n      filter: (objectClass=inetOrgPerson)\n      loginAttr: uid\n      numericalIdAttr: uidNumber\n      cnAttr: cn\n    groupSearch:\n      baseDN: cn=groups,cn=accounts,dc=mydomain,dc=internal\n      filter: (objectClass=posixgroup)\n      linkGroupAttr: member\n      linkUserAttr: DN\n      nameAttr: cn\nEOF\n</code></pre> <p>Trick: To get the <code>rootCaData</code> from a FreeIPA server, log on this server and:</p> <pre><code>$ cd /etc/ipa\n$ cat ca.crt  | base64 -w0\n</code></pre>"},{"location":"ldap/#setup-ldap-ca-in-a-configmap","title":"Setup LDAP CA in a configMap","text":"<p>The <code>rootCaData</code> attribute could be a quite long string, which can be troublesome.  An alternate solution is to store this in a configMap.</p> <p>First, create the configMap with the CA file: </p> <pre><code>kubectl -n skas-system create configmap ldap-ca.crt --from-file=./CA.crt\n</code></pre> <p>Then modify the values file as the following:</p> <pre><code>skMerge:\n  providers:\n    - name: crd\n    - name: ldap\n\nskLdap:\n  enabled: true\n\n  extraConfigMaps:\n    - configMap: ldap-ca.crt\n      volume: ldap-ca\n      mountPath: /tmp/ca/ldap\n\n  # --------------------------------- LDAP configuration\n  ldap:\n    host: ldap.ops.scw01\n    insecureNoSSL: false\n    rootCaPath: /tmp/ca/ldap/CA.crt\n    .....\n</code></pre> <p>The <code>skLdap.extraConfigMaps</code> subsection instruct the POD to mount this configMap to the defined location. The property <code>skLdap.ldap.rootCaPath</code> can now refer to the mounted value. Of course <code>skLdap.ldap.rootCaData</code> should be removed.</p> <p>To apply this configuration:</p> <pre><code>$ helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.ldap.yaml\n</code></pre> <p>And don't forget to restart the pod(s). See Configuration: Pod restart</p>"},{"location":"toolsandtricks/","title":"Tools and Tricks","text":""},{"location":"toolsandtricks/#reloader","title":"reloader","text":"<p>Forgetting to restart a POD after a configuration change is a common source of errors. Fortunately, some tools can  help for this. Such as Reloader</p> <p>The SKAS Helm chart add appropriate annotations on the <code>deployment</code>:  </p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    configmap.reloader.stakater.com/reload: skas-merge-config,skas-auth-config,skas-crd-config,\n</code></pre> <p>The list of <code>configMap</code> is built dynamically by the Helm chart.</p> <p>Of course, if Reloader is not installed in your cluster, this annotation will have no effect.</p>"},{"location":"toolsandtricks/#secret-generator","title":"Secret generator","text":"<p>As stated in Two LDAP servers configuration o Delegated users management, there is the need to generate  a random secret in the deployment. For this, one can use kubernetes-secret-generator, a custom kubernetes controller.</p> <p>Here is a manifest which, once applied, will create the secret <code>skas2-client-secret</code> used the authenticate the communication between the two PODs of the two LDAP configuration referenced above.  </p> <pre><code>---\napiVersion: \"secretgenerator.mittwald.de/v1alpha1\"\nkind: \"StringSecret\"\nmetadata:\nname: skas2-client-secret\nnamespace: skas-system\nspec:\nfields:\n- fieldName: \"clientSecret\"\nencoding: \"base64\"\nlength: \"15\"\n</code></pre>"},{"location":"toolsandtricks/#k9s","title":"k9s","text":"<p>We would like to say two words about this great tool which is k9s</p> <p>As it is able to handle Custom Resources Definition out of the box, K9s is a perfect tool to dynamically display, modify or delete SKAS resources.</p> <p>Note than, as User and Group are ambiguous names, which are used also by others API, alias are provided to ensure ambiguous access.</p> <p>For example, you can access this screen under <code>skusers</code> resource name:</p> <p></p> <p>This one using <code>groupbindings</code>:</p> <p></p> <p>This one using <code>tokens</code>:</p> <p></p> <p>Of course, k9s can't do more than what the launching user is allowed to do. This user can be authenticated using SKAS, but it must have a minimum set of rights to behave correctly.</p> <p>For example, you can launch k9s under the <code>admin</code> user account we have set up in the installation process (Provided it is member of the <code>system:masters</code> group).</p> <pre><code>$ kubectl sk login admin\nPassword:\nlogged successfully..\n\n$ k9s\n....\n</code></pre>"},{"location":"toolsandtricks/#kubernetes-dashboard","title":"Kubernetes dashboard","text":"<p>Login to the Kubernetes dashboard with SKAS is quite easy.</p> <p>First, you must be logged using the CLI. Then using the <code>--all</code> option of the <code>kubectl sk whoami</code> command, you can get your current allocated token:</p> <pre><code>$ kubectl sk login admin\nPassword:\nlogged successfully..\n\n$ kubectl sk whoami --all\nUSER    ID   GROUPS                      AUTH.   TOKEN\nadmin   0    skas-admin,system:masters   crd     znitotnewjbqbuolqacckvgxyhptoxsuykznrzdacuvdhimy\n</code></pre> <p>Now, you just have to cut and paste the token value in the dashboard login screen:</p> <p></p> <p>Of course, the set of operation you will be able to perform through the dashboard will be limited by the logged user's permissions.</p>"},{"location":"toolsandtricks/#tricks-setup-a-meta-helm-chart","title":"Tricks: Setup a meta helm chart","text":"<p>In Two LDAP servers configuration and Delegated users management, we had setup our configuration by performing two closely related Helm deployment.</p> <p>To ease automation, it could be useful to 'package' such kind of deployment by creating a 'meta chart', a chart which will embed other ones as dependencies.</p> <p>Such chart will have the following layout.</p> <pre><code>$ tree\n.\n|-- Chart.yaml\n|-- templates\n|   `-- stringsecret.yaml\n`-- values.yaml\n</code></pre> <p>This example will implement encryption and inter-pod authentication.</p> <p>The <code>Chart.yaml</code> file define the meta-chart <code>skas-skas2-meta</code>.There is two dependencies deploying the same helm chart,  but with different values (See below). Note the <code>alias: skas2</code> on the second deployment.</p> Chart.yaml <pre><code>apiVersion: v2\nname: skas-skas2-meta\nversion: 0.1.0\ndependencies:\n- name: skas\nversion: 0.2.1\nrepository: https://skasproject.github.io/skas-charts\n- name: skas\nalias: skas2\nversion: 0.2.1\nrepository: https://skasproject.github.io/skas-charts\n</code></pre> <p>The following will generate the shared secret allowing inter-pods authentication </p>  templates/stringsecret.yaml <pre><code>---\napiVersion: \"secretgenerator.mittwald.de/v1alpha1\"\nkind: \"StringSecret\"\nmetadata:\nname: skas2-client-secret\nnamespace: skas-system\nspec:\nfields:\n- fieldName: \"clientSecret\"\nencoding: \"base64\"\nlength: \"15\"\n</code></pre> <p>And here is the <code>values.yaml</code> file, with one version for the Two LDAP servers configuration:</p> values.yaml <pre><code># ======================================================== Main SKAS Pod configuration\nskas:\nclusterIssuer: cluster-issuer1\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.kspray6\nkubeconfig:\ncontext:\nname: skas@kspray6\ncluster:\napiServerUrl: https://kubernetes.ingress.kspray6\nskMerge:\nproviders:\n- name: crd\n- name: ldap1\ngroupPattern: \"dep1_%s\"\n- name: ldap2\ngroupPattern: \"dep2_%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\nldap1:\nurl: http://localhost:7013\nldap2:\nurl: https://skas-skas2-ldap.skas-system.svc # Was https://skas2-ldap.skas-system.svc\nrootCaPath: /tmp/cert/ldap2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: ${LDAP2_CLIENT_SECRET}\nextraEnv:\n- name: LDAP2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: ldap2-client-secret\nkey: clientSecret\nextraSecrets:\n- secret: skas-skas2-ldap-cert # Was skas2-ldap-cert\nvolume: ldap2-cert\nmountPath: /tmp/cert/ldap2\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap1.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain1,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain1,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# ======================================================== SKAS2 Pod configuration\nskas2:\nskAuth:\nenabled: false\nskMerge:\nenabled: false\nskCrd:\nenabled: false\nclusterIssuer: cluster-issuer1\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap2.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain2,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain2,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7113\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: skMerge\nsecret: ${LDAP2_CLIENT_SECRET}\nprotected: true\nextraEnv:\n- name: LDAP2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: ldap2-client-secret\nkey: clientSecret\n</code></pre> <p>and one for Delegated users management.</p> values.yaml <pre><code>skas:\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.kspray6\nkubeconfig:\ncontext:\nname: skas@kspray6\ncluster:\napiServerUrl: https://kubernetes.ingress.kspray6\nskMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: https://skas-skas2-crd.skas-system.svc # Was https://skas2-crd.skas-system.svc\nrootCaPath: /tmp/cert/skas2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: ${SKAS2_CLIENT_SECRET}\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\nextraSecrets:\n- secret: skas-skas2-crd-cert  # Was skas2-crd-cert\nvolume: skas2-cert\nmountPath: /tmp/cert/skas2\nskas2:\nskAuth:\nenabled: false\nskMerge:\nenabled: false\nskLdap:\nenabled: false\nclusterIssuer: cluster-issuer1\nskCrd:\nenabled: true\nnamespace: dep1-userdb\nadminGroups:\n- dep1-admin\ninitialUser:\nlogin: dep1-admin\npasswordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\ncommonNames: [ \"DEP1 administrator\" ]\ngroups:\n- admin\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7112\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"skMerge\"\nsecret: ${SKAS2_CLIENT_SECRET}\nprotected: true\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\n</code></pre> <p>There is two blocks: <code>skas</code> and <code>skas2</code>, matching the name or alias in the <code>Chart.yaml</code> file.</p> <p>These two block hold the same definition than the ones defined in the original configuration. With two differences:</p> <ul> <li><code>skas.skMerge.providerInfo.ldap2.url: https://skas-skas2-ldap.skas-system.svc</code>or <code>skas.skMerge.providerInfo.crd_dep1.url: https://skas-skas2-crd.skas-system.svc</code></li> <li><code>skas.skMerge.extraSecrets[0].secret: skas-skas2-ldap-cert</code>  or <code>skas.skMerge.extraSecrets[0].secret: skas-skas2-crd-cert</code></li> </ul> <p>This to accommodate service and secret name change, due to aliasing of the second dependency.</p> <p>Then, to launch the deployment, in the same folder as <code>Chart.yaml</code>, execute:</p> <pre><code>$ helm dependency build &amp;&amp; helm -n skas-system upgrade -i skas .\n</code></pre>"},{"location":"toolsandtricks/#tricks-handle-two-different-sessions","title":"Tricks: Handle two different sessions","text":"<p>When working on user permissions, it could be useful to have separate session, at least one as admin, and one as a user to test its capability.</p> <p>But the default Kubernetes configuration is not bound to a terminal session, but to a user.  So, any modification (<code>kubectl config ....</code>) of the local configuration will have effect on all session.</p> <p>The solution is to change the location of the kubernetes configuration for a given session, by modifying the <code>KUBECONFIG</code> environment variable: </p> <pre><code>$ export KUBECONFIG=/tmp/kconfig\n</code></pre> <p><code>/tmp/kconfig</code> may be an empty or un-existing file</p> <p>Then you can initialize a new Kubernetes/SKAS context</p> <pre><code>$ kubectl sk init https://skas.ingress.mycluster.internal\nSetup new context 'skas@mycluster.internal' in kubeconfig file '/tmp/kconfig'\n</code></pre>"},{"location":"twoldapservers/","title":"Two LDAP servers configuration","text":""},{"location":"twoldapservers/#adding-a-second-ldap-server","title":"Adding a second LDAP server","text":"<p>As the two configurations are quite similar, there is a lot of redundancy between this chapter and Delegated users management chapter</p> <p></p> <p>The obvious solution to add a second LDAP server would be to duplicate the <code>skLdap</code> Identity Provider container, hook it under the <code>skMerge</code> module and link it to our second LDAP server.</p> <p>Unfortunately, the provided SKAS helm chart does not allow several instance of whatever container. Doing so would be possible, but at the price of a more complex chart.</p> <p>A possible solution would be to modify the chart by adding this second LDAP IDP. But such specifics forks should be avoided if possible.</p> <p></p> <p></p> <p>A more convenient solution will be to create another POD hosting only the new LDAP IDP.</p> <p>This pod will be instantiated using the same SKAS helm chart, under another release name. And all other modules than <code>skLdap</code> will be disabled.</p> <p>When all modules are in the same POD, inter-module communication use the <code>localhost</code> internal POD network. In this case, as the second <code>skLdap</code> module run in another POD, communication must go through a kubernetes service.</p> <p>This configuration requires two steps:</p> <ul> <li>Setup a new Helm deployment for <code>skas2</code> pod.</li> <li>Reconfigure the <code>skMerge</code> module of the main SKAS pod to connect to this new IDP.</li> </ul> <p></p> <p>In the following, three variants of this configuration will be described. One with the connection in clear text, and two secured, with network encryption and inter-pod authentication.</p>"},{"location":"twoldapservers/#clear-text-connection","title":"Clear text connection","text":""},{"location":"twoldapservers/#auxiliary-pod-configuration","title":"Auxiliary POD configuration","text":"<p>Here is a sample values file to configure the auxiliary POD:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskCrd:\nenabled: false\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap2.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain2,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain2,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7113\nssl: false\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"*\"\nsecret: \"*\"\nprotected: true\n</code></pre> <p>At the beginning of the file, we disable all other modules than <code>skLdap</code>.</p> <p>Then there is the connection to the second LDAP server which must be adjusted to your context. Refer to LDAP Setup</p> <p>Then, there is the <code>exposure</code> part, who define how this service will be exposed. (The default configuration is expose to <code>localhost</code> in clear text)</p> <ul> <li><code>exposure.internal.enabled: false</code> shutdown the HTTP server bound on localhost.</li> <li><code>exposure.external.enabled: true</code> set the HTTP server bound on the POD IP up. This on port 7113 with no SSL.</li> <li>Then the is the configuration of the <code>identity</code> service to expose:<ul> <li><code>clients[]</code> is a mechanism to validate who is able to access this service, by providing an <code>id</code> and a <code>secret</code>   (or password). the values \"*\" will disable this feature.</li> <li><code>protected: true</code> activate an internal mechanism against attacks of type 'Brut force', by introducing delays on   unsuccessful connection attempts, and by limiting the number of simultaneous connections. There is no reason to   disable it, except if you suspect a misbehavior.</li> </ul> </li> </ul> <p>To deploy this configuration:</p> <pre><code>helm -n skas-system install skas2 skas/skas --values ./values.skas2.yaml\n</code></pre> <p>Note the `skas2' release name</p> <p>The Helm chart will deploy the new pod(s), under the name <code>skas2</code>. it will also deploy an associated Kubernetes <code>service</code>.</p>"},{"location":"twoldapservers/#main-pod-reconfiguration","title":"Main pod reconfiguration","text":"<p>Second step is to reconfigure the main POD</p> <p>Here is a sample of appropriate configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap1\ngroupPattern: \"dep1_%s\"\n- name: ldap2\ngroupPattern: \"dep2_%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\nldap1:\nurl: http://localhost:7013\nldap2:\nurl: http://skas2-ldap.skas-system.svc\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap1.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain1,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain1,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n</code></pre> <p>There is two entries aimed to configure a provider on the bottom of the <code>skMerge</code> module:</p> <ul> <li><code>providers</code> is a list of the connected providers, which allow to define their behavior. The order is important here.   Refers to the IDP chaining: Provider configuration chapter.</li> <li><code>providerInfo</code> is a map providing information on how to reach these providers.For <code>crd</code> and <code>ldap1</code>, we use the   default <code>localhost</code> port.For <code>ldap2</code> we use the service created by the <code>skas2</code> deployment.</li> </ul> <p>The link between these two entries is of course the provider name.</p> <p>Then there is the configuration for the primary LDAP server, which must be adapted to your configuration.</p> <p>Then, the reconfiguration must be applied:</p> <pre><code>$ helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.skas.yaml\n</code></pre> <p>Don't forget to add the <code>values.init.yaml</code>, or to merge it in the <code>values.skas.yaml</code> file. Also, if you have others values file, they must be added on each upgrade</p> <p>And don't forget to restart the pod(s). See Configuration: Pod restart</p> <p>If deploying two separate Charts is a constraint for you, you may setup a 'meta chart'. See here</p>"},{"location":"twoldapservers/#test","title":"Test","text":"<p>Then, you can now test your configuration:</p> <pre><code>$ kubectl sk user describe nobody  --explain\nUSER     STATUS         UID   GROUPS   EMAILS   COMMON NAMES   AUTH\nnobody   userNotFound   0\nDetail:\nPROVIDER   STATUS         UID   GROUPS   EMAILS   COMMON NAMES\ncrd        userNotFound   0\nldap1      userNotFound   0\nldap2      userNotFound   0\n</code></pre> <p>You can check than both ldap server are taken in account. This also ensure connection to both LDAP server are effective, as a provider is <code>critical</code> by default (Refers to the IDP chaining: Provider configuration chapter).</p>"},{"location":"twoldapservers/#securing-connection","title":"Securing connection","text":"<p>It should ne noted than unencrypted passwords will transit through the link between the two pods. So, setting up encryption is a must have.</p>"},{"location":"twoldapservers/#auxiliary-pod-configuration_1","title":"Auxiliary POD configuration","text":"<p>Here is the modified version for the <code>skas2</code> pod configuration:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskCrd:\nenabled: false\nclusterIssuer: your-cluster-issuer\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap2.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain2,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain2,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7113\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: skMerge\nsecret: aSharedSecret\nprotected: true\n</code></pre> <p>The differences are the following:</p> <ul> <li>There is a <code>clusterIssuer</code> definition to be able to generate a certificate. (It is assumed here than <code>cert-manager</code> is deployed in the cluster)</li> <li><code>exposure.external.ssl</code> is set to <code>true</code>. This will also leads the generation of the server certificate.</li> <li>The <code>service.identity.clients</code> authentication is also activated. The <code>id</code> and <code>secret</code> values will have to be provided by the <code>skMerge</code> client.</li> </ul> <p>To deploy this configuration:</p> <pre><code>helm -n skas-system install skas2 skas/skas --values ./values.skas2.yaml\n</code></pre> <p>Note the `skas2' release name</p> <p>The Helm chart will deploy the new pod(s), under the name <code>skas2</code>. it will also deploy an associated Kubernetes <code>service</code> and the <code>cert-manager.io/v1/Certificate</code> request.</p>"},{"location":"twoldapservers/#main-pod-reconfiguration_1","title":"Main pod reconfiguration","text":"<p>Here is the modified version for the main SKAS POD configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap1\ngroupPattern: \"dep1_%s\"\n- name: ldap2\ngroupPattern: \"dep2_%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\nldap1:\nurl: http://localhost:7013\nldap2:\nurl: https://skas2-ldap.skas-system.svc\nrootCaPath: /tmp/cert/ldap2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: aSharedSecret\nextraSecrets:\n- secret: skas2-ldap-cert\nvolume: ldap2-cert\nmountPath: /tmp/cert/ldap2\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap1.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain1,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain1,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n</code></pre> <p>The <code>providerInfo.ldap2</code> has been modified for SSL and authenticated connection:</p> <ul> <li><code>url</code> begins with <code>https</code>.</li> <li><code>clientAuth</code> provides information to authenticated against the <code>skLdap2</code> pod.</li> <li><code>insecureSkipVerify</code> is set to false, as we want to check certificate validity.</li> <li><code>rootCaPath</code> is set to access the <code>ca.crt</code>, the CA validating the <code>skLdap2</code> server certificate.</li> </ul> <p>As stated above, during the deployment of the <code>skas2</code> auxiliary POD, a server certificate has been generated to allow SSL enabled services. This certificate is stored in a secret (of type <code>kubernetes.io/tls</code>) named <code>skas2-ldap-cert</code>. Alongside the private/public key pair, it also contains the root Certificate authority under the name<code>ca.crt</code>.</p> <p>The <code>skMerge.extraSecrets</code> subsection instruct the POD to mount this secret to the defined location. The property <code>skMerge.providerInfo.ldap2.rootCaPath</code> can now refer to the mounted value.</p> <p>Then, the reconfiguration must be applied:</p> <pre><code>$ helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.skas.yaml\n</code></pre> <p>Don't forget to add the <code>values.init.yaml</code>, or to merge it in the <code>values.skas.yaml</code> file. Also, if you have others values file, they must be added on each upgrade</p> <p>And don't forget to restart the pod(s). See Configuration: Pod restart</p> <p>You can now test again your configuration, as described above</p>"},{"location":"twoldapservers/#use-a-kubernetes-secrets","title":"Use a Kubernetes secrets","text":"<p>There is still a security issue, as the shared secret (<code>aSharedSecret</code>) is in clear text in both values file. As such it may ends up in some version control system.</p> <p>The good practice will be to store the secret value in a kubernetes <code>secret</code> resource, such as:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: skas2-client-secret\nnamespace: skas-system\ndata:\nclientSecret: Sk1rbkNyYW5WV1YwR0E5\ntype: Opaque\n</code></pre> <p>Where <code>data.clientSecret</code> is the secret encoded in base 64.</p> <p>There is several solution to generate such secret value. One can use Helm with some random generator function. Or use a Secret generator</p>"},{"location":"twoldapservers/#auxiliary-pod-configuration_2","title":"Auxiliary POD configuration","text":"<p>To use this secret, here is the new modified version for the <code>skas2</code> POD configuration:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskCrd:\nenabled: false\nclusterIssuer: your-cluster-issuer\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap2.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain2,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain2,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7113\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: skMerge\nsecret: ${SKAS2_CLIENT_SECRET}\nprotected: true\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\n</code></pre> <p>The modifications are the following:</p> <ul> <li>The <code>skLdap.extraEnv</code> subsection inject the secret value as an environment variable in the container.</li> <li>the <code>exposure.external.services.identity.clients[0].secret</code> fetch its value through this environment variable.</li> </ul> <p>Most of the values provided by the helm chart ends up inside a configMap, which is then loaded by the SKAS executable. The environment variable interpolation occurs during this load.</p>"},{"location":"twoldapservers/#main-pod-reconfiguration_2","title":"Main pod reconfiguration","text":"<p>Here is the modified version, with <code>secret</code> handling, for the main SKAS pod configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap1\ngroupPattern: \"dep1_%s\"\n- name: ldap2\ngroupPattern: \"dep2_%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\nldap1:\nurl: http://localhost:7013\nldap2:\nurl: https://skas2-ldap.skas-system.svc\nrootCaPath: /tmp/cert/ldap2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: ${SKAS2_CLIENT_SECRET}\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\nextraSecrets:\n- secret: skas2-ldap-cert\nvolume: ldap2-cert\nmountPath: /tmp/cert/ldap2\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap1.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain1,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain1,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n</code></pre> <p>The modifications are the same as the SKAS2 POD</p>"},{"location":"userguide/","title":"User guide","text":""},{"location":"userguide/#local-client-configuration","title":"Local client configuration","text":"<p>It is assumed here than <code>kubectl</code> is installed. (If not, see here)</p> <p>It is also assumed the <code>kubectl-sk</code> CLI extension has been installed (If not, see here)</p> <p>For accessing a kubernetes cluster with kubectl, you need a configuration file (By default in <code>&lt;homedir&gt;/.kube/config</code>).</p> <p>SKAS provide a mechanism to create or update this user's configuration file.</p> <pre><code>$ kubectl sk init https://skas.ingress.mycluster.internal\nSetup new context 'skas@mycluster.internal' in kubeconfig file '/Users/john/.kube/config'\n</code></pre> <p>You can validate this new context is now the current one:</p> <pre><code>$ kubectl config current-context\nskas@mycluster.internal\n</code></pre>"},{"location":"userguide/#got-a-certificate-issue","title":"Got a certificate issue ?","text":"<p>If your system is not configured with the CA which has been used to certify SKAS (cf the <code>clusterIssuer</code> parameter on initial installation), you will encounter an error like:</p> <pre><code>ERRO[0000] error on GET kubeconfig from remote server  error=\"error on http connection: Get \\\"https://skas.ingress.mycluster.internal/v1/kubeconfig\\\": \n tls: failed to verify certificate: x509: certificate signed by unknown authority\"\n</code></pre> <p>You may get rid of this error by providing the root CA certificate as a file:</p> <pre><code>$ kubectl sk init https://skas.ingress.mycluster.internal --authRootCaPath=./CA.crt\n</code></pre> <p>Provided you are a kubernetes system administrator, here is how you can get this CA.crt file:</p> <pre><code>$ kubectl -n skas-system get secret skas-auth-cert -o=jsonpath='{.data.ca\\.crt}' | base64 -d &gt;./CA.crt\n</code></pre> <p>If you are unable to get such CA certificate, you can skip the test by setting a flag:</p> <pre><code>$ kubectl sk init --authInsecureSkipVerify=true https://skas.ingress.mycluster.internal\n</code></pre> <p>This is a security breach, as the target site can be a fake one. Use this flag should be limited to initial evaluation context.</p>"},{"location":"userguide/#basic-usage","title":"Basic usage","text":""},{"location":"userguide/#login-with-default-admin-account","title":"login with default admin account","text":"<p>SKAS manage a local users database, where users are stored as Kubernetes resources.</p> <p>For convenience, a first <code>admin</code> user has been created during the installation.  With password <code>admin</code></p> <p>By default, SKAS users are stored in the namespace <code>skas-system</code>.</p> <p>You could list them, using standard kubectl commands. If you have configured your client as described above, you now  have to be logged to perform any kubectl action. So the login/password interaction</p> <pre><code>$ kubectl -n skas-system get users.userdb.skasproject.io\nLogin:admin\nPassword:\nNAME    COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\nadmin   [\"SKAS administrator\"]\n</code></pre> <p>Several remarks:</p> <ul> <li>Default password is <code>admin</code>. DON'T FORGET TO CHANGE IT. See below.</li> <li>The <code>admin</code> user has been granted to access SKAS resources in <code>skas-system</code> namespace using kubernetes RBAC</li> <li><code>kubectl -n skas-system get users</code> may no works, as <code>users</code> refers also to a standard kubernetes resources.</li> </ul> <p>To ease SKAS user management, an alias <code>skuser</code> has been defined.</p> <pre><code>$ kubectl -n skas-system get skusers\nNAME    COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\nadmin   [\"SKAS administrator\"]\n</code></pre> <p>Note there is now no login/password interaction. A token has been granted during the first login. This token will expire after a delay of inactivity. (Like a Web session). This delay is 30mn by default.</p>"},{"location":"userguide/#logout-and-login","title":"logout and login","text":"<p>Once logged, you can use <code>kubectl</code> as usual. The token will be transparently used until it expire on inactivity.</p> <p>If expired, you will be prompted again to enter login and password.</p> <p>You can also logout at any time by using the command:</p> <pre><code>$ kubectl sk logout\n</code></pre> <p>Note the <code>sk</code> who instruct <code>kubectl</code> to forward the command to the <code>kubectl-sk</code> extension.</p> <p>Then, you will be prompted again for your login/password on the next <code>kubectl</code>command. </p> <p>You can also use explicit login:</p> <pre><code>$ kubectl sk login\nLogin:admin\nPassword:\nlogged successfully..\n</code></pre> <p>or</p> <pre><code>$ kubectl sk login admin\nPassword:\nlogged successfully..\n</code></pre> <p>or</p> <pre><code>$ kubectl sk login admin ${ADMIN_PASSWORD}\nlogged successfully..\n</code></pre> <p><code>sk login</code> perform an <code>sk logout</code> if logged.</p>"},{"location":"userguide/#password-change","title":"Password change","text":"<p>As stated above, you must change the password of this account:</p> <pre><code>$ kubectl sk password\nWill change password for user 'admin'\nOld password:\nNew password:\nConfirm new password:\nPassword has been changed sucessfully.\n</code></pre> <p>Note the <code>sk</code>, as such command is performed by the SKAS kubectl extension.</p> <p>There is a check about password strength. So, you may have such response:</p> <pre><code>$ kubectl sk password\nWill change password for user 'admin'\nOld password:\nNew password:\nConfirm new password:\nUnsatisfactory password strength!\n</code></pre> <p>There is no well defined password criteria (such as length, special character, etc...).  An algorithm provide a score for the password, and this score must match a minimum (configurable) value. There is also a check against a list of commonly used passwords.</p> <p>The easiest way to overcome this restriction is to increase your password length.</p>"},{"location":"userguide/#skas-group-binding","title":"SKAS group binding","text":"<p>In fact, what has been granted to access SKAS resources is not the admin account (It could be), but a group named <code>skas-system</code>.</p> <p>And the user <code>admin</code> has been included in the group by another SKAS resources named <code>groupbindings.userdb.skasproject.io</code>, with <code>groupbindings</code>as an alias:</p> <pre><code>$ kubectl -n skas-system get groupBindings\nNAME               USER    GROUP\nadmin-skas-admin   admin   skas-admin\n</code></pre> <p>In kubernetes, a group does not exist as a concrete resources. It only exists as it is referenced by RBAC <code>roleBinding</code> or <code>clusterRoleBindigs</code>. Or by SKAS <code>groupBinding</code></p>"},{"location":"userguide/#be-a-cluster-admin","title":"Be a cluster admin","text":"<p>Let's try the following:</p> <pre><code>$ kubectl get namespaces\nError from server (Forbidden): namespaces is forbidden: User \"admin\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope\n</code></pre> <p>It is clear than we are successfully authenticated as <code>admin</code>, but this account has no permissions to perform cluster-wide operation.</p> <p>Such permissions can be granted by binding this user to a group having such rights:</p> <pre><code>$ kubectl sk user bind admin system:masters\nGroupBinding 'admin.system.masters' created in namespace 'skas-system'.\n</code></pre> <p>For this to be effective, logout and login back:</p> <pre><code>$ kubectl sk logout\nBye!\n\n$ kubectl get namespaces\nLogin:admin\nPassword:\nNAME              STATUS   AGE\ncert-manager      Active   4d21h\ndefault           Active   4d21h\ningress-nginx     Active   4d21h\n.....\n</code></pre> <p>You can check the new <code>groupBindings</code> list:</p> <pre><code>$ kubectl -n skas-system get groupBindings\nNAME                   USER    GROUP\nadmin-skas-admin       admin   skas-admin\nadmin.system.masters   admin   system:masters\n</code></pre> <p>WARNING: This means any member of the group <code>skas-admin</code> can promote itself as a full cluster administrator.  In fact, anybody able to create or modify resources in the <code>skas-admin</code> namespace can take control of the cluster. So, access to this namespace should be strictly controlled </p> <p>Refer to Advanced configuration/Delegated user management to delegate user management without compromise cluster security. </p>"},{"location":"userguide/#issue-with-stdin","title":"Issue with <code>stdin</code>","text":"<p>If you issue a <code>kubectl</code> command which use <code>stdin</code> as input, you may encounter the following error message:</p> <pre><code>$ cat mymanifest.yaml | kubectl apply -f -\nLogin:\nUnable to access stdin to input login. Try login with `kubectl sk login' or 'kubectl-sk login'.` and issue this command again\n\nUnable to connect to the server: getting credentials: exec: executable kubectl-sk failed with exit code 18\n</code></pre> <p>This occurs if you token is expired. There is a conflict on <code>stdin</code> usage for entering your login/password.</p> <p>Solution is to ensure to be logged before issuing such command:</p> <pre><code>$ kubectl sk login\nLogin:oriley\nPassword:\nlogged successfully..\n\n[d3] m64:addons sa$ cat mymanifest.yaml | kubectl apply -f -\npod/mypod created\n</code></pre>"},{"location":"userguide/#cli-users-management","title":"CLI users management","text":"<p>The SKAS kubectl extension plugin provide a <code>user</code> command with several subcommands</p> <p>You can have a complete list of such subcommands:</p> <pre><code>$ kubectl sk user --help\n.......\n</code></pre> <p>You must be logged as a member of the group <code>skas-admin</code> to be able to use this command.</p>"},{"location":"userguide/#create-a-new-user","title":"Create a new user","text":"<p>Here is an example of user's creation:</p> <pre><code>$ kubectl sk user create luser1 --commonName \"Local user1\" --email \"luser1@internal\" --password \"RtVksSuMgP5f\"\nUser 'luser1' created in namespace 'skas-system'.\n</code></pre> <p>The only mandatory parameters is the user's name:</p> <pre><code>$ kubectl sk user create luser2\nUser 'luser2' created in namespace 'skas-system'.\n</code></pre> <p>As there is no password provided, login for this user will be impossible</p> <p>A complete list of user's creation options can be displayed: </p> <pre><code>$ kubectl sk user create --help\nCreate a new user\n\nUsage:\n  kubectl-sk user create &lt;user&gt; [flags]\n\nFlags:\n      --comment string        User's comment\n      --commonName string     User's common name\n      --email string          User's email\n      --generatePassword      Generate and display a password\n  -h, --help                  help for create\n      --inputPassword         Interactive password request\n  -n, --namespace string      User's DB namespace (default \"skas-system\")\n      --password string       User's password\n      --passwordHash string   User's password hash (Result of 'kubectl skas hash')\n      --state string          User's state (enabled|disabled) (default \"enabled\")\n      --uid int               User's UID\n</code></pre> <p>Most of the options match a user's properties</p> <ul> <li><code>comment</code>, <code>commonName</code>, <code>email</code>, <code>uid</code> are just descriptive parameters, inspired from Unix user attributes.</li> <li><code>state</code> will allow to temporary disable a user.</li> </ul> <p>The <code>--namespace</code> allow to store the user resources in another namespace. See Advanced configuration/Delegated user management</p> <p>There is several options related to the password:</p> <ul> <li><code>--password</code>: The password is provided as a parameter on the command line.</li> <li><code>--inputPassword</code>: There will be a <code>Password: / Confirm password:</code> user interaction.</li> <li><code>--generatePassword</code>: A random password is generated and displayed.</li> <li><code>--passwordHash</code>: Provide the hash of the password, as it will be stored in the resource.    Use <code>kubectl sk hash</code> to generate the value. NB: Doing this way skip the check about password strength. </li> </ul>"},{"location":"userguide/#list-users","title":"List users","text":"<p>Users can be listed using standard <code>kubectl</code> commands:</p> <pre><code>$ kubectl -n skas-system get skusers\nNAME     COMMON NAMES             EMAILS                UID   COMMENT   DISABLED\nadmin    [\"SKAS administrator\"]\nluser1   [\"Local user1\"]          [\"luser1@internal\"]                   false\nluser2                                                                  false\n</code></pre>"},{"location":"userguide/#modify-user","title":"Modify user","text":"<p>A subcommand <code>patch</code> is provided to modify a user. As an example:</p> <pre><code>$ kubectl sk user patch luser2 --state=disabled\nUser 'luser2' updated in namespace 'skas-system'.\n\n$ kubectl -n skas-system get skuser luser2\nNAME     COMMON NAMES   EMAILS   UID   COMMENT   DISABLED\nluser2                                           true\n</code></pre> <p>Most of the options are the same as the <code>user create</code> subcommand. </p> <p>There is also a <code>--create</code> option which will allow user creation if it does not exists.</p>"},{"location":"userguide/#delete-user","title":"Delete user","text":"<p>Users can be deleted using standard <code>kubectl</code> commands:</p> <pre><code>$ kubectl -n skas-system delete skuser luser2\nuser.userdb.skasproject.io \"luser2\" deleted\n</code></pre>"},{"location":"userguide/#manage-users-groups-and-permissions","title":"Manage user's groups and permissions.","text":"<p>To illustrate how SKAS interact with Kubernetes RBAC, we will setup a simple example. We will:</p> <ul> <li>Create a namespace named <code>ldemo</code>.</li> <li>Create a role named <code>configurator</code> in this namespace to manage resources of type <code>configMaps</code>.</li> <li>Create a roleBinding between this role and a group named <code>ldemo-devs</code>.</li> <li>Add the user <code>luser1</code> to this group.</li> </ul> <p>We assume we are logged as 'admin' to perform theses tasks:</p> <p><pre><code>$ kubectl create namespace ldemo\nnamespace/ldemo created\n\n$ kubectl -n ldemo create role configurator --verb='*' --resource=configMaps\nrole.rbac.authorization.k8s.io/configurator created\n\n$ kubectl -n ldemo create rolebinding configurator-ldemo-devs --role=configurator --group=ldemo-devs\nrolebinding.rbac.authorization.k8s.io/configurator-ldemo-devs created\n\n$ kubectl sk user bind luser1 ldemo-devs\nGroupBinding 'luser1.ldemo-devs' created in namespace 'skas-system'.\n</code></pre> Now, we can test. First logout and login under <code>luser1</code>:</p> <pre><code>$ kubectl sk logout\nBye!\n\n$ kubectl sk login\nLogin:luser1\nPassword:\nlogged successfully..\n\n$ kubectl sk whoami\nUSER     ID   GROUPS\nluser1   0    ldemo-devs\n</code></pre> <p>Now ensure we can create a <code>configMap</code> and view it.:</p> <pre><code>$ kubectl -n ldemo create configmap my-config --from-literal=key1=config1\nconfigmap/my-config created\n\n$ kubectl -n ldemo get configmaps my-config -o yaml\napiVersion: v1\ndata:\n  key1: config1\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2023-07-11T14:56:27Z\"\nname: my-config\n  namespace: ldemo\n  resourceVersion: \"257983\"\nuid: ad55b282-9803-4688-b2df-a1c35f708313\n</code></pre> <p>Also, ensure we can delete it</p> <pre><code>$ kubectl -n ldemo delete configmap my-config\nconfigmap \"my-config\" deleted\n</code></pre> <p>Please, note than <code>roles</code> and <code>roleBindings</code> are namespaced resources while <code>users</code> and <code>groups</code> are cluster-wide resources.</p>"},{"location":"userguide/#kubernetes-rbac-referential-integrity","title":"Kubernetes RBAC referential integrity","text":"<p>Kubernetes does not check referential integrity when creating a resource referencing another one. For example, the following will works:</p> <pre><code>kubectl -n ldemo create rolebinding missing-integrity --role=unexisting-role --group=unexisting-group\nrolebinding.rbac.authorization.k8s.io/missing-integrity created\n\n$ kubectl sk user bind unexisting-user unexisting-group\nGroupBinding 'unexisting-user.unexisting-group' created in namespace 'skas-system'.\n</code></pre> <p>May be the referenced resource will be created later. Or the link will be useless.</p> <p>This is clearly a design choice of Kubernetes. SKAS follow the same logic.</p>"},{"location":"userguide/#using-manifests-instead-of-cli","title":"Using Manifests instead of CLI","text":"<p>As users ans groups are defined as Kubernetes custom resources, they can be created and managed as any other resources, through manifests. </p> <p>By default, all SKAS users and groups resources are stored in the namespace <code>skas-system</code>.</p> <p>Kubernetes RBAC has been configured during installation to allow management of such resources by all members of the <code>skas-admin</code> group.</p>"},{"location":"userguide/#user-resources","title":"User resources","text":"<p>Here is the manifest corresponding to the users we created previously:</p> <pre><code>---\napiVersion: userdb.skasproject.io/v1alpha1\nkind: User\nmetadata:\nname: luser1\nnamespace: skas-system\nspec:\ncommonNames:\n- Local user1\nemails:\n- luser1@internal\npasswordHash: $2a$10$q6nEVmP.MHo6VLAprTdTBuy6AHPel1uh3NocZdNjt.yh8HDE7Ja.m\n</code></pre> <ul> <li>The resources name is the user login.</li> <li>The password is stored in a non reversible hash form. The command <code>kubectl sk hash</code> is provided to compute such hash.</li> </ul> <p>Below is a sample of a user with all properties defined:</p> <pre><code>---\napiVersion: userdb.skasproject.io/v1alpha1\nkind: User\nmetadata:\nname: jsmith\nnamespace: skas-system\nspec:\ncommonNames:  - John SMITH\npasswordHash: $2a$10$lnweus6Oe3/XMoRaIImnVOwmxZ.xMp7iRB3X1TOcszzHE8nxfiwJK  # Password: \"Xderghy12\"\nemails: - jsmith@mycompany.com\nuid: 100001\ncomment: A sample user\ndisabled: false </code></pre> <p>To define such user, save the yaml definition if a file and perform a <code>kubectl apply -f &lt;filaName&gt;</code></p> <p>Unfortunately, when logged using SKAS, it is impossible to use stdin on kubectl. So, <code>cat &lt;filename&gt; | kubectl apply -f -</code>  will not work. This is inherent to the way the kubernetes client-go credential plugin works</p>"},{"location":"userguide/#groupbinding-resources","title":"GroupBinding resources","text":"<p>The SKAS <code>GroupBinding</code> resources can also be defined as manifest:</p> <pre><code>---\napiVersion: userdb.skasproject.io/v1alpha1\nkind: GroupBinding\nmetadata:\nname: luser1.ldemo-devs\nnamespace: skas-system\nspec:\ngroup: ldemo-devs\nuser: luser1\n</code></pre>"},{"location":"userguide/#session-management","title":"Session management","text":""},{"location":"userguide/#view-active-sessions","title":"View active sessions","text":"<p>En each user login, a token is generated. This token will expire after a delay of inactivity. (Like a Web session). This delay is 30mn by default.</p> <p>On server side, the SKAS tokens are also stored as Kubernetes custom resources, in the namespace <code>skas-system</code>.  And RBAC has been configured to allow access by any member of the <code>skas-admin</code> group.  </p> <p>The SKAS tokens can be listed as any other kubernetes resources:</p> <pre><code>$ kubectl -n skas-system get tokens\nNAME                                               CLIENT   USER LOGIN   AUTH.   USER ID   CREATION               LAST HIT\nkhrvvqwvpotcufiltvymuumsvrodsbiuwypbzrjiqudzjthg            admin        crd     0         2023-07-12T08:23:36Z   2023-07-12T08:32:13Z\nltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd            luser1       crd     0         2023-07-12T08:27:19Z   2023-07-12T08:27:19Z\n</code></pre> <p>Each token represents an active user session. SKAS will remove it automatically after 30 minutes of inactivity by default.</p> <p>Also, there is a maximum token duration, which is set to 12 hours by default.</p> <p>A detailed view of each token can be displayed:</p> <pre><code>$ kubectl -n skas-system get tokens ltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd -o yaml\napiVersion: session.skasproject.io/v1alpha1\nkind: Token\nmetadata:\n  creationTimestamp: \"2023-07-12T08:27:19Z\"\ngeneration: 1\nname: ltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd\n  namespace: skas-system\n  resourceVersion: \"513150\"\nuid: 220471a2-2ec1-4b7f-af85-8647c4406343\nspec:\n  authority: crd\n  client: \"\"\ncreation: \"2023-07-12T08:27:19Z\"\nuser:\n    commonNames:\n    - Local user1\n    emails:\n    - luser1@internal\n    groups:\n    - ldemo-devs\n    login: luser1\n    uid: 0\nstatus:\n  lastHit: \"2023-07-12T08:27:19Z\"\n</code></pre>"},{"location":"userguide/#terminate-session","title":"Terminate session","text":"<p>To end a session, the corresponding token is to be deleted:</p> <pre><code>$ kubectl -n skas-system delete tokens ltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd\ntoken.session.skasproject.io \"ltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd\" deleted\n</code></pre> <p>Note there is a local cache of 30 seconds on the client side. So the session will remains active on this short (and configurable) delay.</p>"},{"location":"userguide/#others-kubectl-sk-commands","title":"Others <code>kubectl sk</code> commands","text":""},{"location":"userguide/#hash","title":"hash","text":"<p>This command compute the Hash value of a password. It is intended to be used when creating a user through a manifest.</p> <p>Note there is no password strength check doing this way.</p>"},{"location":"userguide/#init","title":"init","text":"<p>This command has been used at the beginning of this chapter. If you enter <code>kubectl sk init --help</code>, you can see there is some more options:</p> <ul> <li>Some are related to certificate management and was already mentioned.</li> <li>Some allow overriding of values provided by the server.</li> <li><code>clientId/Secret</code> is an optional method to restrict access to this command to users provided with these information. To be configured on the server.</li> </ul>"},{"location":"userguide/#login","title":"login","text":"<p>Perform the login/pasword interaction. </p> <p>Will also allow providing login and password on the command line. </p>"},{"location":"userguide/#logout","title":"logout","text":"<p>Logout the user, by deleting locally cached token.</p>"},{"location":"userguide/#password","title":"password","text":"<p>To change current user password. </p> <p>To change the password of another user, use the <code>kubectl sk user patch</code> command. Of course, you need to be member of the group <code>skas-admin</code> to do so.</p>"},{"location":"userguide/#whoami","title":"whoami","text":"<p>Display the currently logged user and the groups its belong to.</p>"},{"location":"userguide/#version","title":"version","text":"<p>Display the current version of this SKAS plugin</p>"},{"location":"userguide/#what-to-provide-to-other-kubernetes-users","title":"What to provide to other Kubernetes users","text":"<p>Here is a small checklist of what to provide to non-admin users to allow them to use kubectl on a SKAS enabled cluster.</p> <ul> <li>Obviously, instructions to install <code>kubectl</code>.</li> <li>Instructions to install <code>kubectl-sk</code></li> <li>If needed, the <code>CA.crt</code> certificate file</li> <li>The <code>kubectl sk init https://skas.....</code> command line.</li> <li>The namespace(s) they are allowed to access</li> </ul> <p>About this last point: You can instruct them to add the <code>--namespaceOverride</code> option on <code>kubectl sk init ...</code> command. This will define the provided namespace as the default one in the <code>~/.kube/config</code> file.</p>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SKAS: Simple Kubernetes Authentication System","text":"<p>SKAS is a powerful Kubernetes extension designed to streamline user authentication and authorization processes.  Whether you're managing a single cluster or a complex multi-cluster environment,  SKAS offers a seamless solution to handle user authentication, integrating seamlessly with Kubernetes CLI and  supporting a range of identity providers.</p>"},{"location":"#main-features","title":"Main Features","text":"<p>SKAS boasts an array of essential features to simplify and enhance your Kubernetes authentication and  authorization experience:</p> <ul> <li> <p>Kubernetes Authentication Webhook and kubectl Extension: SKAS provides a Kubernetes authentication webhook and an  extension for kubectl, ensuring a smooth Kubernetes CLI integration without the need for browser interactions.</p> </li> <li> <p>Custom Users and Groups: Define users and groups as Kubernetes Custom Resources, giving you fine-grained control  over access and permissions.</p> </li> <li> <p>DEX Connector: SKAS includes a DEX connector, making it compatible with all OIDC (OpenID Connect) aware  applications such as Argocd and Argo Workflows, enabling secure and straightforward integration.</p> </li> <li> <p>LDAP Integration: Support for one or several LDAP servers allows you to leverage existing identity  infrastructure seamlessly.</p> </li> </ul> <p></p> <ul> <li> <p>Unified User Information: SKAS allows you to combine user information from multiple sources, including LDAP,  local user databases, and more, providing a consolidated profile for users.</p> </li> <li> <p>Centralized User Management: In multi-cluster environments, SKAS simplifies user management,  ensuring consistency and control across all clusters.</p> </li> <li> <p>Delegated Management: Delegate user and group management to specific individuals or teams, enhancing  collaboration and reducing administrative burden.</p> </li> <li> <p>Flexible Architecture: SKAS is designed with flexibility in mind, accommodating sophisticated user management  requirements in even the most complex environments.</p> </li> <li> <p>ReadOnly LDAP/AD Access: SKAS can operate with ReadOnly access to LDAP/AD servers, allowing user profiles to be  enriched with local information, further enhancing the user experience.</p> </li> </ul> <p>With SKAS, you can ensure a secure, streamlined, and efficient authentication and authorization process,  enabling your Kubernetes clusters to operate at their full potential.</p> <p>This introduction provides a more comprehensive overview of SKAS and its main features. Depending on your requirements,  you can further expand on each feature with dedicated sections in your documentation. If you need assistance with any  specific sections or have more details to add, please let us know.</p>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":""},{"location":"architecture/#initial-deployment","title":"Initial deployment","text":"<p>Here are the different modules involved in SKAS authentication right after base installation:</p> <p></p> <p>SKAS is deployed as a Kubernetes Pod, hosting three containers:</p> <ul> <li><code>skAuth</code> which is responsible for delivering Kubernetes tokens and validating them.</li> <li><code>skmerge</code>, which is responsible for building a consolidated identity from several Identity Providers. In this  configuration, with only one provider, it acts as a simple passthrough.</li> <li><code>skCrd</code>, which is an Identity Provider storing user's information in the Kubernetes storage, in the namespace <code>skas-admin</code>.</li> </ul> <p>In this schema, the arrows represent the main communication flow between the components, all of which involve HTTP exchanges.</p> <p></p> <p>Here is a summary of the exchange for an initial interaction:</p> <ul> <li>The user issues a <code>kubectl</code> command (such as <code>kubectl get pods</code>). For this, a token is needed. It will be provided by the <code>kubectl-sk</code> client-go credential plugins.</li> <li><code>kubectl-sk</code> prompts the user for login and password, then issues a <code>tokenCreate()</code> request to the <code>skAuth</code> module.</li> <li>The <code>skAuth</code> module issues a <code>getIdentity()</code> request with user credentials. This request is forwarded to the <code>skCrd</code> module.</li> <li>The <code>skCrd</code> module retrieves the user's information, checks password validity, and sends the information upward to the <code>skMerge</code> module, which forwards them to the <code>skAuth</code> module.</li> <li>The <code>skAuth</code> module generates a token and sends it back to the <code>kubectl-sk</code> module, which forwards it to <code>kubectl</code>.</li> <li><code>kubectl</code> sends the original request with the token to the Kubernetes API server.</li> <li>The API Server sends a <code>tokenReview()</code> request to the <code>skAuth</code> module, which replies with the user's information (user id and groups).</li> <li>The API Server applies its RBAC rules on user's information to allow or deny the requested operation.</li> </ul> <p>For a more detailed description of this interaction, you can refer to the sequence diagram.</p>"},{"location":"architecture/#ldap-setup","title":"LDAP setup","text":"<p>This schema describes the architecture when an LDAP server has been configured, as detailed in the previous chapter.</p> <p>The <code>skMerge</code> module is now connected to two identity providers: <code>skCrd</code> and <code>skLdap</code>, with <code>skLdap</code> being linked to an external LDAP server.</p> <p></p>"},{"location":"architecture/#modules-and-interfaces","title":"Modules and interfaces","text":"<p>Here is a description of the modules comprising SKAS and their interfaces:</p>"},{"location":"architecture/#identity-providers-skcrd-skldap-skstatic","title":"Identity providers (skCrd, skLdap, skStatic)","text":"<p>The <code>skStatic</code> provider hosts its user database in a configMap. It was used for testing during the primary development stages. It is now deprecated and undocumented.</p> <p>Under the default configuration, interfaces provided by these modules are associated with an HTTP server bound to <code>localhost</code>, making them accessible only from another container in the same pod (typically <code>skMerge</code>).</p>"},{"location":"architecture/#identity","title":"Identity","text":"<p>These modules provide an <code>identity</code> interface. The request contains the user's login and optionally the user's password.</p> <p>The response will convey the user's information (UID, common names, emails, groups) if the user is found, along with a status:</p> <ul> <li><code>UserNotFound</code>: If the user does not exist in this provider.</li> <li><code>PasswordUnchecked</code>: If the password was not provided in the request or if there is no password defined by this provider.</li> <li><code>PassswordChecked</code>: If a password was provided in the request and the provider validates it.</li> <li><code>PasswordFailed</code>:  If a password was provided in the request and the provider does not validate it.</li> <li><code>Disabled</code>:  If the user is found but its <code>disabled</code> flag is set.</li> <li><code>Undefined</code>: If the provider is out of order (for example, if the LDAP server is down).</li> </ul>"},{"location":"architecture/#passwordchange","title":"PasswordChange","text":"<p>The <code>skCrd</code> provider also handles another interface: <code>passwordChange</code>, which allows password modification by providing the old and new password (the latter in its hashed form).</p>"},{"location":"architecture/#skmerge","title":"skMerge","text":"<p>Under the default configuration, the interfaces provided by this module are associated with an HTTP server bound to <code>localhost</code>, making them accessible only from another container in the same pod (typically <code>skAuth</code>).\"</p>"},{"location":"architecture/#identity_1","title":"Identity","text":"<p>The <code>skMerge</code> module supports the same <code>identity</code> interface as a provider. Of course, the returned value is the merge of information from its underlying providers.</p> <p>This module also supports some extensions of this <code>identity</code> protocol:</p> <ul> <li>The returned result also indicates which provider was the 'authority' (the one that validates the password) for this user.</li> <li>The request can include a <code>detailed</code> flag. If set, the response will include a detailed list of responses from each  provider. This is intended to be displayed by the <code>kubectl-sk user describe</code> CLI command.\"</li> </ul>"},{"location":"architecture/#passwordchange_1","title":"PasswordChange","text":"<p>The <code>skMerge</code> module also supports a <code>passwordChange</code> interface. The request must contain the user's authority, which is the target provider to which this message will be forwarded.</p>"},{"location":"architecture/#skauth","title":"skAuth","text":"<p>Under the default configuration, all interfaces provided by this module are exposed to the outside world through an  ingress controller. They are secured using end-to-end SSL, with SSL termination handled by the module itself, and the  ingress configured in SSL passthrough mode.</p> <p>Except for the <code>login</code> and <code>tokenReview</code> interfaces, all others in this module are designed to be called from the <code>kubectl-sk</code> client executable.</p>"},{"location":"architecture/#tokencreate","title":"TokenCreate","text":"<p>The <code>tokenCreate</code> request contains a user's login and password. If the authentication is successful, a token is generated. The response will include:</p> <ul> <li>The generated token.</li> <li>User information for the <code>whoami</code> subcommand.</li> <li>The clientTTL for token expiration in the client's local cache.</li> <li>The authority, indicating the provider that validated the login/password.</li> </ul>"},{"location":"architecture/#tokenrenew","title":"TokenRenew","text":"<p>The <code>tokenRenew</code> interface check if the token is still valid and renew (touch) it. The <code>tokenRenew</code> interface checks if the token is still valid and renews (touch) it.</p>"},{"location":"architecture/#passwordchange_2","title":"PasswordChange","text":"<p>This is a simple passthrough that forwards the request to the underlying <code>skMerge</code> provider.</p>"},{"location":"architecture/#kubeconfig","title":"Kubeconfig","text":"<p>This interface provides a set of information that allows <code>kubectl-sk</code> to create an entry in the client config file,  typically located at <code>~/.kube/config</code>, for accessing the targeted SKAS-enabled cluster. This enables automatic client-side configuration.</p>"},{"location":"architecture/#identity_2","title":"Identity","text":"<p>This interface is intended to be called for a <code>kubectl-sk user describe</code> operation. It forwards the request to the  underlying <code>skMerge</code> module. However, since this operation is reserved for the SKAS administrator, the caller must  provide authentication credentials, typically its token, to ensure they have the necessary rights</p>"},{"location":"architecture/#login","title":"Login","text":"<p>This interface checks user credentials. If successful, the user's information is provided in the response. It is intended to be used by other applications, such as the DEX connector.</p> <p>This interface is disabled by default in the configuration.</p>"},{"location":"architecture/#tokenreview","title":"TokenReview","text":"<p>This interface is designed to support Webhook Token Authentication.  It will be invoked by the Kubernetes API server to validate a token and retrieve associated user attributes.</p>"},{"location":"architecture/#interfaces-exposition","title":"Interfaces exposition","text":"<p>Each interface can be exposed at three levels:</p> <ul> <li>Locally, accessible only from other containers in the same pod.</li> <li>Internally within Kubernetes, by creating a Kubernetes Service.</li> <li>Externally outside Kubernetes, by configuring a Kubernetes ingress controller.</li> </ul> <p>For each module, every exposed API can be accessed on two ports:</p> <ul> <li>One is associated with a server bound to localhost, enabling intra-pod communication. In configuration files,  this interface is referred to as the <code>internal</code> one.</li> <li>The other is associated with a server bound to the pod interface, designed to be exposed as a service or through an  ingress. Communication on this interface is always encrypted using SSL. In configuration files, this interface is  referred to as the <code>external</code> one.</li> </ul> <p>Depending on the configuration, either one or both ports can be activated.</p> <p>In the default configuration:</p> <ul> <li>Only the port bound to localhost is activated for the <code>skCrd</code>, <code>skLdap</code>, and <code>skMerge</code> modules. </li> <li>For the <code>skAuth</code> module, only the port bound to the pod interface is activated.</li> </ul>"},{"location":"architecture/#sequence-diagrams","title":"Sequence diagrams","text":""},{"location":"architecture/#initial-user-connexion","title":"Initial user connexion","text":"<p>Below is the sequence of events for a successful initial connection:</p> <pre><code>sequenceDiagram\n  participant User\n  participant kubectl\n  participant kubectl-sk\n  participant skAuth\n  participant Api server\n  autonumber\n  User-&gt;&gt;kubectl: The user issues a&lt;br&gt;kubectl command\n  kubectl-&gt;&gt;kubectl-sk: kubectl launches&lt;br&gt;the kubectl-sk&lt;br&gt;credential plugin\n  kubectl-sk-&gt;&gt;kubectl-sk: kubectl-sk searches for&lt;br&gt;a token in its local cache.&lt;br&gt; Not found in this case.\n  kubectl-sk-&gt;&gt;User: kubectl-sk prompts&lt;br&gt;for login and password\n  User--&gt;&gt;kubectl-sk: User provides its credential\n  kubectl-sk-&gt;&gt;skAuth: HTTP GET REQ:&lt;br&gt;getToken()\n  skAuth-&gt;&gt;skAuth: skAuth calls&lt;br&gt;skMerge which&lt;br&gt;validates the user's&lt;br&gt;credentials and returns&lt;br&gt;user's information&lt;br&gt;Subsequently, a token is generated.\n  skAuth--&gt;&gt;kubectl-sk: Token in&lt;br&gt;HTTP response\n  kubectl-sk--&gt;&gt;kubectl: kubectl-sk returns&lt;br&gt;the token to kubectl&lt;br&gt;by printing it on&lt;br&gt;stdout and exits.\n  kubectl-&gt;&gt;Api server: kubectl issues the appropriate API call&lt;br&gt;with the provided bearer token\n  Api server-&gt;&gt;skAuth: The API Server&lt;br&gt;issues an&lt;br&gt;HTTP POST&lt;br&gt;with a&lt;br&gt;TokenReview&lt;br&gt;command\n  skAuth--&gt;&gt;Api server: skAuth validates&lt;br&gt;the token and &lt;br&gt;provides user's&lt;br&gt;name and groups&lt;br&gt;in the response.\n  Api server-&gt;&gt;Api server: API Server validates&lt;br&gt;if user is allowed by&lt;br&gt;RBAC to perform&lt;br&gt;the requested action\n  Api server--&gt;&gt;kubectl: API Server returns the action result\n  kubectl--&gt;&gt;User: kubectl displays&lt;br&gt;the result and exits</code></pre>"},{"location":"architecture/#token-renewal","title":"Token renewal","text":"<p>Here is the sequence when a valid token is already present in the client's local cache.</p> <pre><code>sequenceDiagram\n  participant User\n  participant kubectl\n  participant kubectl-sk\n  participant skAuth\n  participant Api server\n  autonumber\n  User-&gt;&gt;kubectl: The user issues a&lt;br&gt;kubectl command\n  kubectl-&gt;&gt;kubectl-sk: kubectl launches&lt;br&gt;the kubectl-sk&lt;br&gt;credential plugin\n  kubectl-sk-&gt;&gt;kubectl-sk: kubeclt-sk lookups&lt;br&gt;for token in its &lt;br&gt;local cache.&lt;br&gt;Found in this case\n  kubectl-sk-&gt;&gt;kubectl-sk: Is the token still&lt;br&gt;valid against the&lt;br&gt;clientTokenTTL&lt;br&gt;YES in this case\n  kubectl-sk--&gt;&gt;kubectl: kubectl-sk returns&lt;br&gt;the token to kubectl&lt;br&gt;by printing it on&lt;br&gt;stdout and exits.\n  kubectl-&gt;&gt;Api server: kubectl issues the appropriate API call&lt;br&gt;with the provided bearer token\n  Api server-&gt;&gt;skAuth: The API Server&lt;br&gt;issues an&lt;br&gt;HTTP POST&lt;br&gt;with a&lt;br&gt;TokenReview&lt;br&gt;command\n  skAuth--&gt;&gt;Api server: skAuth validates&lt;br&gt;the token and &lt;br&gt;provides user's&lt;br&gt;name and groups&lt;br&gt;in the response.\n  Api server-&gt;&gt;Api server: API Server validates&lt;br&gt;if user is allowed by&lt;br&gt;RBAC to perform&lt;br&gt;the requested action\n  Api server--&gt;&gt;kubectl: API Server returns the action result\n  kubectl--&gt;&gt;User: kubectl displays&lt;br&gt;the result and exits</code></pre> <p>Here is the sequence when a token is still valid, but the short-lived local cache has expired.</p> <pre><code>sequenceDiagram\n  participant User\n  participant kubectl\n  participant kubectl-sk\n  participant skAuth\n  participant Api server\n  autonumber\n  User-&gt;&gt;kubectl: The user issues a&lt;br&gt;kubectl command\n  kubectl-&gt;&gt;kubectl-sk: kubectl launches&lt;br&gt;the kubectl-sk&lt;br&gt;credential plugin\n  kubectl-sk-&gt;&gt;kubectl-sk: kubeclt-sk lookups&lt;br&gt;for token in its &lt;br&gt;local cache.&lt;br&gt;Found in this case\n  kubectl-sk-&gt;&gt;kubectl-sk: Is the token still&lt;br&gt;valid against the&lt;br&gt;clientTokenTTL&lt;br&gt;NO in this case\n  kubectl-sk-&gt;&gt;skAuth: HTTP GET REQ:&lt;br&gt;validateToken()\n  skAuth-&gt;&gt;skAuth: skAuth check&lt;br&gt;if token is still valid.&lt;br&gt;Yes in this case\n  skAuth--&gt;&gt;kubectl-sk: tokenValid response\n  kubectl-sk--&gt;&gt;kubectl: kubectl-sk returns&lt;br&gt;the token to kubectl&lt;br&gt;by printing it on&lt;br&gt;stdout and exits.\n  kubectl-&gt;&gt;Api server: kubectl issues the appropriate API call&lt;br&gt;with the provided bearer token\n  Api server-&gt;&gt;skAuth: The API Server&lt;br&gt;issues an&lt;br&gt;HTTP POST&lt;br&gt;with a&lt;br&gt;TokenReview&lt;br&gt;command\n  skAuth--&gt;&gt;Api server: skAuth validates&lt;br&gt;the token and &lt;br&gt;provides user's&lt;br&gt;name and groups&lt;br&gt;in the response.\n  Api server-&gt;&gt;Api server: API Server validates&lt;br&gt;if user is allowed by&lt;br&gt;RBAC to perform&lt;br&gt;the requested action\n  Api server--&gt;&gt;kubectl: API Server returns the action result\n  kubectl--&gt;&gt;User: kubectl displays&lt;br&gt;the result and exits</code></pre>"},{"location":"architecture/#token-expired","title":"Token expired","text":"<p>Here is the sequence when a token has expired.</p> <pre><code>sequenceDiagram\n  participant User\n  participant kubectl\n  participant kubectl-sk\n  participant skAuth\n  participant Api server\n  autonumber\n  User-&gt;&gt;kubectl: The user issues a&lt;br&gt;kubectl command\n  kubectl-&gt;&gt;kubectl-sk: kubectl launches&lt;br&gt;the kubectl-sk&lt;br&gt;credential plugin\n  kubectl-sk-&gt;&gt;kubectl-sk: kubeclt-sk lookups&lt;br&gt;for token in its &lt;br&gt;local cache.&lt;br&gt;Found in this case\n  kubectl-sk-&gt;&gt;kubectl-sk: Is the token still&lt;br&gt;valid against the&lt;br&gt;clientTokenTTL&lt;br&gt;NO in this case\n  kubectl-sk-&gt;&gt;skAuth: HTTP GET REQ:&lt;br&gt;validateToken()\n  skAuth-&gt;&gt;skAuth: skAuth check&lt;br&gt;if token is still valid.&lt;br&gt;NO in this case\n  skAuth--&gt;&gt;kubectl-sk: tokenInvalid&lt;br&gt;response\n  kubectl-sk-&gt;&gt;User: kubectl-sk prompts&lt;br&gt;for login and password</code></pre>"},{"location":"argocd/","title":"Argo CD integration with DEX","text":"<p>Argo CD integrates with DEX, embedding an instance of DEX within it. Argo CD takes care of deploying DEX and most of its configuration. The only part of the DEX configuration that you'll need to set up is the <code>connectors:</code> section.</p> <p>To follow this installation process, it's assumed that you already have an Argo CD instance deployed in a  'standard' way, complete with a user interface and ingress. Deploying Argo CD in this manner is outside the scope  of this documentation, so please refer to the Argo CD documentation or use Argo CD helm chart.</p> <p>Once you have Argo CD set up, you should proceed with:</p> <ul> <li>Deploying SKAS, configuring a <code>login</code> service to be used by the embedded DEX instance in Argo CD.</li> <li>Configuring this embedded DEX instance by patching the existing deployment.\"</li> </ul>"},{"location":"argocd/#skas-deployment","title":"SKAS deployment","text":"<p>SKAS needs to be (re)configured to activate a <code>login</code> service. Below is the appropriate values file for this configuration:</p> values.skas2.yaml <pre><code>skAuth:\nexposure:\nexternal:\nservices:\nlogin:\ndisabled: false\nclients:\n- id: dex-argocd\nsecret: \"aSharedSecret\"\n</code></pre> <p>Note that client authentication has been set up with a pair of id/secret. </p> <p>To apply this configuration:</p> <pre><code>helm -n skas-system upgrade -i skas skas/skas --values ./values.init.yaml --values ./values.skas.login.yaml\n</code></pre> <p>Don't forget to include the <code>values.init.yaml</code> file or merge it into the <code>values.skas.yaml</code> file. Additionally, if you have other values files, make sure to include them in each upgrade.</p> <p>Also, remember to restart the pod(s) after making these configuration changes. You can find more information on how to do this in the Configuration: Pod restart section.</p>"},{"location":"argocd/#patching-argo-cd","title":"Patching argo CD","text":"<p>The next step is to patch the Argo CD deployment. In fact, there are two patches required:</p> <ul> <li>One to use the SKAS-specific DEX image. DEX does not provide an extension mechanism to add connectors externally,  so a specific SKAS DEX image with a SKAS connector must be used.</li> <li>Another to configure the <code>configMap</code> storing the DEX configuration.</li> </ul> <p>Here is the patch file for the DEX image (It is a JSON RFC 6902 Patch):</p> dex-server-patch.json <pre><code>[\n{ \"op\": \"replace\",\n\"path\": \"/spec/template/spec/containers/0/image\",\n\"value\": \"ghcr.io/skasproject/dex:v2.37.0-skas-0.2.1\"\n}\n]\n</code></pre> <p>To apply these patches, use the following command:</p> <pre><code>kubectl -n argocd patch deployment argocd-dex-server --type json --patch-file ./dex-server-patch.json\n</code></pre> <p>Here is the DEX configuration patch (It is a Strategic Merge patch):</p> argocd-cm-patch.yaml <pre><code>data:\nadmin.enabled: \"false\"\nurl: https://argocd.ingress.mycluster.internal\ndex.config: |\nconnectors:\n- type: skas\nid: skas\nname: SKAS\nconfig:\nloginPrompt: \"User\"\nloginProvider:\nurl: https://skas-auth.skas-system.svc\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0......................09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\"\ninsecureSkipVerify: false\nclientAuth:\nid: \"dex-argocd\"\nsecret: \"aSharedSecret\"\n</code></pre> <p>The DEX <code>connectors:</code> config is similar to the one in DEX standalone configuration.</p> <p>You must adjust:</p> <ul> <li>The <code>url: https://argocd.ingress.mycluster.internal</code> to point to your Argo CD server UI.</li> <li>The <code>admin.enabled</code> if you want to still be able to use the local Argo CD <code>admin</code> account.</li> <li>The <code>rootCaData:</code> field is populated with the Certificate Authority of the <code>skAuth</code> kubernetes service.    To find its value, you can extract it from the service's certificate.You can retrieve it with the following command:     <pre><code>kubectl -n skas-system get secret skas-auth-cert -o=jsonpath='{.data.ca\\.crt}'\n</code></pre></li> </ul> <p>Then, you must apply this patch using the following command:</p> <pre><code>kubectl -n argocd patch configMap argocd-cm --type strategic --patch-file ./argocd-cm-patch.yaml\n</code></pre>"},{"location":"argocd/#restart-pods","title":"Restart Pods","text":"<p>For the patch to take effect, you will need to restart some of the Argo CD pods:</p> <pre><code>$ kubectl -n argocd rollout restart deployment argocd-dex-server &amp;&amp; \\\nkubectl -n argocd rollout restart deployment argocd-server\n</code></pre>"},{"location":"argocd/#test","title":"Test","text":"<p>Open your browser and navigate to the Argo CD UI. You should see a page similar to this:</p> <p></p> <p>Click on the <code>LOG IN VIA SKAS</code> button. You should be redirected to the DEX login page. Enter a valid user login and  password:</p> <p></p> <p>And you should land on the usual Argo CD Applications page:</p> <p></p> <p>Click on the <code>User info</code> menu entry on the left to ensure you have the correct user information:</p> <p></p> <p>We can see the groups are correct. The Argo CD web UI chooses to display the user's email in the Username field. If  our current user has no email defined, the field will appear blank.</p>"},{"location":"chaining/","title":"Identity Providers chaining","text":""},{"location":"chaining/#overview","title":"Overview","text":"<p>In the previous chapter, a configuration has been set up with two Identity Providers:</p> <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap\n</code></pre> <p>The <code>crd</code> provider refers to the user database stored in the <code>skas-namespace</code> while the <code>ldap</code> refers to a connected LDAP server.</p> <p>The function of the <code>skMerge</code> module is to unify this chain of providers, allowing them to function as a single entity.. </p> <p>By default, user information is consolidated in the following manner:</p> <ul> <li> <p>If a given user exists in only one provider, that provider is considered the authoritative source for that user.</p> </li> <li> <p>If a given user exists in several providers:</p> <ul> <li>The resulting group set is the union of all groups from all providers hosting this user.</li> <li>The resulting email set is the union of all emails from all providers hosting this user.</li> <li>The resulting commonName set is the union of all commonNames from all providers hosting this user.</li> <li>The first provider in the chain hosting this user will be the authoritative one for password validation.<ul> <li>This means that there can't be two valid passwords for a single user.</li> <li>This also implies that the order of providers in the list is important.</li> <li>There is one exception to this rule: If a user has no password defined (this is a valid case for our <code>crd</code> provider), then the authoritative one is the next provider in the list.</li> </ul> </li> <li>The UID will be defined by the authoritative provider. (The one who validate the password)</li> </ul> </li> </ul>"},{"location":"chaining/#cli-user-management","title":"CLI user management","text":"<p>Of course, all <code>kubectl sk user ...</code> operation such as <code>create</code>, <code>patch</code>, <code>bind/unbind</code> can only modify resources in  the <code>crd</code> provider. They have no impact on <code>ldap</code> or other external provider.</p> <p>From the SKAS perspective, LDAP is 'Read-Only'.</p> <p>A specific <code>kubectl sk user describe</code> subcommand will display consolidated information for any user. For example:</p> <pre><code>$ kubectl sk user describe jsmith\n&gt; USER     STATUS              UID      GROUPS             EMAILS                                          COMMON NAMES   AUTH\n&gt; jsmith   passwordUnchecked   100001   devs,itdep,staff   john.smith@mycompany.com,jsmith@mycompany.com   John SMITH     crd\n</code></pre> <p>Note the last column, which indicates the authoritative provider for each user.</p> <p>Access to this subcommand is restricted to members of the <code>skas-admin</code> group.</p> <p>The flag <code>--explain</code> will help you understand where user's information is sourced from:</p> <pre><code>$ kubectl sk user describe jsmith --explain\n&gt; USER     STATUS              UID      GROUPS             EMAILS                                          COMMON NAMES   AUTH\n&gt; jsmith   passwordUnchecked   100001   devs,itdep,staff   john.smith@mycompany.com,jsmith@mycompany.com   John SMITH     crd\n\n&gt; Detail:\n&gt; PROVIDER   STATUS              UID          GROUPS        EMAILS                     COMMON NAMES\n&gt; crd        passwordUnchecked   100001       devs          jsmith@mycompany.com       John SMITH\n&gt; ldap       passwordUnchecked   1148400004   staff,itdep   john.smith@mycompany.com   John SMITH\n</code></pre> <p>There are also two flags (<code>--password</code> or <code>\u00ecnputPassword</code>) for the administrator to validate a password, if they know it:</p> <pre><code>$ kubectl sk user describe jsmith --explain --inputPassword\n&gt; Password for user 'jsmith':\n&gt; USER     STATUS            UID      GROUPS             EMAILS                                          COMMON NAMES   AUTH\n&gt; jsmith   passwordChecked   100001   devs,itdep,staff   john.smith@mycompany.com,jsmith@mycompany.com   John SMITH     crd\n\n&gt; Detail:\n&gt; PROVIDER   STATUS            UID          GROUPS        EMAILS                     COMMON NAMES\n&gt; crd        passwordChecked   100001       devs          jsmith@mycompany.com       John SMITH\n&gt; ldap       passwordFail      1148400004   staff,itdep   john.smith@mycompany.com   John SMITH\n</code></pre>"},{"location":"chaining/#group-bindings","title":"Group bindings","text":"<p>In the User guide, it has been explained how to bind a group to a user from the <code>crd</code> provider.  This capability is also possible for any user, regardless of their provider. </p> <p>For example, let's say we have a user <code>oriley</code> in the LDAP server (although not defined in our <code>crd</code> provider):\"</p> <pre><code>$ kubectl sk user describe oriley --explain\n&gt; USER     STATUS              UID          GROUPS        EMAILS                 COMMON NAMES   AUTH\n&gt; oriley   passwordUnchecked   1148400003   itdep,staff   oriley@mycompany.com   Oliver RILEY   ldap\n\n&gt; Detail:\n&gt; PROVIDER   STATUS              UID          GROUPS        EMAILS                 COMMON NAMES\n&gt; crd        userNotFound        0\n&gt; ldap       passwordUnchecked   1148400003   staff,itdep   oriley@mycompany.com   Oliver RILEY\n</code></pre> <p>Let's say we want this user to be able to be an admin for SKAS and also for the Kubernetes cluster. For this, we need to set up two GroupBindings:</p> <pre><code>$ kubectl sk user bind oriley system:masters\n&gt; GroupBinding 'oriley.system.masters' created in namespace 'skas-system'.\n\n$ kubectl sk user bind oriley skas-admin\n&gt; GroupBinding 'oriley.skas-admin' created in namespace 'skas-system'.\n\n$ $ kubectl sk user describe oriley --explain\n&gt; USER     STATUS              UID          GROUPS                                  EMAILS                 COMMON NAMES   AUTH\n&gt; oriley   passwordUnchecked   1148400003   itdep,skas-admin,staff,system:masters   oriley@mycompany.com   Oliver RILEY   ldap\n\n&gt; Detail:\n&gt; PROVIDER   STATUS              UID          GROUPS                      EMAILS                 COMMON NAMES\n&gt; crd        userNotFound        0            system:masters,skas-admin\n&gt; ldap       passwordUnchecked   1148400003   staff,itdep                 oriley@mycompany.com   Oliver RILEY\n</code></pre> <p>Of course, this group binding could have been performed on the LDAP server. However, this would require having some  write access on the LDAP server. It is often considered a best practice to manage cluster authorization at the cluster level. (We will explore a way to centralize authorization in a multi-cluster context later on).</p>"},{"location":"chaining/#role-binding","title":"Role binding","text":"<p>As it is possible to bind a group to a user defined in whatever provider, it is also possible to bind a Kubernetes  <code>role</code> (or <code>clusterRole</code>) to a group defined in the LDAP provider:</p> <pre><code>$ kubectl -n ldemo create rolebinding configurator-itdep --role=configurator --group=itdep\n&gt; rolebinding.rbac.authorization.k8s.io/configurator-itdep created\n</code></pre> <p>See the User guide for a sample, including the <code>configurator</code> role definition.</p>"},{"location":"chaining/#provider-configuration","title":"Provider configuration.","text":"<p>Up to this point, the configuration has defined the provider chain as follows:</p> <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap\n</code></pre> <p>Each provider can support optional attributes. Here is a snippet with all the attributes and their default values:</p> <pre><code>skMerge:\nproviders:\n- name: crd\ncredentialAuthority: true\ngroupAuthority: true\ncritical: true\ngroupPattern: \"%s\"\nuidOffset: 0\n- name: ldap\ncredentialAuthority: true\ngroupAuthority: true\ncritical: true\ngroupPattern: \"%s\"\nuidOffset: 0\n</code></pre> <ul> <li><code>credentialAuthority</code>:  Setting this attribute to 'false' will prevent this provider from authenticating any user.</li> <li><code>groupAuthority</code>: Setting this attribute to <code>false</code> will prevent the groups of this provider from being added to each user.</li> <li><code>critical</code>: Defines the behavior of the chain if this provider is down or out of order (e.g., LDAP server is down).  If set, then all authentication will fail in such a case.</li> <li><code>groupPattern</code>: Allows you to 'decorate' all groups provided by this provider. See the example below.</li> <li><code>uidOffset</code>: This will be added to the UID value if this provider is the authority for this user.</li> </ul> <p>For example:</p> <pre><code>skMerge:\nproviders:\n- name: crd\ncredentialAuthority: false\ngroupAuthority: true\ncritical: true\ngroupPattern: \"%s\"\nuidOffset: 0\n- name: ldap\ncredentialAuthority: true\ngroupAuthority: true\ncritical: true\ngroupPattern: \"dep1_%s\"\nuidOffset: 0\n</code></pre> <p>The <code>crd</code> provider will not be able to authenticate any user (<code>credentialAuthority</code> is set to <code>false</code>). This means we have 'lost' our initial <code>admin</code> user.</p> <p>Fortunately, we previously granted <code>oriley</code> with full admin rights. </p> <pre><code>$ kubectl sk login oriley\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl sk whoami\n&gt; USER     ID           GROUPS\n&gt; oriley   1148400003   dep1_itdep,dep1_staff,skas-admin,system:masters\n</code></pre> <p>We can check here that this user still belong to the kubernetes admin groups (<code>skas-admin</code>, <code>system:masters</code>) but  the groups of the <code>ldap</code> provider has been renamed with the <code>dep1_</code> prefix.</p> <p>We can see here that this user still belongs to the Kubernetes admin groups (<code>skas-admin</code> and <code>system:masters</code>),  but the groups from the <code>ldap</code> provider have been prefixed with  <code>dep1_</code>.</p> <p>Let's take a closer look at this user:</p> <pre><code>$ kubectl sk user describe oriley --explain\n&gt; USER     STATUS              UID          GROUPS                                            EMAILS                 COMMON NAMES   AUTH\n&gt; oriley   passwordUnchecked   1148400003   dep1_itdep,dep1_staff,skas-admin,system:masters   oriley@mycompany.com   Oliver RILEY   ldap\n\n&gt; Detail:\n&gt; PROVIDER   STATUS              UID          GROUPS                      EMAILS                 COMMON NAMES\n&gt; crd        userNotFound        0            system:masters,skas-admin\n&gt; ldap       passwordUnchecked   1148400003   staff,itdep                 oriley@mycompany.com   Oliver RILEY\n</code></pre> <p>Now, let's check the <code>admin</code> user:</p> <pre><code>$ kubectl sk user describe admin --explain\n&gt; USER    STATUS            UID   GROUPS                      EMAILS   COMMON NAMES         AUTH\n&gt; admin   passwordMissing   0     skas-admin,system:masters            SKAS administrator\n\n&gt; Detail:\n&gt; PROVIDER   STATUS            UID   GROUPS                      EMAILS   COMMON NAMES\n&gt; crd        passwordMissing   0     skas-admin,system:masters            SKAS administrator\n&gt; ldap       userNotFound      0\n</code></pre> <p>The fact that we denied <code>credentialAuthority</code> will translate to <code>passwordMissing</code> (While, in fact, the password is still physically present in the storage.) </p> <p>Such a configuration aims to comply with certain overarching management policies:</p> <ul> <li>A corporate policy that requires all users to be referenced in a central LDAP server. This constraint is fulfilled,  as even though a user can still be created in the<code>crd</code> provider, their corresponding credentials will not be activated.</li> <li>As Kubernetes cluster administrators, we want to have exclusive control over who can manage the cluster. By adding  a group decorator (<code>groupPattern: \"dep1_%s\"</code>), we prevent a malicious LDAP administrator from granting access to critical groups (<code>skas-admin</code>, <code>system:master</code>, ...) to any LDAP users.\"</li> </ul> <p>Two complementary remarks:</p> <ul> <li>There may still be an interest in creating a user in the <code>crd</code> provider. This is done to add more information, such as an email or commonName, to a user existing in the <code>ldap</code> provider.</li> <li>The <code>crd</code> provider must be the first in the list. Otherwise, an LDAP administrator may create a user with the same name as an existing administrator and gain authority over its password, to gain full Kubernetes access.</li> </ul>"},{"location":"clusterfederation/","title":"Cluster federation","text":"<p>Due to its modular concept and the ability to configure links between modules, even across clusters, SKAS is very effective in a multi-cluster environment.</p> <p>Here's a sample configuration to illustrate this:</p> <ul> <li><code>Cluster0</code> serves as a kind of 'control' cluster, acting as a 'hub' and providing centralized user information.    It can be dedicated to this role or host other control functions.</li> <li><code>Cluster1</code> is a standard cluster that hosts typical workloads. It relies entirely on <code>Cluster0</code> for user authentication.</li> <li><code>Cluster2</code> is similar to <code>Cluster1</code>, but it can also manage some local users and/or groups.\"</li> </ul> <p>Of course, this is just one example of what could be achieved with SKAS.</p> <p>In the rich Kubernetes ecosystem, there are several solutions, such as Submariner, Skupper, Cilium ClusterMesh,  Liqo, and more, aimed at extending the concept of Kubernetes <code>services</code> across clusters. While these can be used  in our cases, the configuration described here relies on 'pure vanilla', independent Kubernetes clusters.</p> <p></p>"},{"location":"clusterfederation/#cluster0-deployment","title":"Cluster0 deployment.","text":"<p>Here are two samples of value files for <code>cluster0</code></p> <ul> <li>One with only the local user database.</li> <li>Another one with also an external LDAP server.</li> </ul> <p>The LDAP server sample will help illustrate the chaining of providers.</p> <p>Here is the values file with only the local user database (It is activated by default):</p> values.cluster0-noldap.yaml <pre><code>clusterIssuer: your-cluster-issuer\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.cluster0\nkubeconfig:\ncontext:\nname: skas@cluster0\ncluster:\napiServerUrl: https://kubernetes.ingress.cluster0\nskMerge:\nexposure:\nexternal:\nenabled: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: cluster1\nsecret: cluster0Cluster1SharedSecret\n- id: cluster2\nsecret: cluster0Cluster2SharedSecret\ningress:\nenabled: true\nhost: skas-skmerge.ingress.cluster0\n</code></pre> <p>The first part includes what was previously in a separate <code>values.init.yaml</code> file in other samples within this  documentation. Consequently, this file can be used for the initial deployment or for upgrading an existing one.</p> <p>Then, there is the part specific to this configuration. The <code>identiy</code>  service of the  <code>skMerge</code> module is exposed externally, and authentication is required from the callers, which will be the two  other clusters.</p> <p>Furthermore, this service must be accessed externally. So, set <code>ingress.enable: true</code> and adjust the <code>ingress.host</code>  value to your specific context.</p> <p>Remember that the <code>ingress.host</code> value must also be defined in your DNS.</p> <p>Here is the values file, including the LDAP external server as well:</p> values.cluster0-ldap.yaml <pre><code>clusterIssuer: your-cluster-issuer\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.cluster0\nkubeconfig:\ncontext:\nname: skas@cluster0\ncluster:\napiServerUrl: https://kubernetes.ingress.cluster0\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\nskMerge:\nproviders:\n- name: crd\n- name: ldap\nexposure:\nexternal:\nenabled: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: cluster1\nsecret: cluster0Cluster1SharedSecret\n- id: cluster2\nsecret: cluster0Cluster2SharedSecret\ningress:\nenabled: true\nhost: skas-skmerge.ingress.cluster0\n</code></pre> <p>Please refer to LDAP Setup for instructions on how to configure the connection to this external LDAP server.</p> <p>To apply the choosen configuration, please ensure that you have a <code>kubectl</code> configuration set up to target <code>cluster1</code>  with administrator privileges, and then enter one of the following commands:</p> <pre><code>helm -n skas-system upgrade -i skas skas/skas --values ./values.cluster0-noldap.yaml --create-namespace\n</code></pre> <p>or: </p> <pre><code>helm -n skas-system upgrade -i skas skas/skas --values ./values.cluster0-ldap.yaml --create-namespace\n</code></pre> <p>If this is the first deployment on this cluster, don't forget also to configure the API Server</p> <p>For the configuration of the two other clusters, you will need the Certificate Authority of the ingress on the  <code>skMerge</code> identity endpoint, encoded in base64. You can retrieve it with the following command:\"</p> <pre><code>kubectl -n skas-system get secret skas-merge-cert -o=jsonpath='{.data.ca\\.crt}'\n</code></pre>"},{"location":"clusterfederation/#cluster1-deployment","title":"Cluster1 deployment","text":"<p>Here is a sample values file for the deployment of <code>cluster1</code>:</p> values.cluster1.yaml <pre><code>clusterIssuer: your-cluster-issuer\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.cluster1\nkubeconfig:\ncontext:\nname: cluster1\ncluster:\napiServerUrl: https://kubernetes.ingress.cluster1\nskMerge:\nproviders:\n- name: cluster0\nproviderInfo:\ncluster0:\nurl: https://skas-skmerge.ingress.cluster0\nrootCaData: \"LS0tLS1CRUdJTiBDRV.............09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\"\ninsecureSkipVerify: false\nclientAuth:\nid: cluster1\nsecret: cluster0Cluster1SharedSecret\nskCrd:\nenabled: false\n</code></pre> <p>Once again, the first part includes what was previously stored in a separate <code>values.init.yaml</code> file,  which has been used in other samples within this documentation. </p> <p>Then, within the configuration:</p> <ul> <li>The <code>skMerge.providers</code> list contains only one provider: <code>cluster0</code>, which is defined just below it.</li> <li>The <code>providerInfo.cluster0.url</code> targets the ingress we've set up on <code>cluster0</code> in order to reach its identity service.</li> <li>The <code>providerInfo.cluster0.rootCaData</code> is configured with the base64-encoded Certificate Authority of the ingress    on the <code>skMerge</code> identity endpoint, as we obtained in the previous step.</li> <li>The <code>providerInfo.cluster0.clientAuth</code> provides the required authentication information for <code>cluster0</code>.</li> </ul> <p>To apply this configuration, please ensure that you have a <code>kubectl</code> configuration set up to target <code>cluster1</code> with  administrator privileges, and then enter the following command:</p> <pre><code>helm -n skas-system upgrade -i skas skas/skas --values ./values.cluster1.yaml --create-namespace\n</code></pre> <p>If this is the first deployment on this cluster, don't forget also to configure the API Server</p>"},{"location":"clusterfederation/#testing","title":"Testing","text":"<p>In the following, we will use the configuration where an LDAP server is connected to <code>cluster0</code>.</p> <p>We can now configure our local Kubernetes config file to authenticate to our clusters through SKAS:</p> <pre><code>$ kubectl sk init https://skas.ingress.cluster0\n&gt; Setup new context 'cluster0' in kubeconfig file '/Users/john/.kube/config'\n$ kubectl sk init https://skas.ingress.cluster1\n&gt; Setup new context 'cluster1' in kubeconfig file '/Users/john/.kube/config'\n</code></pre> <p>We can connect as <code>admin</code> on <code>cluster0</code> and check what this user looks like:</p> <pre><code>$ kubectl config use-context cluster0\n&gt; Switched to context \"cluster0\".\n\n$ kubectl sk login admin\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl sk user describe admin --explain\n&gt; USER    STATUS              UID   GROUPS                          EMAILS           COMMON NAMES               AUTH\n&gt; admin   passwordUnchecked   0     admin,all,auditors,skas-admin   admin@xxxx.com   SKAS administrator,admin   crd\n\n&gt; Detail:\n&gt; PROVIDER   STATUS              UID    GROUPS               EMAILS           COMMON NAMES\n&gt; crd        passwordUnchecked   0      skas-admin                            SKAS administrator\n&gt; ldap       passwordUnchecked   2003   all,auditors,admin   admin@xxxx.com   admin\n</code></pre> <p>In this example, the user <code>admin</code> exists in both of our providers (<code>crd</code> and <code>ldap</code>). Both sets of values are merged  to provide the user profile (Refer to Identity Providers chaining). </p> <p>Now, we can perform the same operation on 'cluster1':\"</p> <pre><code>$ kubectl config use-context cluster1\n&gt; Switched to context \"cluster1\".\n\n$ kubectl sk login admin\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl sk user describe admin --explain\n&gt; USER    STATUS              UID   GROUPS                          EMAILS           COMMON NAMES               AUTH\n&gt; admin   passwordUnchecked   0     admin,all,auditors,skas-admin   admin@xxxx.com   SKAS administrator,admin   cluster0\n\n&gt; Detail:\n&gt; PROVIDER   STATUS              UID   GROUPS                          EMAILS           COMMON NAMES\n&gt; cluster0   passwordUnchecked   0     admin,all,auditors,skas-admin   admin@xxxx.com   SKAS administrator,admin\n</code></pre> <p>From the perspective of <code>cluster1</code>, we have only one provider: <code>cluster0</code>. We abstract away the details of how the  <code>admin</code> profile was constructed.</p> <p>If we wish to expose this information, we can refactor this configuration by connecting <code>cluster1.skMerge</code> directly  to the <code>cluster0</code> identity providers (<code>crd</code> and <code>ldap</code>), one level below <code>skMerge</code>.</p> <p>Now, let's attempt to create a new user on <code>cluster1</code>:</p> <pre><code>$ kubectl config use-context cluster1\n&gt; Switched to context \"cluster1\".\n\n$ kubectl sk whoami\n&gt; USER    ID   GROUPS\n&gt; admin   0    admin,all,auditors,skas-admin\n\n$ kubectl sk user create localuser1 --generatePassword\n&gt; ERRO[0000] API server communication error                error=\"users.userdb.skasproject.io \\\"localuser1\\\" is forbidden: User \\\"admin\\\" cannot get resource \\\"users\\\" in API group \\\"userdb.skasproject.io\\\" in the namespace \\\"skas-system\\\"\"\n</code></pre> <p>The operation fails because the <code>admin</code> user does not have the necessary permissions to write to the <code>skas-system</code>  namespace in <code>cluster1</code>. This behavior is intentional, as we intend for user definitions to exist exclusively in  <code>cluster0</code>.</p> <p>There is a scenario where this operation might appear to succeed: If your <code>admin</code> user also has Kubernetes administrator  privileges (i.e., a member of <code>system:masters</code>), it will be able to write to the <code>skas-system</code> namespace, resulting  in a successful creation. However, it's important to note that users created in this manner will not be recognized  by the system in <code>cluster1</code>, as there is no <code>crd</code> provider present.\"</p> <pre><code>$  kubectl config use-context cluster0\n&gt; Switched to context \"cluster0\".\n\n$ kubectl sk user bind admin system:masters\n&gt; GroupBinding 'admin.system.masters' created in namespace 'skas-system'.\n\n$ kubectl config use-context cluster1\n&gt; Switched to context \"cluster1\".\n\n$ kubectl sk login admin\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl sk whoami\n&gt; USER    ID   GROUPS\n&gt; admin   0    admin,all,auditors,skas-admin,system:masters\n\n$ kubectl sk user create localuser1 --generatePassword\n&gt; The following password has been generated: 'KTeZrzYEgeHS'\n&gt; (Save it as it will not be accessible anymore).\n&gt; User 'localuser1' created in namespace 'skas-system'.\n\n$ kubectl sk login localuser1 KTeZrzYEgeHS\n&gt; Invalid login!\n</code></pre>"},{"location":"clusterfederation/#cluster2-deployment","title":"Cluster2 deployment","text":"<p>Here is a sample values file for the deployment of <code>cluster2</code>:</p> values.cluster2.yaml <pre><code>clusterIssuer: cluster-issuer1\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.cluster2\nkubeconfig:\ncontext:\nname: cluster2\ncluster:\napiServerUrl: https://kubernetes.ingress.cluster2\n# Members of these group will be allowed to perform 'kubectl-sk user describe'\n# Also, they will be granted by RBAC to access token resources\nadminGroups:\n- skas-admin\n- cluster2-skas-admin\nskMerge:\nproviders:\n- name: cluster0\n- name: crd\nproviderInfo:\ncluster0:\nurl: https://skas-skmerge.ingress.cluster0\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS...........VNGelVDQT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\"\ninsecureSkipVerify: false\nclientAuth:\nid: cluster2\nsecret: cluster0Cluster2SharedSecret\nskCrd:\nenabled: true\ninitialUser:\nlogin: cluster2-skas-admin\npasswordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\ncommonNames: [\"Cluster2 SKAS administrator\"]\ngroups:\n- cluster2-skas-admin\n# Members of theses groups will be granted RBAC access to users and groupBinding resources in the namespace above\nadminGroups:\n- cluster2-skas-admin\n- skas-admin\n</code></pre> <p>Once again, the first part includes what was previously stored in a separate <code>values.init.yaml</code> file, which has been used in other samples within this documentation.</p> <p>Then, within the configuration:</p> <ul> <li><code>skMerge.providers</code> reference now our local <code>crd</code> providers aside the global <code>cluster0</code>.</li> <li><code>skMerge.providerInfo.cluster0</code> is the same as for <code>cluster1</code>, except of course the <code>clientAuth</code> part.</li> <li>An initial user (<code>cluster2-skas-system</code>) is created as local admin. This account will only be valid on this cluster.</li> <li>The list of <code>adminGroups</code> must be defined twice:<ul> <li>One in the <code>skAuth</code> module. This is to allow the <code>kubectl-sk user describe</code> command for members of these groups.</li> <li>One in the <code>skCrd</code> module. This is to set up RBAC rules to allow SKAS resources (<code>skusers</code>, <code>groupBindings</code>, <code>tokens</code>) to be managed by members of these groups.</li> </ul> </li> </ul> <p>To apply this configuration, please ensure that you have a <code>kubectl</code> configuration set up to target <code>cluster2</code> with administrator privileges, and then enter the following command:</p> <pre><code>helm -n skas-system upgrade -i skas skas/skas --values ./values.cluster2.yaml --create-namespace\n</code></pre> <p>If this is the first deployment on this cluster, don't forget also to configure the API Server</p>"},{"location":"clusterfederation/#testing_1","title":"Testing","text":"<p>We can now configure our local Kubernetes config file to authenticate to <code>cluster2</code> through SKAS:</p> <pre><code>$ kubectl sk init https://skas.ingress.cluster2\n&gt; Setup new context 'cluster2' in kubeconfig file '/Users/john/.kube/config'\n</code></pre> <p>Now, we can log in as the local admin and describe ourself:</p> <pre><code>$ kubectl sk login cluster2-skas-admin\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl sk user describe cluster2-skas-admin --explain\n&gt; USER                  STATUS              UID   GROUPS                EMAILS   COMMON NAMES                  AUTH\n&gt; cluster2-skas-admin   passwordUnchecked   0     cluster2-skas-admin            Cluster2 SKAS administrator   crd\n\nDetail:\n&gt; PROVIDER   STATUS              UID   GROUPS                EMAILS   COMMON NAMES\n&gt; cluster0   userNotFound        0\n&gt; crd        passwordUnchecked   0     cluster2-skas-admin            Cluster2 SKAS administrator\n</code></pre> <p>Ensure we can perform all usual user management operations with this account:</p> <pre><code>$ kubectl -n skas-system get tokens\n&gt; NAME                                               CLIENT   USER LOGIN            AUTH.   USER ID   CREATION               LAST HIT\n&gt; tjvygwmtparktpwiiuysydzctbunppkycsykprtdswsramtm            cluster2-skas-admin   crd     0         2023-08-31T12:43:03Z   2023-08-31T12:43:26Z\n\n$ kubectl sk user create cluster2user1 --generatePassword\n&gt; The follwing password has been generated: '8fJoM6JFObjO'\n&gt; (Save it as it will not be accessible anymore).\n&gt; User 'cluster2user1' created in namespace 'skas-system'.\n\n$  kubectl sk user bind cluster2user1 cluster2grp1\n&gt; GroupBinding 'cluster2user1.cluster2grp1' created in namespace 'skas-system'.\n\n$ kubectl sk user describe cluster2user1\n&gt; USER            STATUS              UID   GROUPS         EMAILS   COMMON NAMES   AUTH\n&gt; cluster2user1   passwordUnchecked   0     cluster2grp1                           crd\n\n$ kubectl sk user describe cluster2user1 --explain\n&gt; USER            STATUS              UID   GROUPS         EMAILS   COMMON NAMES   AUTH\n&gt; cluster2user1   passwordUnchecked   0     cluster2grp1                           crd\n\n&gt; Detail:\n&gt; PROVIDER   STATUS              UID   GROUPS         EMAILS   COMMON NAMES\n&gt; cluster0   userNotFound        0\n&gt; crd        passwordUnchecked   0     cluster2grp1\n</code></pre> <p>And check if the newly created user is effective:</p> <pre><code>$ kubectl sk login cluster2user1 8fJoM6JFObjO\n&gt; logged successfully..\n\n$ kubectl sk whoami\n&gt; USER            ID   GROUPS\n&gt; cluster2user1   0    cluster2grp1\n\n$ kubectl get ns\n&gt; Error from server (Forbidden): namespaces is forbidden: User \"cluster2user1\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope\n</code></pre> <p>The same operations can also be performed using the <code>admin</code> account because it is a member of the <code>skas-admin</code>  group, which we have included in the two <code>adminGroups</code> lists in the <code>values.cluster2.yaml</code> configuration file.</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#principle","title":"Principle","text":"<p>Installation is performed using a Helm chart, and configurations are made by providing a 'values' file that overrides  the default values.yaml of the Helm chart. </p> <p>This is what was done during the initial configuration, with such a file:</p> values.init.yaml <pre><code>clusterIssuer: your-cluster-issuer\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.mycluster.internal\nkubeconfig:\ncontext:\nname: skas@mycluster.internal\ncluster:\napiServerUrl: https://kubernetes.ingress.mycluster.internal\n</code></pre> <p>SKAS is a highly flexible product, and consequently, there are numerous variables in the default <code>values.yaml</code> of the  Helm chart. Fortunately, the default values are suitable for most use cases.</p> <p>In this chapter, we won't describe all the variables (you can refer to the comments in the file for details),  but we will explain some typical configuration variations. </p> <p>To apply a modified file, you should use the helm upgrade command:</p> <pre><code>helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml\n</code></pre>"},{"location":"configuration/#pod-restart","title":"Pod restart","text":"<p>To ensure that the new configuration is taken into account, you need to restart the SKAS pod(s).  The most straightforward way to do this is by performing a 'rollout' on the skas deployment:</p> <pre><code>$ kubectl -n skas-system rollout restart deployment skas\n&gt; deployment.apps/skas restarted\n</code></pre> <p>There is some solution to perform an automatic restart. See reloader</p>"},{"location":"configuration/#skas-behavior","title":"Skas behavior","text":"<p>Here is a values file that redefines the most common variables related to SKAS behavior:</p> values.behavior.yaml <pre><code># Default value. May be overridden by component\nlog: \n  mode: json # 'json' or 'dev'\n  level: info\n\nskAuth:\n  # Define password requirement\n  passwordStrength:\n    forbidCommon: true    # Test against lists of common password\n    minimumScore: 3       # From 0 (Accept anything) to 4\n\n  tokenConfig:\n    # After this period without token validation, the session expire\n    inactivityTimeout: \"30m\"\n    # After this period, the session expire, in all case.\n    sessionMaxTTL: \"12h\"\n    # This is intended for the client CLI, for token caching\n    clientTokenTTL: \"30s\"\n\nskCrd:\n  initialUser:\n    login: admin\n    # passwordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\n    commonNames: [\"SKAS administrator\"]\n    groups:\n      - skas-admin\n</code></pre> <ul> <li>The <code>log</code> section allows you to adjust the log level and set the log mode.  By default, <code>log.mode</code> is set to <code>json</code>, which is intended for injection into an external log management system.  To have a more human-readable log format, you can set <code>log.mode</code> to <code>dev</code>.</li> <li><code>skAuth.passwordStrength</code> lets you modify the criteria for a valid password.</li> <li>The <code>skAuth.token.config</code> section configures the token lifecycle.</li> <li><code>skCrd.initialUser</code> is used to defines the default admin user. Note that <code>passwordHash</code> has been commented out,  otherwise the password would be reset on each application of these values.</li> </ul> <p>The meaning of <code>skAuth</code> and <code>skCrd</code> subsection is described in the Architecture chapter.</p> <p>To apply a modified configuration, enter the following command:</p> <pre><code>helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.behavior.yaml\n</code></pre> <p>We still need to add <code>values.init.yaml</code>, otherwise, corresponding default/empty values will be reset.</p> <p>Don't forget to restart the pod(s). See above</p>"},{"location":"configuration/#kubernetes-integration","title":"Kubernetes integration","text":"<p>Here is a values file that redefines the most common variables related to SKAS integration with Kubernetes:</p> values.k8s.yaml <pre><code>replicaCount: 1\n\n# -- Annotations to be added to the pod\npodAnnotations: {}\n\n# -- Annotations to be added to all other resources\ncommonAnnotations: {}\n\nimage:\n  pullSecrets: []\n  repository: ghcr.io/skasproject/skas\n  # -- Overrides the image tag whose default is the chart appVersion.\n  tag:\n  pullPolicy: IfNotPresent\n\n# Node placement of SKAS pod(s) \nnodeSelector: {}\ntolerations: []\naffinity: {}\n</code></pre> <ul> <li><code>replicaCount</code> allows you to define the number of pod replicas for the SKAS deployment. Note that we are in an active-active configuration with no need for a leader election mechanism.</li> <li><code>podAnnotations</code> and <code>commonAnnotations</code> allow you to annotate pods and other SKAS resources if required.</li> <li>The <code>image</code> subsection allows you to define an alternate image version or location.  This is useful in an air-gap deployment where the SKAS image is stored in a private repository.</li> <li><code>nodeSelector</code>, <code>toleration</code>, and <code>affinity</code> are standard Kubernetes properties related to the node placement of  SKAS pod(s). See below</li> </ul> <p>To apply a modified configuration, enter the following command:</p> <pre><code>helm -n skas-system upgrade skas skas/skas \\\n--values ./values.init.yaml --values ./values.behavior.yaml --values ./values.k8s.yaml\n</code></pre> <p>Remember to restart the pod(s) after making these configuration changes. See above</p>"},{"location":"configuration/#skas-pods-node-placement","title":"SKAS PODs node placement","text":"<p>With the default configuration, SKAS pods will be scheduled on worker nodes, just like any other workload.</p> <p>To place them on nodes carrying the control plane, the following configuration can be used:</p> values.k8s.yaml <pre><code>replicaCount: 2\n\n# -- Annotations to be added to the pod\npodAnnotations: {}\n\n# -- Annotations to be added to all other resources\ncommonAnnotations: {}\n\nimage:\n  pullSecrets: []\n  repository: ghcr.io/skasproject/skas\n  # -- Overrides the image tag whose default is the chart appVersion.\n  tag:\n  pullPolicy: IfNotPresent\n\n# Node placement of SKAS pod(s) \nnodeSelector: {}\ntolerations:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/master\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/control-plane\n\naffinity:\n  nodeAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n      - preference:\n          matchExpressions:\n            - key: node-role.kubernetes.io/control-plane\n              operator: In\n              values:\n                - \"\"\n        weight: 100\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchExpressions:\n              - key: app.kubernetes.io/instance\n                operator: In\n                values:\n                  - skas\n          topologyKey: kubernetes.io/hostname\n</code></pre> <p>By default, Kubernetes prevents standard workloads from running on the control-plane nodes by using the <code>taint</code>  mechanism. To work around this limitation, a <code>toleration</code> section is defined.</p> <p>Then there is the <code>affinity.nodeAffinity part</code>, which instructs the pod to run on nodes with a control-plane label.</p> <p>Additionally, there is the <code>affinity.podAntiAffinity</code> part, which prevents two SKAS pods from being scheduled on  the same node.</p>"},{"location":"delegated/","title":"Delegated users management","text":"<p>As the two configurations are quite similar, there is a lot of redundancy between this chapter and  Two LDAP servers configuration chapter</p> <p>Aim of this configuration is the ability to delegate the management of a certain set of users and/or their group bindings.</p> <p>But, we want to restrict the rights of the administrator of the delegated space. Especially , we don't want them to be  able to promote themself to global system administrator.</p> <p></p> <p>In this sample configuration, we will set up a separate user database for a department 'dep1'.</p> <p>To achieve this, the solution is to create a specific namespace, <code>dep1-userdb</code>, which will host <code>skusers</code> and <code>groupBinding</code> SKAS resources.</p> <p>To manage this namespace, we need to instantiate a second Identity Provider of type <code>skCrd</code>.</p> <p>For the reasons described in Two LDAP servers configuration, we need to instantiate this POD as  a separate Helm deployment, although using the same Helm Chart.</p> <p>This configuration requires two steps:</p> <ul> <li>Set up a new Helm deployment for the <code>skas2</code> pod.</li> <li>Reconfigure the <code>skMerge</code> module of the main SKAS pod to connect to this new IDP.</li> </ul> <p></p> <p>In the following, three variants of this configuration will be described: one with the connection in clear text and  two secured options with network encryption and inter-pod authentication.</p> <p>It is suggested that, even if your goal is to achieve a fully secured configuration, you begin by implementing the  unsecured, simplest variant first. Then, you can incrementally modify it as described.</p>"},{"location":"delegated/#clear-text-connection","title":"Clear text connection","text":""},{"location":"delegated/#secondary-pod-configuration","title":"Secondary POD configuration","text":"<p>Here is a sample values file for configuring the secondary POD:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskLdap:\nenabled: false\nskCrd:\nenabled: true\nnamespace: dep1-userdb\nadminGroups:\n- dep1-admin\ninitialUser:\nlogin: dep1-admin\npasswordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\ncommonNames: [\"DEP1 administrator\"]\ngroups:\n- admin\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7112\nssl: false\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"*\"\nsecret: \"*\"\nprotected: true\n</code></pre> <ul> <li> <p>At the beginning of the file, we disable all modules except <code>skCrd</code>.</p> </li> <li> <p><code>skCrd.namespace: dep1-userdb</code> defines the namespace that this IDP will use to manage user information.</p> </li> <li> <p>Then, we define an adminGroup: <code>dep1-admin</code>. The Helm chart will set up RBAC rules to allow members of this group  to access SKAS user resources in the specified namespace.</p> </li> <li> <p>Next, we create an initial admin user, <code>dep1-admin</code>, who belongs to the group <code>admin</code> (not <code>dep1-admin</code>. More on this later).</p> </li> </ul> <p>After that, there is the <code>exposure</code> section, which defines how this service will be exposed. (The default  configuration exposes it to <code>localhost</code> in clear text).</p> <ul> <li><code>exposure.internal.enabled: false</code> disables the HTTP server bound to localhost.</li> <li><code>exposure.external.enabled: true</code> enables the HTTP server bound to the POD IP, on port 7112. SSL is disabled for  this unsecure configuration.</li> <li>Then, there is the configuration of the <code>identity</code> service to expose:<ul> <li><code>clients[]</code> is a mechanism for validating who can access this service by providing an id and a secret   (or password). The value \"*\" will disable this feature.</li> <li><code>protected: true</code> activates an internal mechanism against brute force attacks by introducing delays on   unsuccessful connection attempts and limiting the number of simultaneous connections. There is no reason to   disable it unless you suspect misbehavior.</li> </ul> </li> </ul> <p>To deploy this configuration, execute:</p> <pre><code>helm -n skas-system install skas2 skas/skas --values ./values.skas2.yaml\n</code></pre> <p>Note the `skas2' release name</p> <p>The Helm chart will deploy the new pod(s), under the name <code>skas2</code>. it will also deploy an associated Kubernetes <code>service</code>.</p>"},{"location":"delegated/#main-pod-reconfiguration","title":"Main pod reconfiguration","text":"<p>The second step is to reconfigure the main pod. Here is a sample of the appropriate configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: http://skas2-crd.skas-system.svc\n</code></pre> <p>There is two entries aimed to configure a provider on the bottom of the <code>skMerge</code> module:</p> <ul> <li> <p><code>providers</code> is a list of the connected providers, which allow you to define their behavior. For more information, refer to the   Identity Provider chaining: Provider configuration chapter.The order of    providers in this list is important.</p> <p>Note also the <code>groupPattern: \"dep1-%s\"</code>.</p> </li> <li> <p><code>providerInfo</code> is a map that provides information on how to reach these providers.For <code>crd</code>, we use the   default <code>localhost</code> port.For <code>crd_dep1</code> we use the service created by the <code>skas2</code> deployment.</p> </li> </ul> <p>The link between these two entries is of course the provider name.</p> <p>The crd provider must be the first in the list. Otherwise, an administrator of <code>dep1</code> may create a user with the same name  as an existing administrator and gain authority over its password, to gain full Kubernetes access.</p> <p>The reconfiguration must then be applied by executing this command:</p> <pre><code>helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml --values ./values.skas.yaml\n</code></pre> <p>Don't forget to include the <code>values.init.yaml</code> file or merge it into the <code>values.skas.yaml</code> file. Additionally, if you have other values files, make sure to include them in each upgrade.</p> <p>Also, remember to restart the pod(s) after making these configuration changes. You can find more information on how to do this in the Configuration: Pod restart section.</p>"},{"location":"delegated/#test-and-usage","title":"Test and Usage","text":"<p>Now, you can test your configuration. You can log in using the <code>dep1-admin</code> user created in the previous deployment:\"</p> <pre><code>$ kubectl sk login\n&gt; Login:dep1-admin\n&gt; Password:\n&gt; logged successfully..\n</code></pre> <p>Please note that the password for this user is set to 'admin,' and it's highly recommended to change it as soon  as possible using the <code>kubectl sk password</code> command.</p> <p>Now, let's take a look at what our account looks like:</p> <pre><code>$ kubectl sk whoami\n&gt; USER         ID   GROUPS\n&gt; dep1-admin   0    dep1-admin\n</code></pre> <p>Note the group <code>dep1-admin</code>, which includes the prefix <code>dep1-</code> as configured in the <code>groupPattern: \"dep1-%s\"</code> setting  above.</p>"},{"location":"delegated/#user-management","title":"User management","text":"<p>As <code>dep1-admin</code>, you have management access to users in the <code>dep1-userdb</code> namespace:</p> <pre><code>$ kubectl sk -n dep1-userdb user create fred --commonName \"Fred Astair\" --password \"GtaunPMgP5f\"\n&gt; User 'fred' created in namespace 'dep1-userdb'.\n\n$ kubectl sk -n dep1-userdb user bind fred managers\n&gt; GroupBinding 'fred.managers' created in namespace 'dep1-userdb'.\n\n$ kubectl -n dep1-userdb get skusers\n&gt; NAME         COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\n&gt; dep1-admin   [\"DEP1 administrator\"]\n&gt; fred         [\"Fred Astair\"]                                   false\n$ kubectl -n dep1-userdb get groupbindings\n&gt; NAME               USER         GROUP\n&gt; dep1-admin-admin   dep1-admin   admin\n&gt; fred.managers      fred         managers\n</code></pre> <p>Then you can test the user 'fred'</p> <pre><code>$ kubectl sk login\n&gt; Login:fred\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl sk whoami\n&gt; USER   ID   GROUPS\n&gt; fred   0    dep1-managers\n</code></pre> <p>_Note the group prefixed by <code>dep1-</code>. This will ensure no user managed by this identity provider can belong to some  strategic groups such as <code>skas-admin</code> or <code>system:masters</code>.</p> <p>Now, log back in as <code>dep1-admin</code> to ensure that we are limited to our namespace.</p> <pre><code>$ kubectl sk login dep1-admin\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl -n skas-system get skusers\n&gt; Error from server (Forbidden): users.userdb.skasproject.io is forbidden: User \"dep1-admin\" cannot list resource \"users\" in API group \"userdb.skasproject.io\" in the namespace \"skas-system\"\n$ kubectl get --all-namespaces skusers\n&gt; Error from server (Forbidden): users.userdb.skasproject.io is forbidden: User \"dep1-admin\" cannot list resource \"users\" in API group \"userdb.skasproject.io\" at the cluster scope\n</code></pre> <p>The <code>sk user describe</code> subcommand is also unauthorized because it is a cross-provider feature.</p> <pre><code>$ kubectl sk user describe fred\n&gt; Unauthorized!\n</code></pre>"},{"location":"delegated/#default-namespace","title":"Default namespace","text":"<p>Providing the namespace for each command can be tedious. It can be set as the default for both <code>kubectl</code> and  <code>kubectl-sk</code> subcommands:\"</p> <pre><code>kubectl config set-context --current --namespace=dep1-userdb\n</code></pre> <p>Then:</p> <pre><code>$ kubectl get skusers\n&gt; NAME         COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\n&gt; dep1-admin   [\"DEP1 administrator\"]\n&gt; fred         [\"Fred Astair\"]                                   false\n</code></pre> <p>Alternatively, it can be set using an environment variable</p> <pre><code>export SKAS_NAMESPACE=\"dep1-userdb\"\n</code></pre> <p>But this last method will only apply on <code>kubectl-sk</code> subcommands</p>"},{"location":"delegated/#user-describe","title":"User describe","text":"<p>As stated above, a <code>dep1-admin</code> user is not allowed to use the <code>kubectl sk user describe</code> subcommand.</p> <p>This control is performed by the <code>skAuth</code> module, with a list of allowed groups. Here is a modified version of the values  file that allows <code>dep1-admin</code> members to perform a user describe subcommand.</p> values.skas.yaml <pre><code>skAuth:\n# Members of these group will be allowed to perform 'kubectl-sk user describe'\n# Also, they will be granted by RBAC to access token resources\nadminGroups:\n- skas-admin\n- dep1-admin\nskMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: http://skas2-crd.skas-system.svc\n</code></pre> <p>As mentioned in the comment, these users will also be able to view and delete session tokens.</p> <p>So, please be aware that setting a group as an admin for the <code>skAuth</code> module will allow certain operations outside the strict perimeter defined by the <code>dep1-userdb</code> namespace. However, members of such groups will still be prevented from listing or editing users and groupBindings outside of their initial namespace.</p>"},{"location":"delegated/#the-skas-admin-user","title":"The SKAS admin user","text":"<p>And what about the initial SKAS global <code>admin</code> user? Is it able to manage the <code>dep1-userdb</code> database as well?</p> <p>It depends on whether you have assigned this user to the <code>system:master</code> group. If you have, it will have full cluster access:</p> <pre><code>$ kubectl get -n dep1-userdb skusers\n&gt; NAME         COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\n&gt; dep1-admin   [\"DEP1 administrator\"]\n&gt; fred         [\"Fred Astair\"]                                   false\n</code></pre> <p>You can remove this binding (then logout/login):</p> <pre><code>$ kubectl sk whoami\n&gt; USER    ID   GROUPS\n&gt; admin   0    skas-admin,system:masters\n\n$ kubectl sk user unbind admin system:masters\n&gt; GroupBinding 'admin.system.masters' in namespace 'skas-system' has been deleted.\n\n$ kubectl sk logout\n&gt; Bye!\n\n$ kubectl sk login admin\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl sk whoami\n&gt; USER    ID   GROUPS\n&gt; admin   0    skas-admin\n</code></pre> <p>Now, access should be denied:</p> <pre><code>$ kubectl get -n dep1-userdb skusers\n&gt; Error from server (Forbidden): users.userdb.skasproject.io is forbidden: User \"admin\" cannot list resource \"users\" in API group \"userdb.skasproject.io\" in the namespace \"dep1-userdb\"\n</code></pre> <p>But, as the SKAS admin, you can promote yourself as a member of the <code>dep1-admin</code> group.</p> <pre><code>$ kubectl sk user bind admin dep1-admin\n&gt; GroupBinding 'admin.dep1-admin' created in namespace 'skas-system'.\n\n$ kubectl sk logout\n&gt; Bye!\n\n$ kubectl sk login\n&gt; Login:admin\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl sk whoami\n&gt; USER    ID   GROUPS\n&gt; admin   0    dep1-admin,skas-admin\n\n$ kubectl get -n dep1-userdb skusers\n&gt; NAME         COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\n&gt; dep1-admin   [\"DEP1 administrator\"]\n&gt; fred         [\"Fred Astair\"]                                   false\n</code></pre>"},{"location":"delegated/#securing-connection","title":"Securing connection","text":"<p>It should be noted that unencrypted passwords will transit through the link between the two pods. Therefore, setting up encryption is a must-have.</p>"},{"location":"delegated/#secondary-pod-configuration_1","title":"Secondary POD configuration","text":"<p>Here is the modified version for the <code>skas2</code> pod configuration:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskLdap:\nenabled: false\nclusterIssuer: your-cluster-issuer\nskCrd:\nenabled: true\nnamespace: dep1-userdb\nadminGroups:\n- dep1-admin\ninitialUser:\nlogin: dep1-admin\npasswordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\ncommonNames: [\"DEP1 administrator\"]\ngroups:\n- admin\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7112\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"skMerge\"\nsecret: \"aSharedSecret\"\nprotected: true\n</code></pre> <p>The differences are the following:</p> <ul> <li>There is a <code>clusterIssuer</code> definition to enable the generation of a certificate. (It is assumed here that   <code>cert-manager</code> is deployed in the cluster).</li> <li><code>exposure.external.ssl</code> is set to <code>true</code>. This will also lead to the generation of the server certificate.</li> <li>The <code>service.identity.clients</code> authentication is also activated. The <code>id</code> and <code>secret</code> values will have to be   provided by the <code>skMerge</code> client.</li> </ul> <p>To deploy this configuration:</p> <pre><code>helm -n skas-system install skas2 skas/skas --values ./values.skas2.yaml\n</code></pre> <p>Note the `skas2' release name</p> <p>The Helm chart will deploy the new pod(s) with the name <code>skas2</code>. It will also deploy an associated Kubernetes <code>service</code> and submit a <code>cert-manager.io/v1/Certificate</code> request.</p>"},{"location":"delegated/#main-pod-reconfiguration_1","title":"Main pod reconfiguration","text":"<p>Here is the modified version for the main SKAS POD configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: https://skas2-crd.skas-system.svc\nrootCaPath: /tmp/cert/skas2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: aSharedSecret\nextraSecrets:\n- secret: skas2-crd-cert\nvolume: skas2-cert\nmountPath: /tmp/cert/skas2\n</code></pre> <p>The <code>providerInfo.crd_dep1</code> has been modified for SSL and authenticated connection:</p> <ul> <li><code>url</code> begins with <code>https</code>.</li> <li><code>clientAuth</code> provides information to authenticate against the <code>skas2</code> pod.</li> <li><code>insecureSkipVerify</code> is set to <code>false</code>, as we want to check certificate validity.</li> <li><code>rootCaPath</code> is set to access the <code>ca.crt</code>, the CA validating the <code>skas2</code> server certificate.</li> </ul> <p>As stated above, during the deployment of the <code>skas2</code> secondary POD, a server certificate has been generated to allow SSL enabled services. This certificate is stored in a secret (of type <code>kubernetes.io/tls</code>) named <code>skas2-crd-cert</code>. Alongside the private/public key pair, it also contains the root Certificate Authority under the name<code>ca.crt</code>.</p> <p>The <code>skMerge.extraSecrets</code> subsection instructs the POD to mount this secret at the defined location. The property <code>skMerge.providerInfo.crd_dep1.rootCaPath</code> can now reference the mounted value.</p> <p>Then, the reconfiguration must be applied:</p> <pre><code>helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml --values ./values.skas.yaml\n</code></pre> <p>Don't forget to include the <code>values.init.yaml</code> file or merge it into the <code>values.skas.yaml</code> file. Additionally, if you have other values files, make sure to include them in each upgrade.</p> <p>Also, remember to restart the pod(s) after making these configuration changes. You can find more information on how to do this in the Configuration: Pod restart section.</p> <p>You can now test again your configuration, as described above</p>"},{"location":"delegated/#using-a-kubernetes-secrets","title":"Using a Kubernetes secrets","text":"<p>There is still a security issue because the shared secret (<code>aSharedSecret</code>) is stored in plain text in both values files, which could lead to it being accidentally committed to a version control system.</p> <p>The best practice is to store the secret value in a Kubernetes <code>secret</code> resource, like this:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: skas2-client-secret\nnamespace: skas-system\ndata:\nclientSecret: Sk1rbkNyYW5WV1YwR0E5\ntype: Opaque\n</code></pre> <p>Where <code>data.clientSecret</code> is the secret encoded in base64.</p> <p>There are several solutions to generate such a secret value. One can use Helm with a random generator function. Another one is to use a Secret generator.\"</p>"},{"location":"delegated/#secondary-pod-configuration_2","title":"Secondary POD configuration","text":"<p>To use this secret, here is the new modified version of the <code>skas2</code> POD configuration:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskLdap:\nenabled: false\nclusterIssuer: your-cluster-issuer\nskCrd:\nenabled: true\nnamespace: dep1-userdb\nadminGroups:\n- dep1-admin\ninitialUser:\nlogin: dep1-admin\npasswordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\ncommonNames: [\"DEP1 administrator\"]\ngroups:\n- admin\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7112\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"skMerge\"\nsecret: ${SKAS2_CLIENT_SECRET}\nprotected: true\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\n</code></pre> <p>The modifications are the following:</p> <ul> <li>The <code>skCrd.extraEnv</code> subsection injects the secret value as an environment variable into the container.</li> <li>the <code>skCrd.exposure.external.services.identity.clients[0].secret</code> retrieves its value through this environment variable.</li> </ul> <p>Most of the values provided by the Helm chart end up inside a ConfigMap, which is then loaded by the SKAS executable.  Environment variable interpolation occurs during this loading process.</p>"},{"location":"delegated/#main-pod-reconfiguration_2","title":"Main pod reconfiguration","text":"<p>Here is the modified version of the main SKAS pod configuration, which incorporates <code>secret</code> handling:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: https://skas2-crd.skas-system.svc\nrootCaPath: /tmp/cert/skas2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: ${SKAS2_CLIENT_SECRET}\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\nextraSecrets:\n- secret: skas2-crd-cert\nvolume: skas2-cert\nmountPath: /tmp/cert/skas2\n</code></pre> <p>The modifications for the <code>skMerge</code> module are the same as those made for the SKAS2 POD.</p>"},{"location":"delegated/#set-up-a-meta-helm-chart","title":"Set up a meta helm chart","text":"<p>Up to this point, we have configured our deployment by performing two closely related Helm deployments. To simplify automation, it can be helpful to create a 'meta chart,' a chart that includes other charts as dependencies.</p> <p>Such a chart will have the following layout:</p> <pre><code>.\n|-- Chart.yaml\n|-- templates\n|   `-- stringsecret.yaml\n`-- values.yaml\n</code></pre> <p>In this example, we will implement encryption and inter-pod authentication.</p> <p>The <code>Chart.yaml</code> file defines the meta-chart <code>skas-skas2-meta</code>. It has two dependencies that deploy the same Helm chart but with different values (as shown below). Please note the <code>alias: skas2</code> in the second deployment.</p> Chart.yaml <pre><code>apiVersion: v2\nname: skas-skas2-meta\nversion: 0.1.0\ndependencies:\n- name: skas\nversion: 0.2.1\nrepository: https://skasproject.github.io/skas-charts\n- name: skas\nalias: skas2\nversion: 0.2.1\nrepository: https://skasproject.github.io/skas-charts\n</code></pre> <p>The following manifest will generate the shared secret required for inter-pod authentication.</p>  templates/stringsecret.yaml <pre><code>---\napiVersion: \"secretgenerator.mittwald.de/v1alpha1\"\nkind: \"StringSecret\"\nmetadata:\nname: skas2-client-secret\nnamespace: skas-system\nspec:\nfields:\n- fieldName: \"clientSecret\"\nencoding: \"base64\"\nlength: \"15\"\n</code></pre> <p>And here is the global <code>values.yaml</code> file:</p> values.yaml <pre><code>skas:\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.kspray6\nkubeconfig:\ncontext:\nname: skas@kspray6\ncluster:\napiServerUrl: https://kubernetes.ingress.kspray6\nskMerge:\nproviders:\n- name: crd\n- name: crd_dep1\ngroupPattern: \"dep1-%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\ncrd_dep1:\nurl: https://skas-skas2-crd.skas-system.svc # Was https://skas2-crd.skas-system.svc\nrootCaPath: /tmp/cert/skas2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: ${SKAS2_CLIENT_SECRET}\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\nextraSecrets:\n- secret: skas-skas2-crd-cert  # Was skas2-crd-cert\nvolume: skas2-cert\nmountPath: /tmp/cert/skas2\nskas2:\nskAuth:\nenabled: false\nskMerge:\nenabled: false\nskLdap:\nenabled: false\nclusterIssuer: your-cluster-issuer\nskCrd:\nenabled: true\nnamespace: dep1-userdb\nadminGroups:\n- dep1-admin\ninitialUser:\nlogin: dep1-admin\npasswordHash: $2a$10$ijE4zPB2nf49KhVzVJRJE.GPYBiSgnsAHM04YkBluNaB3Vy8Cwv.G  # admin\ncommonNames: [ \"DEP1 administrator\" ]\ngroups:\n- admin\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7112\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"skMerge\"\nsecret: ${SKAS2_CLIENT_SECRET}\nprotected: true\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\n</code></pre> <p>There are two blocks: <code>skas</code> and <code>skas2</code>, matching the name or alias in the <code>Chart.yaml</code> file.</p> <p>These two blocks hold the same definitions as the ones defined in the original configuration, with two differences:</p> <ul> <li><code>skas.skMerge.providerInfo.crd_dep1.url: https://skas-skas2-crd.skas-system.svc</code></li> <li><code>skas.skMerge.extraSecrets[0].secret: skas-skas2-crd-cert</code></li> </ul> <p>This is to accommodate service and secret name changes due to aliasing of the second dependency.</p> <p>Then, to launch the deployment, in the same folder as <code>Chart.yaml</code>, execute:</p> <pre><code>helm dependency build &amp;&amp; helm -n skas-system upgrade -i skas .\n</code></pre>"},{"location":"dex/","title":"DEX integration","text":"<p>DEX is an OpenID Connect provider. As such, it will serve OpenID Connect clients to provide Single Sign-On services.</p> <p>DEX does not host any user identity information by itself but relies on other Identity Providers for this through configurable <code>connectors</code>.</p> <p>A connector has been developed for SKAS. As DEX does not provide an extension mechanism, adding a connector requires  patching the code. So, a specific DEX image with a SKAS connector has been built.</p> <p>Deploying DEX in standalone mode requires two operations:</p> <ul> <li>Reconfiguring SKAS to open a service for the usage of DEX (the <code>login</code> service).</li> <li>Deploying DEX itself with the proper connector configuration.</li> </ul> <p>In the following, three variants of this configuration will be described. One with the connection in clear text, and two secured, with network encryption and inter-pod authentication.</p> <p>Even if your target is a fully secured configuration, we suggest you first implement the unsecured, simplest variant, and then modify it incrementally, as described.</p>"},{"location":"dex/#clear-text-connection","title":"Clear text connection","text":""},{"location":"dex/#skas-reconfiguration","title":"SKAS reconfiguration","text":"<p>The <code>login</code> service is provided by the <code>skAuth</code> module in SKAS. By default, this service is disabled and must be  enabled to be used. You can find more information about SKAS modules and interfaces in the  Architecture/Modules and interface section.</p> <p>Below is a sample values file that enables the login service:</p> values.skas.login.yaml <pre><code>skAuth:\nexposure:\nexternal:\nservices:\nlogin:\ndisabled: false\n</code></pre> <p>Please note that by default, the <code>skAuth</code> module provides only SSL-encrypted services. This includes our login service.</p> <p>To deploy this configuration:</p> <pre><code>helm -n skas-system upgrade -i skas skas/skas --values ./values.init.yaml --values ./values.skas.login.yaml\n</code></pre> <p>Don't forget to include the <code>values.init.yaml</code> file or merge it into the <code>values.skas.yaml</code> file. Additionally, if you have other values files, make sure to include them in each upgrade.</p> <p>Also, remember to restart the pod(s) after making these configuration changes. You can find more information on how to do this in the Configuration: Pod restart section.</p>"},{"location":"dex/#dex-deployment","title":"DEX deployment","text":"<p>For this example, we will use the official DEX Helm chart.  However, this will require some configuration adjustments by providing a specific value file.</p> <p>Here is such sample file (Please note that some values may need to be customized to match your specific context):</p> values.dex.yaml <pre><code>image:\nrepository: ghcr.io/skasproject/dex\ntag: v2.37.0-skas-0.2.1\nconfig:\nissuer: http://dex.ingress.mycluster.internal\nstorage:\ntype: kubernetes\nconfig:\ninCluster: true\nweb:\nhttp: 0.0.0.0:5556\nlogger:\nlevel: info\nformat: text\noauth2:\nskipApprovalScreen: true\nconnectors:\n- type: skas\nid: skas\nname: SKAS\nconfig:\nloginPrompt: \"User\"\nloginProvider:\nurl: https://skas-auth.skas-system.svc\ninsecureSkipVerify: true\nstaticClients:\n- id: example-app\nredirectURIs:\n- 'http://127.0.0.1:5555/callback'\nname: 'Example App'\nsecret: ZXhhbXBsZS1hcHAtc2VjcmV0\nsecurityContext:\nallowPrivilegeEscalation: false\ncapabilities:\ndrop:\n- ALL\nreadOnlyRootFilesystem: false\nrunAsNonRoot: true\nrunAsUser: 1000\nseccompProfile:\ntype: RuntimeDefault\ningress:\nenabled: true\nclassName: nginx\nhosts:\n- host: dex.ingress.mycluster.internal\npaths:\n- path: /\npathType: ImplementationSpecific\nbackend:\nservice:\nname: dex\nport:\nnumber: 5556\n</code></pre> <p>Here are some comments about this values file:</p> <ul> <li>The <code>image</code> section is used to specify the SKAS patched image of DEX.</li> <li>The <code>config</code> section represents the DEX configuration file. For a more detailed explanation, please refer to the   sample DEX config file.</li> <li>The <code>config.issuer</code> value needs to be adjusted according to your local domain name. Note that it is an unsecured URL.</li> <li>The <code>config.connnectors[0]</code> is the SKAS specific section. </li> <li>The <code>config.connnectors[0].config.loginProvider.url</code> value points to the <code>skAuth</code> service. As mentioned earlier,    this service uses SSL encryption</li> <li>The <code>config.connnectors[0].config.loginProvider.insecureSkipVerify</code> is set to <code>true</code>. This is because the targeted    service uses HTTPS, and we're skipping the certificate authority validation for this first sample.</li> <li>the <code>staticClients</code> section defines a first OIDC client with parameters compatible with the <code>example-app</code> described below. </li> <li>The <code>securityContext</code> section specifies some security constraints. This can be useful if your cluster enforces   security restrictions on running PODs.</li> <li>The <code>ingress</code> section should be adjusted, especially for the <code>host:</code> URL, and possibly more if you're using an   ingress controller other than nginx.  </li> </ul> <p>This will add the DEX Helm chart repository to your Helm configuration and update the list of available charts. </p> <pre><code>helm repo add dex https://charts.dexidp.io\nhelm repo update\n</code></pre> <p>Now you can proceed to deploy DEX using Helm with the provided values file.</p> <pre><code>helm -n skas-system upgrade -i dex dex/dex --values ./values.dex.yaml\n</code></pre> <p>f everything is set up correctly, you should have two running Pods:</p> <pre><code>$ kubectl -n skas-system get pods\n&gt; NAME                    READY   STATUS    RESTARTS   AGE\n&gt; dex-54b4698bcd-9wbz6    1/1     Running   0          5h5m\n&gt; skas-5cc75b8ff9-pw7nd   3/3     Running   0          6h23m\n</code></pre> <p>In case of problems, you may want to check the resulting configuration. Unfortunately, this Helm chart stores it in a  secret. This means the configuration values are encoded in base64.</p> <p>To display it, you can use the following command:</p> <pre><code>kubectl get secret -n skas-system dex -o jsonpath=\"{ $.data.config\\.yaml }\" | base64 -d\n</code></pre> <p>If you modify some value in the <code>values.skas.login.yaml</code> file, execute again the Helm deployment command and  restart the DEX Pod:</p> <pre><code>helm -n skas-system upgrade -i dex dex/dex --values ./values.dex.yaml\nkubectl -n skas-system rollout restart deployment dex\n</code></pre>"},{"location":"dex/#testing","title":"Testing","text":"<p>By convention, all OIDC providers must provide a <code>well-known</code> endpoint that describes their other endpoints and  configuration values. </p> <p>You can test this endpoint with the following command:</p> <pre><code>$ curl http://dex.ingress.mycluster.internal/.well-known/openid-configuration\n&gt; {\n&gt;   \"issuer\": \"http://dex.ingress.mycluster.internal\",\n&gt;   \"authorization_endpoint\": \"http://dex.ingress.mycluster.internal/auth\",\n&gt;   \"token_endpoint\": \"http://dex.ingress.mycluster.internal/token\",\n&gt;   \"jwks_uri\": \"http://dex.ingress.mycluster.internal/keys\",\n&gt;   ....\n</code></pre> <p>Of course, you must adjust the URL to match your specific context</p> <p>Running the provided command will help ensure that DEX is up and running and that your ingress configuration  is functional.</p> <p>To delve deeper, DEX provides a raw example-app.  The primary objective of this application is to offer a starting point for developers to integrate an OIDC client into  their code. However, it also provides an interactive tool for testing an OIDC service.</p> <p>For your convenience, we have set up a repository to host  binaries of this application for various  operating systems and processors.</p> <p>For instance, to download and install this binary on a Mac Intel:</p> <pre><code>cd /tmp\ncurl -L https://github.com/skasproject/dex-example-app/releases/download/2.37.0/example-app_2.37.0_darwin_amd64 -o ./example-app\nsudo chmod 755 example-app\nsudo mv example-app /usr/local/bin\n</code></pre> <p>Then, you can launch it with the DEX issuer URL:</p> <pre><code>$ example-app --issuer  http://dex.ingress.mycluster.internal\n&gt; 2023/08/28 18:50:07 listening on http://127.0.0.1:5555\n</code></pre> <p>Now, launch your browser and navigate to the provided link (http://127.0.0.1:5555). You should land on a page like this:</p> <p></p> <p>Click on the <code>Login</code> button. You should then be directed to a login page:</p> <p></p> <p>Enter the credentials of a valid SKAS user account (e.g., 'admin'), and you should be directed to a page like this:</p> <p></p> <p>This is not really 'user-friendly', but it is a test application.</p> <p>You can take a look at the logs for both SKAS and DEX.</p> <p>You can also test an invalid login.</p> <p>To ensure this works correctly, you must maintain the configuration of <code>staticClients</code> in the DEX config file intact.</p> <p>The DEX GitHub repo also provides the <code>example-app</code> as a container. You can launch it using the following command:\"</p> <pre><code>$ docker run -p 5555:5555 ghcr.io/dexidp/example-app:latest  example-app --issuer  http://dex.ingress.mycluster.internal --listen http://0.0.0.0:5555\n&gt; 2023/08/28 17:29:04 listening on http://0.0.0.0:5555\n</code></pre>"},{"location":"dex/#securing-connection","title":"Securing connection","text":"<p>The previous configuration has a significant security issue: The login and password information are transmitted over  an unencrypted connection.</p> <p>The following configuration aims to address this vulnerability by implementing secure communication between user's browser and DEX. It also add authentication from DEX to SKAS. Additionally, it verifies the authenticity of the SKAS certificate.\"</p>"},{"location":"dex/#skas-reconfiguration_1","title":"SKAS reconfiguration","text":"<p>The following is the modified values file for SKAS reconfiguration:</p> values.skas.login.yaml <pre><code>skAuth:\nexposure:\nexternal:\nservices:\nlogin:\ndisabled: false\nclients:\n- id: dex\nsecret: \"aSharedSecret\"\n</code></pre> <p>The <code>service</code> authentication has been activated.</p> <p>To deploy this configuration, use the same command as previously:</p> <pre><code>helm -n skas-system upgrade -i skas skas/skas --values ./values.init.yaml --values ./values.skas.login.yaml\n</code></pre> <p>Don't forget to include the <code>values.init.yaml</code> file or merge it into the <code>values.skas.yaml</code> file. Additionally, if you have other values files, make sure to include them in each upgrade.</p> <p>Also, remember to restart the pod(s) after making these configuration changes. You can find more information on how to do this in the Configuration: Pod restart section.</p>"},{"location":"dex/#dex-deployment_1","title":"DEX deployment","text":"<p>And here is the modified values file for DEX deployment</p> values.dex.yaml <pre><code>image:\nrepository: ghcr.io/skasproject/dex\ntag: v2.37.0-skas-0.2.1\nconfig:\nissuer: https://dex.ingress.mycluster.internal\nstorage:\ntype: kubernetes\nconfig:\ninCluster: true\nweb:\nhttp: 0.0.0.0:5556\nlogger:\nlevel: info\nformat: text\noauth2:\nskipApprovalScreen: true\nconnectors:\n- type: skas\nid: skas\nname: SKAS\nconfig:\nloginPrompt: \"User\"\nloginProvider:\nurl: https://skas-auth.skas-system.svc\nrootCaPath: \"\"\nrootCaData: \"LS0tLS1CRUdJTiBDRVJU.......................ENFUlRJRklDQVRFLS0tLS0K\"\ninsecureSkipVerify: false\nclientAuth:\nid: \"dex\"\nsecret: \"aSharedSecret\"\nstaticClients:\n- id: example-app\nredirectURIs:\n- 'http://127.0.0.1:5555/callback'\nname: 'Example App'\nsecret: ZXhhbXBsZS1hcHAtc2VjcmV0\nsecurityContext:\nallowPrivilegeEscalation: false\ncapabilities:\ndrop:\n- ALL\nreadOnlyRootFilesystem: false\nrunAsNonRoot: true\nrunAsUser: 1000\nseccompProfile:\ntype: RuntimeDefault\ningress:\nenabled: true\nclassName: nginx\nannotations:\ncert-manager.io/cluster-issuer: your-cluster-issuer\nnginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\nhosts:\n- host: dex.ingress.mycluster.internal\npaths:\n- path: /\npathType: ImplementationSpecific\ntls:\n- secretName: dex-server-tls\nhosts:\n- dex.ingress.kspray6\n</code></pre> <p>The modification are the following:</p> <ul> <li>The <code>config.issuer</code> endpoint now uses HTTPS.</li> <li>The <code>config.connectors[0].config.loginProvider.rootCaData</code> is populated with the Certificate Authority of the  <code>skAuth</code> service. To obtain its value, you can extract it from the service's certificate as follows:   <pre><code>kubectl -n skas-system get secret skas-auth-cert -o=jsonpath='{.data.ca\\.crt}'\n</code></pre></li> <li>The <code>config.connectors[0].config.loginProvider.clientAuth</code> is configured to authenticate using the ID/secret defined    earlier for the <code>skAuth</code> login service.</li> <li>The <code>ingress</code> is now configured to handle SSL connections and enforce SSL for non-SSL connections.- </li> </ul> <p>To apply this new configuration, use the same command as before.</p> <pre><code>helm -n skas-system upgrade -i dex dex/dex --values ./values.dex.yaml\n</code></pre> <p>And restart the DEX POD:</p> <pre><code>kubectl -n skas-system rollout restart deployment dex\n</code></pre> <p>Another security enhancement involves configuring the ingress for SSL passthrough and enabling SSL for the DEX  Pod itself. This would ensure end-to-end encryption. However, please note that achieving this configuration with the  current DEX Helm Chart is not feasible, and refactoring it is beyond the scope of this documentation.\"</p>"},{"location":"dex/#testing_1","title":"Testing","text":"<p>You can test again the DEX <code>well-known</code> endpoint with the following command. Note the https:// now on URLs You can test the DEX <code>well-known</code> endpoint again using the following command.</p> <pre><code>$ curl https://dex.ingress.mycluster.internal/.well-known/openid-configuration\n&gt; {\n&gt;   \"issuer\": \"https://dex.ingress.mycluster.internal\",\n&gt;   \"authorization_endpoint\": \"https://dex.ingress.mycluster.internal/auth\",\n&gt;   \"token_endpoint\": \"https://dex.ingress.mycluster.internal/token\",\n&gt;   \"jwks_uri\": \"https://dex.ingress.mycluster.internal/keys\",\n&gt;   \"userinfo_endpoint\": \"https://dex.ingress.mycluster.internal/userinfo\", &gt;   ....\n</code></pre> <p>Please note the change to https:// in the URLs.\"</p> <p>Use the <code>example-app</code> application again, but this time note the 'https://' in the issuer.</p> <pre><code>$ example-app --issuer  https://dex.ingress.mycluster.internal\n&gt; 2023/08/28 18:50:07 listening on http://127.0.0.1:5555\n</code></pre>"},{"location":"dex/#encountering-a-certificate-issue","title":"Encountering a certificate issue?","text":"<p>You might see the following error on the curl request:</p> <pre><code>$ curl https://dex.ingress.mycluster.internal/.well-known/openid-configuration\n&gt; curl: (60) SSL certificate problem: unable to get local issuer certificate\n&gt; More details here: https://curl.haxx.se/docs/sslcerts.html\n&gt; ....\n</code></pre> <p>This issue occurs when the DEX issuer's certificate is signed by an authority that your workstation don't recognize. </p> <p>To resolve this, you need to retrieve the issuer's certificate:</p> <pre><code>kubectl -n skas-system get secret dex-server-tls -o=jsonpath='{.data.ca\\.crt}' | base64 -d &gt;./CA.crt\n</code></pre> <p>And to provide it to the Curl command:</p> <pre><code>$ curl https://dex.ingress.mycluster.internal/.well-known/openid-configuration \\\n--cacert ./CA.crt\n&gt; {\n&gt;   \"issuer\": \"https://dex.ingress.mycluster.internal\",\n&gt;   \"authorization_endpoint\": \"https://dex.ingress.mycluster.internal/auth\",\n&gt;   \"token_endpoint\": \"https://dex.ingress.mycluster.internal/token\",\n&gt;   \"jwks_uri\": \"https://dex.ingress.mycluster.internal/keys\",\n&gt;   \"userinfo_endpoint\": \"https://dex.ingress.mycluster.internal/userinfo\", &gt;   ....\n</code></pre> <p>You may encounter the same issue with the <code>example-app</code> test application. In this case, also provide the certificate:</p> <pre><code>$ example-app --issuer https://dex.ingress.kspray6 --issuer-root-ca ./CA.crt\n&gt; 2023/08/29 09:31:53 listening on http://127.0.0.1:5555\n</code></pre>"},{"location":"dex/#using-a-kubernetes-secret","title":"Using a Kubernetes secret","text":"<p>There is still a security issue as two shared secrets (aSharedSecret and the staticClients secret)  are stored in plain text in both values files. As a result, they could potentially end up in a version control system.</p> <p>So, let's store these values in Kubernetes secrets and access them using environment variables.</p> <p>Here is a secret intended to be shared between DEX and SKAS. Its value can be randomly generated, as it is accessed by both parties.</p> dex-client-secret.yaml <pre><code>apiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\nname: dex-client-secret\nnamespace: skas-system\ndata:\nDEX_CLIENT_SECRET: cGZRM3lXSTBBN2M3aGJE\n</code></pre> <p>There are several solutions to generate such a secret value. One can use Helm with some random generator function or use a Secret generator.\"</p> <p>Here is the secret shared between DEX (in <code>config.staticClients[0]</code>) and the <code>example-app</code> application binary.\"</p> example-app-secret.yaml <pre><code>apiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\nname: example-app-secret\nnamespace: skas-system\ndata:\nEXAMPLE_APP_SECRET: WlhoaGJYQnNaUzFoY0hBdGMyVmpjbVYw    # Result of printf \"ZXhhbXBsZS1hcHAtc2VjcmV0\" | base64\n</code></pre> <p>Its value is hard-coded in <code>example-app</code>, so it must not be changed (or you must pass the new value as the <code>--client-secret parameter</code> on launch).</p> <p>Note that both secrets are formatted in a way that is compatible with <code>spec.containers[X].envFrom</code>. This is required by the DEX Helm chart.</p>"},{"location":"dex/#skas-reconfiguration_2","title":"SKAS reconfiguration","text":"<p>Here is the modified values file for the SKAS reconfiguration:</p> values.skas.login.yaml <pre><code>skAuth:\nexposure:\nexternal:\nservices:\nlogin:\ndisabled: false\nclients:\n- id: dex\nsecret: ${DEX_CLIENT_SECRET}\nextraEnv:\n- name: DEX_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: dex-client-secret\nkey: DEX_CLIENT_SECRET\n</code></pre> <p>The modifications are the following:</p> <ul> <li>The <code>skAuth.extraEnv</code> subsection injects the secret value as an environment variable in the container.   The <code>skAuth.exposure.external.services.identity.clients[0].secret</code> fetches its value through this environment variable.</li> </ul> <p>Most of the values provided by the Helm chart end up inside a <code>configMap</code>, which is then loaded by the SKAS  executable. The environment variable interpolation occurs during this load.</p>"},{"location":"dex/#dex-deployment_2","title":"DEX deployment","text":"<p>Here is the modified values file for DEX deployment:</p> values.dex.yaml <pre><code>image:\nrepository: ghcr.io/skasproject/dex\ntag: v2.37.0-skas-0.2.1\nconfig:\nissuer: https://dex.ingress.mycluster.internal\nstorage:\ntype: kubernetes\nconfig:\ninCluster: true\nweb:\nhttp: 0.0.0.0:5556\nlogger:\nlevel: info\nformat: text\noauth2:\nskipApprovalScreen: true\nconnectors:\n- type: skas\nid: skas\nname: SKAS\nconfig:\nloginPrompt: \"User\"\nloginProvider:\nurl: https://skas-auth.skas-system.svc\nrootCaPath: \"\"\nrootCaData: \"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\"\ninsecureSkipVerify: false\nclientAuth:\nid: \"dex\"\nsecret: ${DEX_CLIENT_SECRET}\nstaticClients:\n- id: example-app\nredirectURIs:\n- 'http://127.0.0.1:5555/callback'\nname: 'Example App'\nsecretEnv: EXAMPLE_APP_SECRET\nenvFrom:\n- secretRef:\nname: dex-client-secret\n- secretRef:\nname: example-app-secret\nsecurityContext:\nallowPrivilegeEscalation: false\ncapabilities:\ndrop:\n- ALL\nreadOnlyRootFilesystem: false\nrunAsNonRoot: true\nrunAsUser: 1000\nseccompProfile:\ntype: RuntimeDefault\ningress:\nenabled: true\nclassName: nginx\nannotations:\ncert-manager.io/cluster-issuer: your-cluster-issuer\nnginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\nhosts:\n- host: dex.ingress.kspray6\npaths:\n- path: /\npathType: ImplementationSpecific\ntls:\n- secretName: dex-server-tls\nhosts:\n- dex.ingress.mycluster.internal\n</code></pre> <p>DEX handles environment variable expansion in two different ways, depending on the subsection:</p> <ul> <li>Environment variables are expanded in the usual way within the <code>connectors</code> definition.</li> <li>However, in other parts of the configuration, such as <code>staticClients</code>, standard environment variable expansion does  not occur. To address this, two new attributes have been introduced in the <code>staticClients</code> definition:  <code>idEnv</code> and <code>secretEnv</code>. They take the variable name as parameter. In our case, only the <code>secretEnv</code> attribute is used.</li> </ul> <p>Both secret values are injected into the Pod using the <code>envFrom</code> subsection.\"</p>"},{"location":"installation/","title":"INSTALLATION","text":""},{"location":"installation/#installing-with-skas-helm-chart","title":"Installing with SKAS Helm Chart","text":"<p>The most straightforward and recommended method for installing the SKAS server is by using the provided Helm chart. </p> <p>Before you begin, make sure you meet the following prerequisites:</p> <ul> <li> <p>Certificate Manager: Ensure that the Certificate Manager is deployed in your target Kubernetes cluster, and a <code>ClusterIssuer</code> is defined for certificate management.</p> </li> <li> <p>Ingress Controller: An NGINX ingress controller should be deployed in your target Kubernetes cluster.</p> </li> <li> <p>Kubectl Configuration: You should have a local client Kubernetes configuration with full administrative  rights on the target cluster.</p> </li> <li> <p>Helm: Helm must be installed locally on your system.</p> </li> </ul> <p>Follow these steps to install SKAS using Helm:</p> <ul> <li> <p>Add the SKAS Helm repository by running the following command:</p> <pre><code>helm repo add skas https://skasproject.github.io/skas-charts\n</code></pre> </li> <li> <p>Create a dedicated namespace for SKAS:</p> <pre><code>kubectl create namespace skas-system\n</code></pre> </li> <li> <p>Deploy the SKAS Helm chart using the following command:</p> <pre><code>helm -n skas-system install skas skas/skas \\\n--set clusterIssuer=your-cluster-issuer \\\n--set skAuth.exposure.external.ingress.host=skas.ingress.mycluster.internal \\\n--set skAuth.kubeconfig.context.name=skas@mycluster.internal \\\n--set skAuth.kubeconfig.cluster.apiServerUrl=https://kubernetes.ingress.mycluster.internal\n</code></pre> </li> </ul> <p>Replace the values with your specific configuration:</p> <ul> <li><code>clusterIssuer</code>: The ClusterIssuer from your Certificate Manager for certificate management.</li> <li><code>skAuth.exposure.external.ingress.host</code>: The hostname used for accessing the SKAS service from outside the cluster. <p>Make sure to define this hostname in your DNS.</p> </li> <li><code>skAuth.kubeconfig.context.name</code>: A unique context name for this cluster in your local configuration.</li> <li><code>skAuth.kubeconfig.cluster.apiServerUrl</code>: The API server URL from outside the cluster. You can find this information  in an existing Kubernetes config file under <code>clusters[X].cluster.server</code>.</li> </ul> <p>Alternatively, you can create a local YAML values file as follows:</p> values.init.yaml <pre><code>clusterIssuer: your-cluster-issuer\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.mycluster.internal\nkubeconfig:\ncontext:\nname: skas@mycluster.internal\ncluster:\napiServerUrl: https://kubernetes.ingress.mycluster.internal\n</code></pre> <p>And then install SKAS using this values file:</p> <pre><code>helm -n skas-system install skas skas/skas --values ./values.init.yaml\n</code></pre> <p>After a successful installation, verify the SKAS server pod is running:</p> <pre><code>$ kubectl -n skas-system get pods\n&gt; NAME                    READY   STATUS    RESTARTS   AGE\n&gt; skas-746c54dc75-v8v2f   3/3     Running   0          25s\n</code></pre>"},{"location":"installation/#use-another-ingress-controller-instead-of-nginx","title":"Use another ingress controller instead of nginx","text":"<p>If you are using an ingress controller other than NGINX, you can specify the ingress class by adding  the --set ingressClass=xxxx flag when launching the Helm chart. In this case, the Helm chart won't create an ingress  resource, and you will need to set up your own ingress.  (Here is the nginx definition, as a starting point.)</p> <p>Please note that the ingress is configured with <code>ssl-passthroughs</code>. The underlying service will handle SSL.</p>"},{"location":"installation/#no-certificate-manager","title":"No Certificate Manager","text":"<p>If you are not using a Certificate Manager, you can still install SKAS. Follow these steps:</p> <ul> <li>Launch the helm chart without <code>ClusterIssuer</code> definition. Then, the secret hosting the certificate for the services  will be missing, so the <code>skas</code> pod will fail</li> <li>Prepare a PEM-encoded self-signed certificate and key files. The certificate should be valid for the following hostnames:<ul> <li><code>skas-auth</code></li> <li><code>skas-auth.skas-system.svc</code></li> <li><code>localhost</code></li> <li><code>skas.ingress.mycluster.internal</code> (Adjust this to your actual hostname)</li> </ul> </li> <li>Base64-encode the CA certificate (in PEM format) and its key.</li> <li>Create a secret in the skas-system namespace:     <pre><code>$ kubectl -n skas-system create secret tls skas-auth-cert --cert=&lt;CERTIFICATE FILE&gt; --key=&lt;KEY FILE&gt;\n</code></pre></li> <li>The skas pod should start successfully.</li> </ul>"},{"location":"installation/#api-server-configuration","title":"API Server configuration.","text":"<p>The API server's Authentication Webhook must be configured to communicate with our authentication module.</p>"},{"location":"installation/#manual-configuration","title":"Manual configuration","text":"<p>Depending on your specific installation, the directory mentioned below may vary. For reference, the clusters used for testing and documentation purposes were built using kubespray.</p> <p>Additionally, this procedure assumes that the API Server is managed by the Kubelet as a static Pod. If your API Server is managed by another system, such as systemd, you should make the necessary adjustments accordingly.</p> <p>Please note that the following operations must be executed on all nodes hosting an instance of the Kubernetes API server, typically encompassing all nodes within the control plane.</p> <p>These operations require 'root' access on these nodes._</p> <p>To initiate the process, start by creating a dedicated folder for 'skas':\"</p> <pre><code>mkdir -p /etc/kubernetes/skas\n</code></pre> <p>Next, create the Authentication Webhook configuration file within this directory. You can conveniently copy and paste the following configuration:</p> /etc/kubernetes/skas/hookconfig.yaml <pre><code>apiVersion: v1\nkind: Config\n# clusters refers to the remote service.\nclusters:\n- name: sk-auth\ncluster:\ncertificate-authority: /etc/kubernetes/skas/skas_auth_ca.crt        # CA for verifying the remote service.\nserver: https://sk-auth.skas-system.svc:7014/v1/tokenReview # URL of remote service to query. Must use 'https'.\n# users refers to the API server's webhook configuration.\nusers:\n- name: skasapisrv\n# kubeconfig files require a context. Provide one for the API server.\ncurrent-context: authwebhook\ncontexts:\n- context:\ncluster: sk-auth\nuser: skasapisrv\nname: authwebhook\n</code></pre> <p>As indicated within this file, there is a reference to the certificate authority of the authentication webhook service.  Consequently, you should retrieve it and place it in this location:</p> <pre><code>kubectl -n skas-system get secret skas-auth-cert \\\n-o=jsonpath='{.data.ca\\.crt}' | base64 -d &gt;/etc/kubernetes/skas/skas_auth_ca.crt  </code></pre> <p>Please ensure that the kubectl command is installed on this node with administrator configuration.</p> <p>Inspect the folder's contents:</p> <pre><code>$ ls -l /etc/kubernetes/skas\n&gt; total 8\n&gt; -rw-r--r--. 1 root root  620 May 11 12:36 hookconfig.yaml\n&gt; -rw-r--r--. 1 root root 1220 May 11 12:58 skas_auth_ca.crt\n</code></pre> <p>Now, you need to modify the API Server manifest file located at <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> to include the <code>hookconfig.yaml</code> file:\"</p> <pre><code>vi /etc/kubernetes/manifests/kube-apiserver.yaml\n</code></pre> <p>The initial step involves adding two flags to the kube-apiserver command line:</p> <ul> <li><code>--authentication-token-webhook-cache-ttl</code>: This determines the duration for caching authentication decisions.</li> <li><code>--authentication-token-webhook-config-file</code>: This refers to the path of the configuration file we've just set up.</li> </ul> <p>This is how it should appear:</p> <pre><code>...\nspec:\ncontainers:\n- command:\n- kube-apiserver\n- --authentication-token-webhook-cache-ttl=30s\n- --authentication-token-webhook-config-file=/etc/kubernetes/skas/hookconfig.yaml\n- --advertise-address=192.168.33.16\n- --allow-privileged=true\n- --anonymous-auth=True\n...\n</code></pre> <p>The second step involves mapping the node folder <code>/etc/kubernetes/skas</code> inside the API server pod, using the same path.  This mapping is necessary because these files are accessed within the context of the API Server container.</p> <p>To achieve this, you should add a new volumeMounts entry as follows:</p> <pre><code>    volumeMounts:\n- mountPath: /etc/kubernetes/skas\nname: skas-config\n....\n</code></pre> <p>Additionally, you need to include a corresponding new volumes entry:</p> <pre><code>  volumes:\n- hostPath:\npath: /etc/kubernetes/skas\ntype: \"\"\nname: skas-config\n....\n</code></pre> <p>Furthermore, you should define another configuration parameter. Specifically, you must set the <code>dnsPolicy</code> to  <code>ClusterFirstWithHostNet</code>. Please verify that this key doesn't already exist and add or modify it accordingly:</p> <pre><code>  hostNetwork: true\ndnsPolicy: ClusterFirstWithHostNet </code></pre> <p>With these adjustments, you have completed the configuration for the API Server. Saving the edited file will trigger  a restart of the API Server to take the changes into account.</p> <p>For additional information, refer to the Kubernetes documentation on this topic, available here</p> <p>Please remember to carry out this procedure on all nodes that host an instance of the API Server.</p>"},{"location":"installation/#using-an-ansible-role","title":"Using an Ansible role","text":"<p>If Ansible is one of your preferred tools, you can automate these laborious tasks by utilizing an Ansible role.</p> <p>You can obtain such a role here.</p> <p>Similar to manual installation, you might need to customize it to suit your local context.</p> <p>To utilize this role, we assume that you have an Ansible configuration in place, along with an inventory that defines the target cluster.</p> <p>Additionally, this role utilizes the <code>kubernetes.core.k8s_info module</code>. Please review the requirements for this module</p> <p>Then, follow these steps:</p> <ul> <li>Download and extract the role archive provided above into a folder that is part of the role path.</li> <li>Create a playbook file, for example:</li> </ul> skas.yaml <pre><code>- hosts: kube_control_plane  # This group must target all the nodes hosting an instance of the kubernetes API server\ntags: [ \"skas\" ]\nvars:\nskas_state: present\nroles:\n- skas-apiserver\n</code></pre> <ul> <li>Launch this playbook:</li> </ul> <pre><code>ansible-playbook ./skas.yaml\n</code></pre> <p>The playbook will execute all the steps outlined in the manual installation process detailed above. Consequently,  this will trigger a restart of the API server.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If there is a minor typo or a configuration inconsistency, it could potentially prevent the API Server from restarting.  In such cases, it's advisable to examine the logs of the Kubelet. (Remember that, as a static pod, the API Server is  managed by the Kubelet). These logs can provide insights into what might be causing the issue.</p> <p>If you've made any modifications to the <code>hookconfig.yaml</code> file or updated the CA file, it's necessary to restart the  API Server to apply the new configuration. However, since the API Server is a 'static pod' managed by the Kubelet,  it can't be restarted like a standard pod.</p> <p>The simplest method to effectively trigger a reload of the API Server is to make a modification to the  <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code> file. It's essential that this modification is a substantive change,  as simply using the touch command may not suffice. A common approach is to make a slight modification to the  <code>authentication-token-webhook-cache-ttl</code> flag value. This will prompt the API Server to reload its configuration  and apply the changes.</p>"},{"location":"installation/#installation-of-skas-cli","title":"Installation of SKAS CLI","text":"<p>SKAS offers a command-line interface (CLI) as an extension of kubectl.</p> <p>The installation process is straightforward:</p> <ul> <li>Download the executable that corresponds to your operating system and architecture from this location.</li> <li>Rename the downloaded executable to <code>kubectl-sk</code> to adhere to the naming convention of kubectl extensions.</li> <li>Make the file executable.</li> <li>Move the <code>kubectl-sk</code> executable to a directory that is included in your system's PATH environment variable.</li> </ul> <p>For instance, on a Mac with an Intel processor, you can use the following commands:</p> <pre><code>cd /tmp\ncurl -L https://github.com/skasproject/skas/releases/download/0.2.1/kubectl-sk_0.2.1_darwin_amd64 -o ./kubectl-sk\nchmod 755 kubectl-sk\nsudo mv kubectl-sk /usr/local/bin\n</code></pre> <p>Now, you can verify whether the extension is working as intended.</p> <pre><code>kubectl sk\n</code></pre> <p>It should display:</p> <pre><code>A kubectl plugin for Kubernetes authentication\n\nUsage:\nkubectl-sk [command]\n\nAvailable Commands:\ncompletion  Generate the autocompletion script for the specified shell\nhash        Provided password hash, for use in config file\nhelp        Help about any command\ninit        Add a new context in Kubeconfig file for skas access\nlogin       Logout and get a new token\nlogout      Clear local token\npassword    Change current password\nuser        Skas user management\nversion     display skas client version\nwhoami      Display current logged user, if any\n\nFlags:\n-h, --help                help for kubectl-sk\n--kubeconfig string   kubeconfig file path. Override default configuration.\n--logLevel string     Log level (default \"INFO\")\n--logMode string      Log mode: 'dev' or 'json' (default \"dev\")\n\nUse \"kubectl-sk [command] --help\" for more information about a command.\n</code></pre> <p>SKAS is now successfully installed. You can proceed with the User guide for further instructions.</p> <p>Depending on your cluster architecture, you may need to adjust your configuration for a safer and more resilient  installation. Please refer to the Configuration: Kubernetes Integration  section for more information.</p>"},{"location":"installation/#skas-removal","title":"SKAS Removal","text":"<p>When it comes to uninstalling SKAS, the initial step involves reconfiguring the API server.  The approach depends on how you initially configured it:</p> <p>If you configured it manually, remove the two entries, <code>--authentication-token-webhook-cache-ttl</code> and  <code>--authentication-token-webhook-config-file</code>, from the API server manifest file located at  <code>/etc/kubernetes/manifests/kube-apiserver.yaml</code>.</p> <p>If you used the Ansible role for configuration, simply modify the playbook by setting <code>skas_state</code> to <code>absent</code>:</p> skas.yaml <pre><code>- hosts: kube_control_plane  # This group must target all the nodes hosting an instance of the kubernetes API server\ntags: [ \"skas\" ]\nvars:\nskas_state: absent\nroles:\n- skas-apiserver\n</code></pre> <p>After making these changes, execute the playbook:</p> <pre><code>ansible-playbook ./skas.yaml\n</code></pre> <p>Once you have successfully reconfigured the Kubernetes API server, you can proceed to uninstall the Helm chart.</p> <pre><code>helm -n skas-system uninstall skas\n</code></pre> <p>And to delete the namespace</p> <pre><code>kubectl delete namespace skas-system\n</code></pre>"},{"location":"ldap/","title":"LDAP Setup","text":""},{"location":"ldap/#overview","title":"Overview","text":"<p>SKAS supports the concept of an 'Identity Provider' (IDP). An Identity Provider can be defined as a database that hosts  users and groups, providing user authentication and information.</p> <p>Up to this point, we have used a single Identity Provider (IDP) based on resources stored in Kubernetes,  within the <code>skas-system</code> namespace. SKAS also provides an alternative Identity Provider backed by an LDAP directory.</p> <p>The SKAS LDAP identity provider code was heavily inspired by the LDAP connector of the DEX project. Many thanks to all of its contributors for their valuable contributions.</p>"},{"location":"ldap/#security-considerations","title":"Security considerations","text":"<p>SKAS attempts to bind with the underlying LDAP server using both the admin and user's plain text password.  While some LDAP implementations permit the use of hashed passwords, SKAS does not support this feature.  Instead, it strongly recommends that all administrators employ TLS (Transport Layer Security) for secure communication.</p> <p>TLS can often be enabled by using port 636 instead of the default port 389 and configuring the appropriate certificate  authority. This ensures that passwords are transmitted securely over the network.</p> <p>SKAS currently permits insecure connections to ensure compatibility with a wide range of LDAP implementations and to  simplify the initial setup process. However, it's important to note that these configurations should never be used  in a production environment, as they can actively expose and compromise passwords, posing a significant security risk.</p>"},{"location":"ldap/#ldap-configuration","title":"LDAP Configuration","text":"<p>For a more comprehensive understanding of what this configuration entails, you can refer to the Architecture Overview documentation.</p>"},{"location":"ldap/#the-helm-values-file","title":"The helm values file","text":"<p>LDAP activation and configuration will be performed by adding specific values during Helm (re)deployment. </p> <p>Here is a template of such values:</p> values.ldap.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\n# The host and port of the LDAP server.\n# If port isn't supplied, it will be guessed based on the TLS configuration. 389 or 636.\nhost:\nport:\n# Timeout on connection to ldap server. Default to 10\ntimeoutSec: 10\n# Required if LDAP host does not use TLS.\ninsecureNoSSL: false\n# Don't verify the CA.\ninsecureSkipVerify: false\n# Connect to the insecure port then issue a StartTLS command to negotiate a\n# secure connection. If not supplied secure connections will use the LDAPS protocol.\nstartTLS: false\n# Path to a trusted root certificate file, or Base64 encoded PEM data containing root CAs.\nrootCaPath:\nrootCaData:\n# If server require client authentication with certificate.\n#  Path to a client cert file and a private key file\nclientCert:\nclientKey:\n# BindDN and BindPW for an application service account. The connector uses these\n# credentials to search for users and groups.\nbindDN:\nbindPW:\nuserSearch:\n# BaseDN to start the search from. For example \"cn=users,dc=example,dc=com\"\nbaseDN:\n# Optional filter to apply when searching the directory. For example \"(objectClass=person)\"\nfilter:\n# Attribute to match against the login. This will be translated and combined\n# with the other filter as \"(&lt;loginAttr&gt;=&lt;login&gt;)\".\nloginAttr:\n#  Can either be:\n# * \"sub\" - search the whole sub tree (Default)\n# * \"one\" - only search one level\nscope: \"sub\"\n# The attribute providing the numerical user ID\nnumericalIdAttr:\n# The attribute providing the user's email\nemailAttr:\n# The attribute providing the user's common name\ncnAttr:\ngroupSearch:\n# BaseDN to start the search from. For example \"cn=groups,dc=example,dc=com\"\nbaseDN:\n# Optional filter to apply when searching the directory. For example \"(objectClass=posixGroup)\"\nfilter: (objectClass=posixgroup)\n# Defaults to \"sub\"\nscope: \"sub\"\n# The attribute of the group that represents its name.\nnameAttr: cn\n# The filter for group/user relationship will be: (&lt;linkGroupAttr&gt;=&lt;Value of LinkUserAttr for the user&gt;)\n# If there is several value for LinkUserAttr, we will loop on.\nlinkGroupAttr:\nlinkUserAttr:\n</code></pre> <ul> <li><code>skMerge</code> is the SKAS module aimed at merging information from several Identity Provider. This merging process follows  specific rules, which are be described here.</li> <li><code>skMerge.providers</code> is the ordered list of Identity Providers. </li> <li><code>crd</code> is the name of the provider managing the user database in the <code>skas-system</code> namespace.</li> <li><code>ldap</code> is the name of our LDAP provider, which will be configured under the <code>skLdap</code> subsection.</li> <li><code>skLdap.enabled</code> must be set to <code>true</code> (It is <code>false</code> by default). </li> <li><code>skLdap.ldap.*</code> is the LDAP configuration with all parameters and their description.</li> </ul> <p>To apply this configuration, use the following command:</p> <pre><code>helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.ldap.yaml\n</code></pre> <p>Don't forget to include the <code>values.init.yaml</code> file, or merge its content into the values.ldap.yaml file.  Additionally, if you have other custom values files, make sure to include them in each <code>helm upgrade</code> command as well.</p> <p>Remember to restart the SKAS pod(s) after applying any configuration changes. See Configuration: Pod restart</p> <p>In this configuration, there are two sources of identity: our original <code>skas-system</code> user database and the newly added  <code>ldap</code> server. Merging multiple sources of identity is an important aspect of SKAS identity management.  The next chapter will delve into this topic.</p> <p>After deployment, you can test your configuration using the <code>kubectl sk user describe &lt;login&gt; --explain</code> command. For more information, refer to the next chapter.</p>"},{"location":"ldap/#sample-configurations","title":"Sample configurations","text":"<p>Here is a sample configuration for connecting to an OpenLDAP server:</p> values.openldap.yaml <pre><code>skMerge:\n  providers:\n    - name: crd\n    - name: ldap\n\nskLdap:\n  enabled: true\n  # --------------------------------- LDAP configuration\n  ldap:\n    host: ldap.mydomain.internal\n    insecureNoSSL: false\n    rootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\n    bindDN: cn=Manager,dc=mydomain,dc=internal\n    bindPW: admin123\n    groupSearch:\n      baseDN: ou=Groups,dc=mydomain,dc=internal\n      filter: (objectClass=posixgroup)\n      linkGroupAttr: memberUid\n      linkUserAttr: uid\n      nameAttr: cn\n    timeoutSec: 10\n    userSearch:\n      baseDN: ou=Users,dc=mydomain,dc=internal\n      cnAttr: cn\n      emailAttr: mail\n      filter: (objectClass=inetOrgPerson)\n      loginAttr: uid\n      numericalIdAttr: uidNumber\n</code></pre> <p>Note that, since the connection uses SSL, you need to provide the Certificate Authority of the LDAP Server.  You can provide this CA in the field <code>skLdap.ldap.rootCaData</code> as a base64-encoded certificate file.</p> <p>Here is a sample configuration for connecting to a FreeIPA LDAP server.</p> values.freeipa.yaml <pre><code>skMerge:\n  providers:\n    - name: crd\n    - name: ldap\n\nskLdap:\n  enabled: true\n  # --------------------------------- LDAP configuration\n  ldap:\n    host: ipa1.mydomain.internal\n    port: 636\n    rootCaData: \"LS0tLS1CRUdJTiBDRU4zclBySE.........................JRklDQVRFLS0tLS0K\"\n    bindDN: uid=admin,cn=users,cn=accounts,dc=mydomain,dc=internal\n    bindPW: ipaadmin\n    userSearch:\n      baseDN: cn=users,cn=accounts,dc=mydomain,dc=internal\n      emailAttr: mail\n      filter: (objectClass=inetOrgPerson)\n      loginAttr: uid\n      numericalIdAttr: uidNumber\n      cnAttr: cn\n    groupSearch:\n      baseDN: cn=groups,cn=accounts,dc=mydomain,dc=internal\n      filter: (objectClass=posixgroup)\n      linkGroupAttr: member\n      linkUserAttr: DN\n      nameAttr: cn\n</code></pre> <p>Trick: To obtain the 'rootCaData' from a FreeIPA server, log onto this server and enter the following:</p> <pre><code>cd /etc/ipa &amp;&amp; cat ca.crt  | base64 -w0\n</code></pre>"},{"location":"ldap/#setup-ldap-ca-in-a-configmap","title":"Setup LDAP CA in a configMap","text":"<p>The <code>rootCaData</code> attribute can be a rather lengthy string, which can be troublesome.  An alternative solution is to store it in a ConfigMap.</p> <p>First, create the configMap with the CA file: </p> <pre><code>kubectl -n skas-system create configmap ldap-ca.crt --from-file=./CA.crt\n</code></pre> <p>Then, modify the values file as follows:</p> values.ldap.yaml <pre><code>skMerge:\n  providers:\n    - name: crd\n    - name: ldap\n\nskLdap:\n  enabled: true\n\n  extraConfigMaps:\n    - configMap: ldap-ca.crt\n      volume: ldap-ca\n      mountPath: /tmp/ca/ldap\n\n  # --------------------------------- LDAP configuration\n  ldap:\n    host: ldap.mydomain.internal\n    port: 636\n    rootCaPath: /tmp/ca/ldap/CA.crt\n    bindDN: ..........\n</code></pre> <ul> <li>The <code>skLdap.extraConfigMaps</code> subsection instructs the POD to mount this configMap at the defined location. </li> <li>The property <code>skLdap.ldap.rootCaPath</code> can now refer to the mounted value, and, of course, <code>skLdap.ldap.rootCaData</code> should be removed.</li> </ul> <p>You can apply this modification with the following command:</p> <pre><code>helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml \\\n--values ./values.ldap.yaml\n</code></pre> <p>Remember to restart the SKAS pod(s) after applying any configuration changes. See Configuration: Pod restart</p>"},{"location":"toolsandtricks/","title":"Tools and Tricks","text":""},{"location":"toolsandtricks/#reloader","title":"reloader","text":"<p>Forgetting to restart a POD after making a configuration change is a common source of errors. Fortunately,  there are tools available to help with this, such as Reloader.</p> <p>The SKAS Helm chart adds the necessary annotations to the <code>deployment</code> to automate this process:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    configmap.reloader.stakater.com/reload: skas-merge-config,skas-auth-config,skas-crd-config,\n</code></pre> <p>The list of <code>configMap</code> is built dynamically by the Helm chart.</p> <p>Of course, if Reloader is not installed in your cluster, this annotation will be ineffective.\"</p>"},{"location":"toolsandtricks/#secret-generator","title":"Secret generator","text":"<p>As mentioned in Two LDAP servers configuration and Delegated users management,  it's necessary to generate a random secret during deployment. To achieve this, you can utilize  kubernetes-secret-generator, a custom Kubernetes controller.</p> <p>Here is a manifest that, once applied, will create the secret <code>skas2-client-secret</code>, which is used for authenticating  communication between the two pods in the configurations mentioned above.</p> <pre><code>---\napiVersion: \"secretgenerator.mittwald.de/v1alpha1\"\nkind: \"StringSecret\"\nmetadata:\nname: skas2-client-secret\nnamespace: skas-system\nspec:\nfields:\n- fieldName: \"clientSecret\"\nencoding: \"base64\"\nlength: \"15\"\n</code></pre>"},{"location":"toolsandtricks/#k9s","title":"k9s","text":"<p>We'd like to highlight the utility of a fantastic tool called k9s.  K9s is exceptional in that it can seamlessly handle Custom Resource Definitions (CRDs), making it a perfect choice  for dynamically displaying, modifying or deleting SKAS resources.</p> <p>As an example, you can access this screen using the <code>skusers</code> resource name:</p> <p></p> <p>This one using <code>groupbindings</code>:</p> <p></p> <p>This one using <code>tokens</code>:</p> <p></p> <p>Of course, k9s can't do more than what the launching user is allowed to do. This user can be authenticated using SKAS, but it must have a minimum set of rights to behave correctly.</p> <p>For instance, you can launch k9s using the <code>admin</code> user account we have set up during the installation process, provided it is a member of the <code>system:masters</code> group.</p> <pre><code>$ kubectl sk login admin\nPassword:\nlogged successfully..\n\n$ k9s\n....\n</code></pre>"},{"location":"toolsandtricks/#kubernetes-dashboard","title":"Kubernetes dashboard","text":"<p>To log in to the Kubernetes dashboard with SKAS, follow these steps:</p> <ul> <li>Ensure you are logged in using the CLI.</li> <li>Use the kubectl <code>sk whoami --all</code> command to retrieve your currently allocated token.\"</li> </ul> <pre><code>$ kubectl sk login admin\nPassword:\nlogged successfully..\n\n$ kubectl sk whoami --all\nUSER    ID   GROUPS                      AUTH.   TOKEN\nadmin   0    skas-admin,system:masters   crd     znitotnewjbqbuolqacckvgxyhptoxsuykznrzdacuvdhimy\n</code></pre> <p>Now, simply copy and paste the token value into the dashboard's login screen:</p> <p></p> <p>Of course, the operations you can perform through the dashboard will be limited by the logged-in user's permissions.</p>"},{"location":"toolsandtricks/#tricks-handle-two-different-sessions","title":"Tricks: Handle two different sessions","text":"<p>When working on user permissions, it could be useful to have separate session, at least one as admin, and one as  a user to test its capability.</p> <p>But, the default Kubernetes configuration is not bound to a terminal session but to a user. Therefore, any modifications  made to the local configuration (<code>kubectl config ...</code>) will affect all sessions.</p> <p>To work around this, you can change the location of the Kubernetes configuration for a specific session by modifying  the <code>KUBECONFIG</code> environment variable:</p> <pre><code>$ export KUBECONFIG=/tmp/kconfig\n</code></pre> <p><code>/tmp/kconfig</code> may be an empty or un-existing file</p> <p>Then you can initialize a new Kubernetes/SKAS context</p> <pre><code>$ kubectl sk init https://skas.ingress.mycluster.internal\nSetup new context 'skas@mycluster.internal' in kubeconfig file '/tmp/kconfig'\n</code></pre>"},{"location":"twoldapservers/","title":"Two LDAP servers configuration","text":""},{"location":"twoldapservers/#adding-a-second-ldap-server","title":"Adding a second LDAP server","text":"<p>As the two configurations are quite similar, there is a lot of redundancy between this chapter and Delegated users management chapter</p> <p></p> <p>The obvious solution to add a second LDAP server would be to duplicate the <code>skLdap</code> Identity Provider container, hook it under the <code>skMerge</code> module and link it to our second LDAP server.</p> <p>Unfortunately, the provided SKAS helm chart does not allow several instance of whatever container. Doing so would be possible, but at the price of a more complex chart.</p> <p>A possible solution would be to modify the chart by adding this second LDAP IDP. However, it's best to avoid such specific forks if possible.</p> <p></p> <p></p> <p>A more convenient solution will be to create another POD hosting only the new LDAP IDP.</p> <p>This pod will be instantiated using the same SKAS helm chart, under another release name. And all other modules than <code>skLdap</code> will be disabled.</p> <p>When all modules are in the same POD, inter-module communication use the <code>localhost</code> internal POD network. In this case, as the second <code>skLdap</code> module run in another POD, communication must go through a kubernetes service.</p> <p>This configuration requires two steps:</p> <ul> <li>Setup a new Helm deployment for the <code>skas2</code> secondary pod.</li> <li>Reconfigure the <code>skMerge</code> module of the main SKAS pod to connect to this new IDP.</li> </ul> <p></p> <p>In the following section, we will describe three variants of this configuration: one with a clear text connection and two secured variants with network encryption and inter-pod authentication.</p> <p>We recommend starting with the unsecured, simplest variant even if your ultimate goal is a fully secured configuration. You can then incrementally modify it as described.</p>"},{"location":"twoldapservers/#clear-text-connection","title":"Clear text connection","text":""},{"location":"twoldapservers/#secondary-pod-configuration","title":"Secondary POD configuration","text":"<p>Here is a sample values file for configuring the secondary POD:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskCrd:\nenabled: false\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap2.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain2,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain2,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7113\nssl: false\nservices:\nidentity:\ndisabled: false\nclients:\n- id: \"*\"\nsecret: \"*\"\nprotected: true\n</code></pre> <p>At the beginning of the file, we disable all modules except for <code>skLdap</code>.</p> <p>Then, you will need to adjust the configuration for the second LDAP server according to your specific context. Please refer to LDAP Setup for guidance.\"</p> <p>Then, there is the <code>exposure</code> part, who define how this service will be exposed. (The default configuration is expose to <code>localhost</code> in clear text)</p> <ul> <li><code>exposure.internal.enabled: false</code> shutdown the HTTP server bound to localhost.</li> <li><code>exposure.external.enabled: true</code> set up the HTTP server bound on the POD IP. This run on port 7113 with no SSL.</li> <li>Then the is the configuration of the <code>identity</code> service to expose:<ul> <li><code>clients[]</code> is a mechanism to validate who is able to access this service, by providing an <code>id</code> and a <code>secret</code>   (or password). the values \"*\" will disable this feature.</li> <li><code>protected: true</code> activate an internal mechanism against attacks of type 'Brut force', by introducing delays on   unsuccessful connection attempts, and by limiting the number of simultaneous connections. There is no reason to   disable it, except if you suspect a misbehavior.</li> </ul> </li> </ul> <p>To deploy this configuration, execute:</p> <pre><code>helm -n skas-system install skas2 skas/skas --values ./values.skas2.yaml\n</code></pre> <p>Note the `skas2' release name</p> <p>The Helm chart will deploy the new pod(s) under the name <code>skas2</code> and also create an associated Kubernetes <code>service</code>.</p>"},{"location":"twoldapservers/#main-pod-reconfiguration","title":"Main pod reconfiguration","text":"<p>The second step is to reconfigure the main pod. Here is a sample of the appropriate configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap1\ngroupPattern: \"dep1_%s\"\n- name: ldap2\ngroupPattern: \"dep2_%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\nldap1:\nurl: http://localhost:7013\nldap2:\nurl: http://skas2-ldap.skas-system.svc\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap1.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain1,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain1,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n</code></pre> <p>At the top of the <code>skMerge</code> module, there are two entries designed to configure a provider:</p> <ul> <li> <p><code>providers</code> is a list of connected providers, which allows you to define their behavior. The order of providers in  this list is important. For more information, refer to the  Identity Provider chaining: Provider configuration chapter.</p> </li> <li> <p><code>providerInfo</code> is a map that provides information on how to reach these providers. For <code>crd</code> and <code>ldap1</code>, we use  the default localhost port. For <code>ldap2</code>, we use the service created by the skas2 deployment. </p> </li> </ul> <p>The link between these two entries is, of course, the provider name.</p> <p>Next, there is the configuration for the primary LDAP server, which must be adapted to your specific configuration.\"</p> <p>The reconfiguration must then be applied by executing this command:</p> <pre><code>helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml --values ./values.skas.yaml\n</code></pre> <p>Don't forget to include the <code>values.init.yaml</code> file or merge it into the <code>values.skas.yaml</code> file. Additionally,  if you have other values files, make sure to include them in each upgrade.</p> <p>Also, remember to restart the pod(s) after making these configuration changes. You can find more information on how  to do this in the Configuration: Pod restart section.</p>"},{"location":"twoldapservers/#test","title":"Test","text":"<p>Now, you can test your configuration:</p> <pre><code>$ kubectl sk user describe nobody  --explain\n&gt; USER     STATUS         UID   GROUPS   EMAILS   COMMON NAMES   AUTH\n&gt; nobody   userNotFound   0\n&gt; Detail:\n&gt; PROVIDER   STATUS         UID   GROUPS   EMAILS   COMMON NAMES\n&gt; crd        userNotFound   0\n&gt; ldap1      userNotFound   0\n&gt; ldap2      userNotFound   0\n</code></pre> <p>You can check that both LDAP servers are taken into account. This also ensures that connections to both LDAP servers  are effective, as a provider is <code>critical</code> by default (Refers to the  Identity Provider chaining: Provider configuration chapter</p> <p>Of course, another test will be to describe some users existing in your LDAP servers.</p>"},{"location":"twoldapservers/#securing-connection","title":"Securing connection","text":"<p>It should be noted that unencrypted passwords will transit through the link between the two pods. Therefore, setting up encryption is a must-have.</p>"},{"location":"twoldapservers/#secondary-pod-configuration_1","title":"Secondary POD configuration","text":"<p>Here is the modified version for the <code>skas2</code> pod configuration:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskCrd:\nenabled: false\nclusterIssuer: your-cluster-issuer\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap2.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain2,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain2,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7113\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: skMerge\nsecret: aSharedSecret\nprotected: true\n</code></pre> <p>The differences are as follows:</p> <ul> <li>There is a <code>clusterIssuer</code> definition to enable the generation of a certificate. (It is assumed here that  <code>cert-manager</code> is deployed in the cluster).</li> <li><code>exposure.external.ssl</code> is set to <code>true</code>. This will also lead to the generation of the server certificate.</li> <li>The <code>service.identity.clients</code> authentication is also activated. The <code>id</code> and <code>secret</code> values will have to be  provided by the <code>skMerge</code> client.</li> </ul> <p>To deploy this configuration:</p> <pre><code>helm -n skas-system install skas2 skas/skas --values ./values.skas2.yaml\n</code></pre> <p>Note the `skas2' release name</p> <p>The Helm chart will deploy the new pod(s) with the name <code>skas2</code>. It will also deploy an associated Kubernetes <code>service</code> and submit a <code>cert-manager.io/v1/Certificate</code> request.</p>"},{"location":"twoldapservers/#main-pod-reconfiguration_1","title":"Main pod reconfiguration","text":"<p>Here is the modified version for the main SKAS POD configuration:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap1\ngroupPattern: \"dep1_%s\"\n- name: ldap2\ngroupPattern: \"dep2_%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\nldap1:\nurl: http://localhost:7013\nldap2:\nurl: https://skas2-ldap.skas-system.svc\nrootCaPath: /tmp/cert/ldap2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: aSharedSecret\nextraSecrets:\n- secret: skas2-ldap-cert\nvolume: ldap2-cert\nmountPath: /tmp/cert/ldap2\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap1.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain1,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain1,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n</code></pre> <p>The <code>providerInfo.ldap2</code> has been modified for SSL and authenticated connection:</p> <ul> <li><code>url</code> begins with <code>https</code>.</li> <li><code>clientAuth</code> provides information to authenticate against the <code>skLdap2</code> pod.</li> <li><code>insecureSkipVerify</code> is set to <code>false</code>, as we want to check certificate validity.</li> <li><code>rootCaPath</code> is set to access the <code>ca.crt</code>, the CA validating the <code>skLdap2</code> server certificate.</li> </ul> <p>As stated above, during the deployment of the <code>skas2</code> secondary POD, a server certificate was generated to allow  SSL-enabled services. This certificate is stored in a secret (of type <code>kubernetes.io/tls</code>) named <code>skas2-ldap-cert</code>.  Alongside the private/public key pair, it also contains the root Certificate Authority under the name <code>ca.crt</code>.</p> <p>The <code>skMerge.extraSecrets</code> subsection instructs the POD to mount this secret at the defined location.  The property <code>skMerge.providerInfo.ldap2.rootCaPath</code> can now reference the mounted value.</p> <p>Then, the reconfiguration must be applied:</p> <pre><code>helm -n skas-system upgrade skas skas/skas --values ./values.init.yaml --values ./values.skas.yaml\n</code></pre> <p>Don't forget to include the <code>values.init.yaml</code> file or merge it into the <code>values.skas.yaml</code> file. Additionally, if you have other values files, make sure to include them in each upgrade.</p> <p>Also, remember to restart the pod(s) after making these configuration changes. You can find more information on how to do this in the Configuration: Pod restart section.</p> <p>You can now test again your configuration, as described above</p>"},{"location":"twoldapservers/#using-a-kubernetes-secrets","title":"Using a Kubernetes secrets","text":"<p>There is still a security issue because the shared secret (<code>aSharedSecret</code>) is stored in plain text in both values  files, which could lead to it being accidentally committed to a version control system.</p> <p>The best practice is to store the secret value in a Kubernetes <code>secret</code> resource, like this:</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\nname: skas2-client-secret\nnamespace: skas-system\ndata:\nclientSecret: Sk1rbkNyYW5WV1YwR0E5\ntype: Opaque\n</code></pre> <p>Where <code>data.clientSecret</code> is the secret encoded in base64.</p> <p>There are several solutions to generate such a secret value. One can use Helm with a random generator function. Another one is to use a Secret generator.\"</p>"},{"location":"twoldapservers/#secondary-pod-configuration_2","title":"Secondary POD configuration","text":"<p>To use this secret, here is the new modified version of the <code>skas2</code> POD configuration:</p> values.skas2.yaml <pre><code>skAuth:\nenabled: false\nskMerge:\nenabled: false\nskCrd:\nenabled: false\nclusterIssuer: your-cluster-issuer\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap2.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain2,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain2,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7113\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: skMerge\nsecret: ${SKAS2_CLIENT_SECRET}\nprotected: true\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\n</code></pre> <p>The modifications are as follows:</p> <ul> <li>The <code>skLdap.extraEnv</code> subsection injects the secret value as an environment variable into the container.</li> <li>The <code>skLdap.exposure.external.services.identity.clients[0].secret</code> retrieves its value through this environment variable.</li> </ul> <p>Most of the values provided by the Helm chart end up inside a ConfigMap, which is then loaded by the SKAS executable.  Environment variable interpolation occurs during this loading process.</p>"},{"location":"twoldapservers/#main-pod-reconfiguration_2","title":"Main pod reconfiguration","text":"<p>Here is the modified version of the main SKAS pod configuration, which incorporates <code>secret</code> handling:</p> values.skas.yaml <pre><code>skMerge:\nproviders:\n- name: crd\n- name: ldap1\ngroupPattern: \"dep1_%s\"\n- name: ldap2\ngroupPattern: \"dep2_%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\nldap1:\nurl: http://localhost:7013\nldap2:\nurl: https://skas2-ldap.skas-system.svc\nrootCaPath: /tmp/cert/ldap2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: ${SKAS2_CLIENT_SECRET}\nextraEnv:\n- name: SKAS2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: skas2-client-secret\nkey: clientSecret\nextraSecrets:\n- secret: skas2-ldap-cert\nvolume: ldap2-cert\nmountPath: /tmp/cert/ldap2\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap1.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain1,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain1,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n</code></pre> <p>The modifications for the <code>skMerge</code> module are the same as those made for the SKAS2 POD.</p>"},{"location":"twoldapservers/#set-up-a-meta-helm-chart","title":"Set up a meta helm chart","text":"<p>Up to this point, we have configured our deployment by performing two closely related Helm deployments.  To simplify automation, it can be helpful to create a 'meta chart,' a chart that includes other charts as dependencies.</p> <p>Such a chart will have the following layout:</p> <pre><code>.\n|-- Chart.yaml\n|-- templates\n|   `-- stringsecret.yaml\n`-- values.yaml\n</code></pre> <p>In this example, we will implement encryption and inter-pod authentication.</p> <p>The <code>Chart.yaml</code> file defines the meta-chart <code>skas-skas2-meta</code>. It has two dependencies that deploy the same Helm  chart but with different values (as shown below). Please note the <code>alias: skas2</code> in the second deployment.</p> Chart.yaml <pre><code>apiVersion: v2\nname: skas-skas2-meta\nversion: 0.1.0\ndependencies:\n- name: skas\nversion: 0.2.1\nrepository: https://skasproject.github.io/skas-charts\n- name: skas\nalias: skas2\nversion: 0.2.1\nrepository: https://skasproject.github.io/skas-charts\n</code></pre> <p>The following manifest will generate the shared secret required for inter-pod authentication.</p>  templates/stringsecret.yaml <pre><code>---\napiVersion: \"secretgenerator.mittwald.de/v1alpha1\"\nkind: \"StringSecret\"\nmetadata:\nname: skas2-client-secret\nnamespace: skas-system\nspec:\nfields:\n- fieldName: \"clientSecret\"\nencoding: \"base64\"\nlength: \"15\"\n</code></pre> <p>And here is the global <code>values.yaml</code> file: </p> values.yaml <pre><code># ======================================================== Main SKAS Pod configuration\nskas:\nclusterIssuer: your-cluster-issuer\nskAuth:\nexposure:\nexternal:\ningress:\nhost: skas.ingress.kspray6\nkubeconfig:\ncontext:\nname: skas@kspray6\ncluster:\napiServerUrl: https://kubernetes.ingress.kspray6\nskMerge:\nproviders:\n- name: crd\n- name: ldap1\ngroupPattern: \"dep1_%s\"\n- name: ldap2\ngroupPattern: \"dep2_%s\"\nproviderInfo:\ncrd:\nurl: http://localhost:7012\nldap1:\nurl: http://localhost:7013\nldap2:\nurl: https://skas-skas2-ldap.skas-system.svc # Was https://skas2-ldap.skas-system.svc\nrootCaPath: /tmp/cert/ldap2/ca.crt\ninsecureSkipVerify: false\nclientAuth:\nid: skMerge\nsecret: ${LDAP2_CLIENT_SECRET}\nextraEnv:\n- name: LDAP2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: ldap2-client-secret\nkey: clientSecret\nextraSecrets:\n- secret: skas-skas2-ldap-cert # Was skas2-ldap-cert\nvolume: ldap2-cert\nmountPath: /tmp/cert/ldap2\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap1.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain1,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain1,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# ======================================================== SKAS2 Pod configuration\nskas2:\nskAuth:\nenabled: false\nskMerge:\nenabled: false\nskCrd:\nenabled: false\nclusterIssuer: your-cluster-issuer\nskLdap:\nenabled: true\n# --------------------------------- LDAP configuration\nldap:\nhost: ldap2.mydomain.internal\ninsecureNoSSL: false\nrootCaData: \"LS0tLS1CRUdJTiBDRVJUSUZ................................lRJRklDQVRFLS0tLS0K\"\nbindDN: cn=Manager,dc=mydomain2,dc=internal\nbindPW: admin123\ngroupSearch:\nbaseDN: ou=Groups,dc=mydomain2,dc=internal\nfilter: (objectClass=posixgroup)\nlinkGroupAttr: memberUid\nlinkUserAttr: uid\nnameAttr: cn\ntimeoutSec: 10\nuserSearch:\nbaseDN: ou=Users,dc=mydomain,dc=internal\ncnAttr: cn\nemailAttr: mail\nfilter: (objectClass=inetOrgPerson)\nloginAttr: uid\nnumericalIdAttr: uidNumber\n# By default, only internal (localhost) server is activated, to be called by another container running in the same pod.\n# Optionally, another server (external) can be activated, which can be accessed through a kubernetes service\n# In such case:\n# - A Client list should be provided to control access.\n# - ssl: true is strongly recommended.\n# - And protection against BFA should be activated (protected: true)\nexposure:\ninternal:\nenabled: false\nexternal:\nenabled: true\nport: 7113\nssl: true\nservices:\nidentity:\ndisabled: false\nclients:\n- id: skMerge\nsecret: ${LDAP2_CLIENT_SECRET}\nprotected: true\nextraEnv:\n- name: LDAP2_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: ldap2-client-secret\nkey: clientSecret\n</code></pre> <p>There are two blocks: <code>skas</code> and <code>skas2</code>, matching the name or alias in the <code>Chart.yaml</code> file.</p> <p>These two blocks hold the same definitions as the ones defined in the original configuration, with two differences:</p> <ul> <li><code>skas.skMerge.providerInfo.ldap2.url: https://skas-skas2-ldap.skas-system.svc</code></li> <li><code>skas.skMerge.extraSecrets[0].secret: skas-skas2-ldap-cert</code></li> </ul> <p>This is to accommodate service and secret name changes due to aliasing of the second dependency.</p> <p>Then, to launch the deployment, in the same folder as <code>Chart.yaml</code>, execute:</p> <pre><code>helm dependency build &amp;&amp; helm -n skas-system upgrade -i skas .\n</code></pre>"},{"location":"userguide/","title":"User guide","text":""},{"location":"userguide/#local-client-configuration","title":"Local client configuration","text":"<p>For this guide, we assume that kubectl is already installed. If it's not, you can refer to the official Kubernetes documentation for installation instructions.</p> <p>We also assume that you've installed the <code>kubectl-sk</code> CLI extension as outlined in the installation guide.</p> <p>To access a Kubernetes cluster using kubectl, you need a configuration file. By default, this file is located in <code>&lt;homedir&gt;/.kube/config</code>.</p> <p>SKAS provides a mechanism to create or update this user's configuration file, simplifying the setup process.</p> <p><pre><code>kubectl sk init https://skas.ingress.mycluster.internal\n</code></pre> <pre><code>Setup new context 'skas@mycluster.internal' in kubeconfig file '/Users/john/.kube/config'\n</code></pre></p> <p>You can verify that this new context is now set as the current one:</p> <p><pre><code>kubectl config current-context\n</code></pre> <pre><code>skas@mycluster.internal\n</code></pre></p>"},{"location":"userguide/#encountering-certificate-issues","title":"Encountering Certificate Issues?","text":"<p>If your system doesn't have the CA certificate that was used to certify SKAS (refer to the <code>clusterIssuer</code> parameter  during the initial installation), you may encounter an error similar to the following:</p> <pre><code>ERRO[0000] error on GET kubeconfig from remote server  error=\"error on http connection: Get \\\"https://skas.ingress.mycluster.internal/v1/kubeconfig\\\": \n tls: failed to verify certificate: x509: certificate signed by unknown authority\"\n</code></pre> <p>You can resolve this error by providing the root CA certificate as a file:</p> <pre><code>kubectl sk init https://skas.ingress.mycluster.internal --authRootCaPath=./CA.crt\n</code></pre> <p>Assuming you are a Kubernetes system administrator, here is how you can obtain the <code>CA.crt</code> file:</p> <pre><code>kubectl -n skas-system get secret skas-auth-cert -o=jsonpath='{.data.ca\\.crt}' | base64 -d &gt;./CA.crt\n</code></pre> <p>If you are unable to get such CA certificate, you can skip the test by setting a flag:</p> <pre><code>kubectl sk init --authInsecureSkipVerify=true https://skas.ingress.mycluster.internal\n</code></pre> <p>Using this flag should be limited to the initial evaluation context due to potential security risks, as the target site could be a fraudulent one.</p>"},{"location":"userguide/#basic-usage","title":"Basic usage","text":""},{"location":"userguide/#logging-in-with-the-default-admin-account","title":"Logging in with the Default Admin Account","text":"<p>SKAS manages a local user database where users are stored as Kubernetes resources. </p> <p>During installation, a default <code>admin</code> user with the password <code>admin</code> is created for convenience.</p> <p>By default, SKAS users are stored in the namespace <code>skas-system</code>. You could list them, using standard kubectl commands:</p> <pre><code>$ kubectl -n skas-system get users.userdb.skasproject.io\n&gt; Login:admin\n&gt; Password:\n&gt; NAME    COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\n&gt; admin   [\"SKAS administrator\"]\n</code></pre> <p>If you have configured your client as described above, you must now be logged in to execute any kubectl action. This involves the login and password interaction.</p> <p>A few important points to note:</p> <ul> <li>The default password is <code>admin</code>. It's crucial to change it for obvious security reasons. See the instructions below.</li> <li>The <code>admin</code> user has been granted access to SKAS resources in the <code>skas-system</code> namespace using Kubernetes RBAC.</li> <li>The command <code>kubectl -n skas-system get users</code> might not work as expected, as users is also a standard Kubernetes resource.</li> </ul> <p>To simplify SKAS user management, an alias <code>skuser</code> has been defined.</p> <pre><code>$ kubectl -n skas-system get skusers\n&gt; NAME    COMMON NAMES             EMAILS   UID   COMMENT   DISABLED\n&gt; admin   [\"SKAS administrator\"]\n</code></pre> <p>Please note that there is no longer a login/password interaction. Instead, a token was provided during the first login.  This token will expire after a period of inactivity, similar to a web session. The default inactivity timeout is 30 minutes.</p>"},{"location":"userguide/#logging-out-and-logging-in","title":"Logging Out and Logging In","text":"<p>Once you are logged in, you can use <code>kubectl</code> as you normally would. The token will be transparently used until it  expires due to inactivity.</p> <p>If the token expires, you will be prompted to enter your login and password again.</p> <p>You can also log out at any time by using the following command:</p> <pre><code>$ kubectl sk logout\n</code></pre> <p>Then, you will be prompted again for your login and password when you run the next <code>kubectl</code>command.</p> <p>Please note the <code>sk</code> subcommand, which instructs <code>kubectl</code> to forward the command to the <code>kubectl-sk</code> extension</p> <p>Alternatively, you can also use explicit login:</p> <pre><code>$ kubectl sk login\n&gt; Login:admin\n&gt; Password:\n&gt; logged successfully..\n</code></pre> <p>or</p> <pre><code>$ kubectl sk login admin\n&gt; Password:\n&gt; logged successfully..\n</code></pre> <p>or</p> <pre><code>$ kubectl sk login admin ${ADMIN_PASSWORD}\n&gt; logged successfully..\n</code></pre> <p>Running <code>sk login</code> will first perform an <code>sk logout</code> if you are currently logged in.</p>"},{"location":"userguide/#password-change","title":"Password change","text":"<p>As previously mentioned, it's essential to change the password of this account for security reasons.  Here's how you can do it:</p> <pre><code>$ kubectl sk password\n&gt; Will change password for user 'admin'\n&gt; Old password:\n&gt; New password:\n&gt; Confirm new password:\n&gt; Password has been changed sucessfully.\n</code></pre> <p>Please note the use of <code>sk</code> as this command is executed by the SKAS kubectl extension.</p> <p>There is a password strength check in place, so you may receive a response like this:</p> <pre><code>$ kubectl sk password\n&gt; Will change password for user 'admin'\n&gt; Old password:\n&gt; New password:\n&gt; Confirm new password:\n&gt; Unsatisfactory password strength!\n</code></pre> <p>The password criteria do not follow specific rules such as length or special character requirements.  Instead, an algorithm assigns a score to the password, and this score must meet a minimum (configurable) value.  Additionally, there is a check against a list of commonly used passwords.</p> <p>The simplest way to meet these criteria is to increase the length of your password.</p>"},{"location":"userguide/#skas-group-binding","title":"SKAS group binding","text":"<p>In reality, access to SKAS resources is granted not to the <code>admin</code> account (although it could be), but to a group named <code>skas-system</code>.</p> <p>The user <code>admin</code> has been included in the group <code>skas-system</code> through another SKAS resource named <code>groupbindings.userdb.skasproject.io</code>, with <code>groupbindings</code> serving as an alias.</p> <pre><code>$ kubectl -n skas-system get groupBindings\n&gt; NAME               USER    GROUP\n&gt; admin-skas-admin   admin   skas-admin\n</code></pre> <p>In Kubernetes, a group doesn't exist as a concrete resource; it only exists as a reference used in RBAC <code>roleBinding</code> or <code>clusterRoleBindings</code>, or in SKAS <code>groupBindings</code>.</p>"},{"location":"userguide/#becoming-a-cluster-administrator","title":"Becoming a Cluster Administrator","text":"<p>Let's attempt the following:</p> <pre><code>$ kubectl get namespaces\n&gt; Error from server (Forbidden): namespaces is forbidden: User \"admin\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope\n</code></pre> <p>It is clear that we have successfully authenticated as <code>admin</code>. However, this account does not possess the necessary permissions to execute cluster-wide operations.</p> <p>To gain these permissions, we must associate this user with a group that has the required rights:</p> <pre><code>$ kubectl sk user bind admin system:masters\n&gt; GroupBinding 'admin.system.masters' created in namespace 'skas-system'.\n</code></pre> <p>To make this effective, please log out and then log back in:</p> <pre><code>$ kubectl sk logout\n&gt; Bye!\n\n$ kubectl get namespaces\n&gt; Login:admin\n&gt; Password:\n&gt; NAME              STATUS   AGE\n&gt; cert-manager      Active   4d21h\n&gt; default           Active   4d21h\n&gt; ingress-nginx     Active   4d21h\n&gt; .....\n</code></pre> <p>You can verify the updated list of <code>groupBindings</code>:</p> <pre><code>$ kubectl -n skas-system get groupBindings\n&gt; NAME                   USER    GROUP\n&gt; admin-skas-admin       admin   skas-admin\n&gt; admin.system.masters   admin   system:masters\n</code></pre> <p>WARNING: This implies that any member of the <code>skas-admin</code> group can elevate their privileges to become a full cluster administrator. In reality, anyone with the capability to create or modify resources in the <code>skas-admin</code> namespace  can potentially take control of the entire cluster. Therefore, access to this namespace must be rigorously managed  and restricted. </p> <p>You can refer to Advanced Delegated User Management to learn how to delegate certain aspects of user management without compromising cluster security.</p>"},{"location":"userguide/#an-issue-with-stdin","title":"An issue with <code>stdin</code>","text":"<p>If you issue a <code>kubectl</code> command that use <code>stdin</code> as input, you may encounter the following error message:</p> <pre><code>$ cat mymanifest.yaml | kubectl apply -f -\n&gt; Login:\n&gt; Unable to access stdin to input login. Try login with `kubectl sk login' or 'kubectl-sk login'.` and issue this command again\n\n&gt; Unable to connect to the server: getting credentials: exec: executable kubectl-sk failed with exit code 18\n</code></pre> <p>This issue arises when your token has expired, and there is a conflict in using <code>stdin</code> for entering your login/password.</p> <p>The solution is to make sure you are logged in before executing such a command:</p> <pre><code>$ kubectl sk login\n&gt; Login:oriley\n&gt; Password:\n&gt; logged successfully..\n\n$ cat mymanifest.yaml | kubectl apply -f -\n&gt; pod/mypod created\n</code></pre>"},{"location":"userguide/#cli-users-management","title":"CLI users management","text":"<p>The SKAS kubectl extension plugin offers a <code>user</code> command with several subcommands</p> <p>You can obtain a complete list of these subcommands by running:</p> <pre><code>$ kubectl sk user --help\n&gt; Skas user management\n&gt;\n&gt; Usage:\n&gt;   kubectl-sk user [command]\n&gt;\n&gt; Available Commands:\n&gt; .....\n</code></pre> <p>To use this subcommand, you must be logged in as a member of the <code>skas-admin</code> group.</p>"},{"location":"userguide/#create-a-new-user","title":"Create a new user","text":"<p>Here  is an example of the user creation process:</p> <pre><code>$ kubectl sk user create luser1 --commonName \"Local user1\" --email \"luser1@internal\" --password \"RtVksSuMgP5f\"\n&gt; User 'luser1' created in namespace 'skas-system'.\n</code></pre> <p>The only mandatory parameters is the user's name:</p> <pre><code>$ kubectl sk user create luser2\n&gt; User 'luser2' created in namespace 'skas-system'.\n</code></pre> <p>Since no password is provided, it will be impossible for this user to log in.</p> <p>You can display a complete list of user creation options by running:</p> <pre><code>$ kubectl sk user create --help\n&gt; Create a new user\n&gt; \n&gt; Usage:\n&gt;   kubectl-sk user create &lt;user&gt; [flags]\n&gt; \n&gt; Flags:\n&gt;       --comment string        User's comment\n&gt;       --commonName string     User's common name\n&gt;       --email string          User's email\n&gt;       --generatePassword      Generate and display a password\n&gt;   -h, --help                  help for create\n&gt;       --inputPassword         Interactive password request\n&gt;   -n, --namespace string      User's DB namespace (default \"skas-system\")\n&gt;       --password string       User's password\n&gt;       --passwordHash string   User's password hash (Result of 'kubectl skas hash')\n&gt;       --state string          User's state (enabled|disabled) (default \"enabled\")\n&gt;       --uid int               User's UID\n</code></pre> <p>Many of the options correspond to a user's properties.</p> <ul> <li>The <code>comment</code>, <code>commonName</code>, <code>email</code>, <code>uid</code> parameters are purely descriptive and draw inspiration from Unix user attributes.</li> <li>The <code>state</code> parameter will enable the temporary disabling of a user account.</li> </ul> <p>The <code>--namespace</code> option permits the storage of user resources in a different namespace.  Refer to Delegated User Management for more details.</p> <p>There are several options related to the password:</p> <ul> <li><code>--password</code>: The password is supplied as a parameter on the command line.</li> <li><code>--inputPassword</code>: : This prompts the user for input with <code>Password:</code> / <code>Confirm password</code>: interaction.</li> <li><code>--generatePassword</code>: A random password is generated and displayed.</li> <li><code>--passwordHash</code>: This option allows you to provide the hash of the password, as it will be stored in the resource.    You can use <code>kubectl sk hash</code> command to generate this value. Please note that using this method bypasses the    password strength check.</li> </ul>"},{"location":"userguide/#list-users","title":"List users","text":"<p>You can list users using standard <code>kubectl</code> commands:</p> <pre><code>$ kubectl -n skas-system get skusers\n&gt; NAME     COMMON NAMES             EMAILS                UID   COMMENT   DISABLED\n&gt; admin    [\"SKAS administrator\"]\n&gt; luser1   [\"Local user1\"]          [\"luser1@internal\"]                   false\n&gt; luser2                                                                  false\n</code></pre>"},{"location":"userguide/#modify-user","title":"Modify user","text":"<p>There's a <code>patch</code> subcommand available for modifying a user. Here's an example:</p> <pre><code>$ kubectl sk user patch luser2 --state=disabled\n&gt; User 'luser2' updated in namespace 'skas-system'.\n\n$ kubectl -n skas-system get skuser luser2\n&gt; NAME     COMMON NAMES   EMAILS   UID   COMMENT   DISABLED\n&gt; luser2                                           true\n</code></pre> <p>Most of the options are the same as the <code>user create</code> subcommand. </p> <p>Additionally, there is a <code>--create</code> option that allows user creation if it does not already exist.</p>"},{"location":"userguide/#delete-user","title":"Delete user","text":"<p>You can delete users using standard <code>kubectl</code> commands:</p> <pre><code>$ kubectl -n skas-system delete skuser luser2\n&gt; user.userdb.skasproject.io \"luser2\" deleted\n</code></pre>"},{"location":"userguide/#manage-user-groups-and-permissions","title":"Manage user groups and permissions.","text":"<p>To illustrate how SKAS interact with Kubernetes RBAC, we will setup a simple example. We will:</p> <ul> <li>Create a namespace named <code>ldemo</code>.</li> <li>Create a role named <code>configurator</code> in this namespace to manage resources of type <code>configMaps</code>.</li> <li>Create a <code>roleBinding</code> between this role and a group named <code>ldemo-devs</code>.</li> <li>Add the user <code>luser1</code> to this group.</li> </ul> <p>We assume we are logged as <code>admin</code> to perform theses tasks:</p> <pre><code>$ kubectl create namespace ldemo\n&gt; namespace/ldemo created\n\n$ kubectl -n ldemo create role configurator --verb='*' --resource=configMaps\n&gt; role.rbac.authorization.k8s.io/configurator created\n\n$ kubectl -n ldemo create rolebinding configurator-ldemo-devs --role=configurator --group=ldemo-devs\n&gt; rolebinding.rbac.authorization.k8s.io/configurator-ldemo-devs created\n\n$ kubectl sk user bind luser1 ldemo-devs\n&gt; GroupBinding 'luser1.ldemo-devs' created in namespace 'skas-system'.\n</code></pre> <p>Now, we can proceed with testing. First, log out and then log in as <code>luser1</code>:\"</p> <pre><code>$ kubectl sk logout\n&gt; Bye!\n\n$ kubectl sk login\n&gt; Login:luser1\n&gt; Password:\n&gt; logged successfully..\n\n$ kubectl sk whoami\n&gt; USER     ID   GROUPS\n&gt; luser1   0    ldemo-devs\n</code></pre> <p>Now, let's ensure that we can create a <code>configMap</code> and view it:</p> <pre><code>$ kubectl -n ldemo create configmap my-config --from-literal=key1=config1\n&gt; configmap/my-config created\n\n$ kubectl -n ldemo get configmaps my-config -o yaml\n&gt; apiVersion: v1\n&gt; data:\n&gt;   key1: config1\n&gt; kind: ConfigMap\n&gt; metadata:\n&gt;   creationTimestamp: \"2023-07-11T14:56:27Z\"\n&gt;   name: my-config\n&gt;   namespace: ldemo\n&gt;   resourceVersion: \"257983\"\n&gt;   uid: ad55b282-9803-4688-b2df-a1c35f708313\n</code></pre> <p>Also, make sure that we can delete it:</p> <pre><code>$ kubectl -n ldemo delete configmap my-config\n&gt; configmap \"my-config\" deleted\n</code></pre> <p>Please, note than <code>roles</code> and <code>roleBindings</code> are namespaced resources while <code>users</code> and <code>groups</code> are cluster-wide resources.</p>"},{"location":"userguide/#kubernetes-rbac-referential-integrity","title":"Kubernetes RBAC referential integrity","text":"<p>Kubernetes does not check referential integrity when creating a resource that references another one.  For example, the following will work:</p> <pre><code>kubectl -n ldemo create rolebinding missing-integrity --role=unexisting-role --group=unexisting-group\n&gt; rolebinding.rbac.authorization.k8s.io/missing-integrity created\n\n$ kubectl sk user bind unexisting-user unexisting-group\n&gt; GroupBinding 'unexisting-user.unexisting-group' created in namespace 'skas-system'.\n</code></pre> <p>Maybe the referenced resource will be created later, or the link will be useless.</p> <p>This is evidently a design choice in Kubernetes, and SKAS follows the same logic.</p>"},{"location":"userguide/#using-manifests-instead-of-the-cli","title":"Using Manifests instead of the CLI","text":"<p>As users and groups are defined as Kubernetes custom resources, they can be created and managed using manifests.</p> <p>By default, all SKAS user and group resources are stored in the skas-system namespace.</p> <p>Kubernetes RBAC has been configured during installation to allow members of the <code>skas-admin</code> group to manage these resources.</p>"},{"location":"userguide/#user-resources","title":"User resources","text":"<p>Here is the manifest for the users we previously created:</p> <pre><code>---\napiVersion: userdb.skasproject.io/v1alpha1\nkind: User\nmetadata:\nname: luser1\nnamespace: skas-system\nspec:\ncommonNames:\n- Local user1\nemails:\n- luser1@internal\npasswordHash: $2a$10$q6nEVmP.MHo6VLAprTdTBuy6AHPel1uh3NocZdNjt.yh8HDE7Ja.m\n</code></pre> <ul> <li>The resources name is the user login.</li> <li>The password is stored in a non-reversible hashed form. You can compute such a hash using the <code>kubectl sk hash</code> command.</li> </ul> <p>Below is a sample user with all properties defined:</p> <pre><code>---\napiVersion: userdb.skasproject.io/v1alpha1\nkind: User\nmetadata:\nname: jsmith\nnamespace: skas-system\nspec:\ncommonNames:  - John SMITH\npasswordHash: $2a$10$lnweus6Oe3/XMoRaIImnVOwmxZ.xMp7iRB3X1TOcszzHE8nxfiwJK  # Password: \"Xderghy12\"\nemails: - jsmith@mycompany.com\nuid: 100001\ncomment: A sample user\ndisabled: false </code></pre> <p>To define a user, save the YAML definition in a file and execute the <code>kubectl apply -f &lt;fileName&gt;</code> command.</p>"},{"location":"userguide/#groupbinding-resources","title":"GroupBinding resources","text":"<p>The SKAS <code>GroupBinding</code> resources can also be defined using manifest:</p> <pre><code>---\napiVersion: userdb.skasproject.io/v1alpha1\nkind: GroupBinding\nmetadata:\nname: luser1.ldemo-devs\nnamespace: skas-system\nspec:\ngroup: ldemo-devs\nuser: luser1\n</code></pre>"},{"location":"userguide/#session-management","title":"Session management","text":""},{"location":"userguide/#view-active-sessions","title":"View active sessions","text":"<p>A token is generated for each user login, and it will expire after a period of inactivity, similar to a web session.  By default, this expiration period is set to 30 minutes. Additionally, there is a maximum token duration, which is set to 12 hours by default.</p> <p>On the server side, SKAS tokens are also stored as Kubernetes custom resources in the skas-system namespace.  RBAC has been configured to grant access to these resources to any member of the skas-admin group.</p> <p>SKAS tokens can be listed just like any other Kubernetes resources:</p> <pre><code>$ kubectl -n skas-system get tokens\n&gt; NAME                                               CLIENT   USER LOGIN   AUTH.   USER ID   CREATION               LAST HIT\n&gt; khrvvqwvpotcufiltvymuumsvrodsbiuwypbzrjiqudzjthg            admin        crd     0         2023-07-12T08:23:36Z   2023-07-12T08:32:13Z\n&gt; ltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd            luser1       crd     0         2023-07-12T08:27:19Z   2023-07-12T08:27:19Z\n</code></pre> <p>Each token represents an active user session, and SKAS will automatically remove it after 30 minutes of inactivity by default.</p> <p>You can view the details of each token in a detailed manner:</p> <pre><code>$ kubectl -n skas-system get tokens ltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd -o yaml\n&gt; apiVersion: session.skasproject.io/v1alpha1\n&gt; kind: Token\n&gt; metadata:\n&gt;   creationTimestamp: \"2023-07-12T08:27:19Z\"\n&gt;   generation: 1\n&gt;   name: ltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd\n&gt;   namespace: skas-system\n&gt;   resourceVersion: \"513150\"\n&gt;   uid: 220471a2-2ec1-4b7f-af85-8647c4406343\n&gt; spec:\n&gt;   authority: crd\n&gt;   client: \"\"\n&gt;   creation: \"2023-07-12T08:27:19Z\"\n&gt;   user:\n&gt;     commonNames:\n&gt;     - Local user1\n&gt;     emails:\n&gt;     - luser1@internal\n&gt;     groups:\n&gt;     - ldemo-devs\n&gt;     login: luser1\n&gt;     uid: 0\n&gt; status:\n&gt;   lastHit: \"2023-07-12T08:27:19Z\"\n</code></pre>"},{"location":"userguide/#terminate-session","title":"Terminate session","text":"<p>To end a session, you need to delete the corresponding token:</p> <pre><code>$ kubectl -n skas-system delete tokens ltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd\n&gt; token.session.skasproject.io \"ltdrlwnzzzhpxipgqgsvsaftmmucxxmfzhhwrdtuijabhvfd\" deleted\n</code></pre> <p>Please note that there is a local cache of 30 seconds on the client side. So, the session will remain active for this short (and configurable) period even after the token is deleted.</p>"},{"location":"userguide/#others-kubectl-sk-commands","title":"Others <code>kubectl sk</code> commands","text":""},{"location":"userguide/#hash","title":"hash","text":"<p>This command computes the hash value of a password. It is intended to be used when creating a user through a manifest. Please note that there is no password strength check when using this method.</p>"},{"location":"userguide/#init","title":"init","text":"<p>This command has been used at the beginning of this chapter. If you enter <code>kubectl sk init --help</code>, you can see there is some more options:</p> <ul> <li>Some are related to certificate management and was already mentioned.</li> <li>Some allow overriding of values provided by the server.</li> <li><code>clientId/Secret</code> is an optional method to restrict access to this command to users provided with these information. To be configured on the server.</li> </ul> <p>This command has been used at the beginning of this chapter. If you enter <code>kubectl sk init --help</code>, you can see that  there are some more options:</p> <ul> <li>Some are related to certificate management and have already been mentioned.</li> <li>Some allow for overriding values provided by the server.</li> <li><code>clientId/Secret</code> is an optional method to restrict access to this command to users provided with this information.   This needs to be configured on the server.</li> </ul>"},{"location":"userguide/#login","title":"login","text":"<p>Perform the login/password interaction. You can also provide the login and password on the command line.</p>"},{"location":"userguide/#logout","title":"logout","text":"<p>Log out the user by deleting the locally cached token.</p>"},{"location":"userguide/#password","title":"password","text":"<p>Change the current user password. </p> <p>If you want to change the password of another user, you can use the <code>kubectl sk user patch</code> command. However,  please note that you need to be a member of the <code>skas-admin</code> group to perform this action.</p>"},{"location":"userguide/#whoami","title":"whoami","text":"<p>Display the currently logged-in user and the groups to which it belongs.</p>"},{"location":"userguide/#version","title":"version","text":"<p>Display the current version of this SKAS plugin</p>"},{"location":"userguide/#what-to-provide-to-other-kubernetes-users","title":"What to provide to other Kubernetes users","text":"<p>Here is a small checklist of what to provide to non-admin users to allow them to use kubectl on a SKAS enabled cluster.</p> <ul> <li>Obviously, instructions to install <code>kubectl</code>.</li> <li>Instructions to install <code>kubectl-sk</code></li> <li>If needed, the <code>CA.crt</code> certificate file</li> <li>The <code>kubectl sk init https://skas.....</code> command line.</li> <li>The namespace(s) they are allowed to access</li> </ul> <p>About this last point: You can instruct them to add the <code>--namespaceOverride</code> option on <code>kubectl sk init ...</code> command. This will define the provided namespace as the default one in the <code>~/.kube/config</code> file.</p> <p>Here is a small checklist of what to provide to non-admin users to allow them to use kubectl on a SKAS enabled cluster:</p> <ul> <li>Installation instructions for <code>kubectl</code>.</li> <li>Installation instructions for <code>kubectl-sk</code>.</li> <li>The <code>CA.crt</code> certificate file (if needed).</li> <li>The <code>kubectl sk init https://skas.....</code> command line.</li> <li>The namespace(s) they are allowed to access.</li> <li>Regarding the last point, you can instruct them to add the <code>--namespaceOverride</code> option to the <code>kubectl sk init ...</code>  command. This will set the provided namespace as the default one in the <code>~/.kube/config</code> file.</li> </ul>"}]}